{"mean 100 episode reward": 0.0, "reward": 0.025000000000000015, "% time spent exploring": 99, "steps": 108, "episodes": 2}
{"mean 100 episode reward": 0.0, "reward": 0.002, "% time spent exploring": 98, "steps": 217, "episodes": 3}
{"mean 100 episode reward": 0.0, "reward": 0.03900000000000003, "% time spent exploring": 97, "steps": 326, "episodes": 4}
{"mean 100 episode reward": 0.0, "reward": 0.002, "% time spent exploring": 96, "steps": 435, "episodes": 5}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 95, "steps": 544, "episodes": 6}
{"mean 100 episode reward": 0.0, "reward": 0.01800000000000001, "% time spent exploring": 94, "steps": 653, "episodes": 7}
{"mean 100 episode reward": 0.0, "reward": 0.016000000000000007, "% time spent exploring": 93, "steps": 762, "episodes": 8}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 92, "steps": 871, "episodes": 9}
{"mean 100 episode reward": 0.0, "reward": 0.025000000000000015, "% time spent exploring": 91, "steps": 980, "episodes": 10}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 90, "steps": 1089, "episodes": 11}
{"mean 100 episode reward": 0.0, "reward": 0.002, "% time spent exploring": 89, "steps": 1198, "episodes": 12}
{"mean 100 episode reward": 0.0, "reward": 0.009000000000000001, "% time spent exploring": 88, "steps": 1307, "episodes": 13}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 87, "steps": 1416, "episodes": 14}
{"mean 100 episode reward": 0.0, "reward": 0.027000000000000017, "% time spent exploring": 86, "steps": 1525, "episodes": 15}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 85, "steps": 1634, "episodes": 16}
{"mean 100 episode reward": 0.0, "reward": 0.016000000000000007, "% time spent exploring": 84, "steps": 1743, "episodes": 17}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 83, "steps": 1852, "episodes": 18}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 82, "steps": 1961, "episodes": 19}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 81, "steps": 2070, "episodes": 20}
{"mean 100 episode reward": 0.0, "reward": 0.006, "% time spent exploring": 80, "steps": 2179, "episodes": 21}
{"mean 100 episode reward": 0.0, "reward": 0.015000000000000006, "% time spent exploring": 79, "steps": 2288, "episodes": 22}
{"mean 100 episode reward": 0.0, "reward": 0.015000000000000006, "% time spent exploring": 78, "steps": 2397, "episodes": 23}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 77, "steps": 2506, "episodes": 24}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 76, "steps": 2615, "episodes": 25}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 75, "steps": 2724, "episodes": 26}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 74, "steps": 2833, "episodes": 27}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 73, "steps": 2942, "episodes": 28}
{"mean 100 episode reward": 0.0, "reward": 0.01900000000000001, "% time spent exploring": 72, "steps": 3051, "episodes": 29}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 71, "steps": 3160, "episodes": 30}
{"mean 100 episode reward": 0.0, "reward": 0.01900000000000001, "% time spent exploring": 70, "steps": 3269, "episodes": 31}
{"mean 100 episode reward": 0.0, "reward": 0.02000000000000001, "% time spent exploring": 69, "steps": 3378, "episodes": 32}
{"mean 100 episode reward": 0.0, "reward": 0.022000000000000013, "% time spent exploring": 68, "steps": 3487, "episodes": 33}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 67, "steps": 3596, "episodes": 34}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 66, "steps": 3705, "episodes": 35}
{"mean 100 episode reward": 0.0, "reward": 0.005, "% time spent exploring": 65, "steps": 3814, "episodes": 36}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 64, "steps": 3923, "episodes": 37}
{"mean 100 episode reward": 0.0, "reward": 0.046000000000000034, "% time spent exploring": 63, "steps": 4032, "episodes": 38}
{"mean 100 episode reward": 0.0, "reward": 0.002, "% time spent exploring": 62, "steps": 4141, "episodes": 39}
{"mean 100 episode reward": 0.0, "reward": 0.005, "% time spent exploring": 61, "steps": 4250, "episodes": 40}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 60, "steps": 4359, "episodes": 41}
{"mean 100 episode reward": 0.0, "reward": 0.016000000000000007, "% time spent exploring": 59, "steps": 4468, "episodes": 42}
{"mean 100 episode reward": 0.0, "reward": 0.009000000000000001, "% time spent exploring": 58, "steps": 4577, "episodes": 43}
{"mean 100 episode reward": 0.0, "reward": 0.013000000000000005, "% time spent exploring": 57, "steps": 4686, "episodes": 44}
{"mean 100 episode reward": 0.0, "reward": 0.04000000000000003, "% time spent exploring": 56, "steps": 4795, "episodes": 45}
{"mean 100 episode reward": 0.0, "reward": 0.009000000000000001, "% time spent exploring": 55, "steps": 4904, "episodes": 46}
{"mean 100 episode reward": 0.0, "reward": 0.023000000000000013, "% time spent exploring": 54, "steps": 5013, "episodes": 47}
{"mean 100 episode reward": 0.0, "reward": 0.006, "% time spent exploring": 53, "steps": 5122, "episodes": 48}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 52, "steps": 5231, "episodes": 49}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 51, "steps": 5340, "episodes": 50}
{"mean 100 episode reward": 0.0, "reward": 0.06700000000000005, "% time spent exploring": 50, "steps": 5449, "episodes": 51}
{"mean 100 episode reward": 0.0, "reward": 0.036000000000000025, "% time spent exploring": 49, "steps": 5558, "episodes": 52}
{"mean 100 episode reward": 0.0, "reward": 0.01900000000000001, "% time spent exploring": 48, "steps": 5667, "episodes": 53}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 48, "steps": 5776, "episodes": 54}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 47, "steps": 5885, "episodes": 55}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 46, "steps": 5994, "episodes": 56}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 45, "steps": 6103, "episodes": 57}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 44, "steps": 6212, "episodes": 58}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 43, "steps": 6321, "episodes": 59}
{"mean 100 episode reward": 0.0, "reward": 0.025000000000000015, "% time spent exploring": 42, "steps": 6430, "episodes": 60}
{"mean 100 episode reward": 0.0, "reward": 0.02900000000000002, "% time spent exploring": 41, "steps": 6539, "episodes": 61}
{"mean 100 episode reward": 0.0, "reward": 0.02000000000000001, "% time spent exploring": 40, "steps": 6648, "episodes": 62}
{"mean 100 episode reward": 0.0, "reward": 0.05500000000000004, "% time spent exploring": 39, "steps": 6757, "episodes": 63}
{"mean 100 episode reward": 0.0, "reward": 0.01800000000000001, "% time spent exploring": 38, "steps": 6866, "episodes": 64}
{"mean 100 episode reward": 0.0, "reward": 0.05300000000000004, "% time spent exploring": 37, "steps": 6975, "episodes": 65}
{"mean 100 episode reward": 0.0, "reward": 0.01900000000000001, "% time spent exploring": 36, "steps": 7084, "episodes": 66}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 35, "steps": 7193, "episodes": 67}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 34, "steps": 7302, "episodes": 68}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 33, "steps": 7411, "episodes": 69}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 32, "steps": 7520, "episodes": 70}
{"mean 100 episode reward": 0.0, "reward": 0.023000000000000013, "% time spent exploring": 31, "steps": 7629, "episodes": 71}
{"mean 100 episode reward": 0.0, "reward": 0.07300000000000005, "% time spent exploring": 30, "steps": 7738, "episodes": 72}
{"mean 100 episode reward": 0.0, "reward": 0.06700000000000005, "% time spent exploring": 29, "steps": 7847, "episodes": 73}
{"mean 100 episode reward": 0.0, "reward": 0.06500000000000004, "% time spent exploring": 28, "steps": 7956, "episodes": 74}
{"mean 100 episode reward": 0.0, "reward": 0.04200000000000003, "% time spent exploring": 27, "steps": 8065, "episodes": 75}
{"mean 100 episode reward": 0.0, "reward": 0.009000000000000001, "% time spent exploring": 26, "steps": 8174, "episodes": 76}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 25, "steps": 8283, "episodes": 77}
{"mean 100 episode reward": 0.0, "reward": 0.027000000000000017, "% time spent exploring": 24, "steps": 8392, "episodes": 78}
{"mean 100 episode reward": 0.0, "reward": 0.03300000000000002, "% time spent exploring": 23, "steps": 8501, "episodes": 79}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 22, "steps": 8610, "episodes": 80}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 21, "steps": 8719, "episodes": 81}
{"mean 100 episode reward": 0.0, "reward": 0.08000000000000006, "% time spent exploring": 20, "steps": 8828, "episodes": 82}
{"mean 100 episode reward": 0.0, "reward": 0.09500000000000007, "% time spent exploring": 19, "steps": 8937, "episodes": 83}
{"mean 100 episode reward": 0.0, "reward": 0.07200000000000005, "% time spent exploring": 18, "steps": 9046, "episodes": 84}
{"mean 100 episode reward": 0.0, "reward": 0.04300000000000003, "% time spent exploring": 17, "steps": 9155, "episodes": 85}
{"mean 100 episode reward": 0.0, "reward": 0.09300000000000007, "% time spent exploring": 16, "steps": 9264, "episodes": 86}
{"mean 100 episode reward": 0.0, "reward": 0.08300000000000006, "% time spent exploring": 15, "steps": 9373, "episodes": 87}
{"mean 100 episode reward": 0.0, "reward": 0.09700000000000007, "% time spent exploring": 14, "steps": 9482, "episodes": 88}
{"mean 100 episode reward": 0.0, "reward": 0.028000000000000018, "% time spent exploring": 13, "steps": 9591, "episodes": 89}
{"mean 100 episode reward": 0.0, "reward": 0.04300000000000003, "% time spent exploring": 12, "steps": 9700, "episodes": 90}
{"mean 100 episode reward": 0.0, "reward": 0.005, "% time spent exploring": 11, "steps": 9809, "episodes": 91}
{"mean 100 episode reward": 0.0, "reward": 0.024000000000000014, "% time spent exploring": 10, "steps": 9918, "episodes": 92}
{"mean 100 episode reward": 0.0, "reward": 0.01900000000000001, "% time spent exploring": 9, "steps": 10027, "episodes": 93}
{"mean 100 episode reward": 0.0, "reward": 0.023000000000000013, "% time spent exploring": 9, "steps": 10136, "episodes": 94}
{"mean 100 episode reward": 0.0, "reward": 0.08300000000000006, "% time spent exploring": 9, "steps": 10245, "episodes": 95}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 9, "steps": 10354, "episodes": 96}
{"mean 100 episode reward": 0.0, "reward": 0.09500000000000007, "% time spent exploring": 9, "steps": 10463, "episodes": 97}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 10572, "episodes": 98}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 10681, "episodes": 99}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 10790, "episodes": 100}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 10899, "episodes": 101}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 9, "steps": 11008, "episodes": 102}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 11117, "episodes": 103}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 9, "steps": 11226, "episodes": 104}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 11335, "episodes": 105}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 11444, "episodes": 106}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 11553, "episodes": 107}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 9, "steps": 11662, "episodes": 108}
{"mean 100 episode reward": 0.0, "reward": 0.01800000000000001, "% time spent exploring": 9, "steps": 11771, "episodes": 109}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 11880, "episodes": 110}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 9, "steps": 11989, "episodes": 111}
{"mean 100 episode reward": 0.0, "reward": 0.005, "% time spent exploring": 9, "steps": 12098, "episodes": 112}
{"mean 100 episode reward": 0.0, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 12207, "episodes": 113}
{"mean 100 episode reward": 0.0, "reward": 0.02000000000000001, "% time spent exploring": 9, "steps": 12316, "episodes": 114}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 12425, "episodes": 115}
{"mean 100 episode reward": 0.0, "reward": 0.002, "% time spent exploring": 9, "steps": 12534, "episodes": 116}
{"mean 100 episode reward": 0.0, "reward": 0.008, "% time spent exploring": 9, "steps": 12643, "episodes": 117}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 12752, "episodes": 118}
{"mean 100 episode reward": 0.0, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 12861, "episodes": 119}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 12970, "episodes": 120}
{"mean 100 episode reward": 0.0, "reward": 0.05000000000000004, "% time spent exploring": 9, "steps": 13079, "episodes": 121}
{"mean 100 episode reward": 0.0, "reward": 0.024000000000000014, "% time spent exploring": 9, "steps": 13188, "episodes": 122}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 9, "steps": 13297, "episodes": 123}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 13406, "episodes": 124}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 13515, "episodes": 125}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 13624, "episodes": 126}
{"mean 100 episode reward": 0.0, "reward": 0.03100000000000002, "% time spent exploring": 9, "steps": 13733, "episodes": 127}
{"mean 100 episode reward": 0.0, "reward": 0.028000000000000018, "% time spent exploring": 9, "steps": 13842, "episodes": 128}
{"mean 100 episode reward": 0.0, "reward": 0.03300000000000002, "% time spent exploring": 9, "steps": 13951, "episodes": 129}
{"mean 100 episode reward": 0.0, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 14060, "episodes": 130}
{"mean 100 episode reward": 0.0, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 14169, "episodes": 131}
{"mean 100 episode reward": 0.0, "reward": 0.010000000000000002, "% time spent exploring": 9, "steps": 14278, "episodes": 132}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 14387, "episodes": 133}
{"mean 100 episode reward": 0.0, "reward": 0.08400000000000006, "% time spent exploring": 9, "steps": 14496, "episodes": 134}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 9, "steps": 14605, "episodes": 135}
{"mean 100 episode reward": 0.0, "reward": 0.04000000000000003, "% time spent exploring": 9, "steps": 14714, "episodes": 136}
{"mean 100 episode reward": 0.0, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 14823, "episodes": 137}
{"mean 100 episode reward": 0.0, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 14932, "episodes": 138}
{"mean 100 episode reward": 0.0, "reward": 0.06900000000000005, "% time spent exploring": 9, "steps": 15041, "episodes": 139}
{"mean 100 episode reward": 0.0, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 15150, "episodes": 140}
{"mean 100 episode reward": 0.0, "reward": 0.05500000000000004, "% time spent exploring": 9, "steps": 15259, "episodes": 141}
{"mean 100 episode reward": 0.0, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 15368, "episodes": 142}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 15477, "episodes": 143}
{"mean 100 episode reward": 0.0, "reward": 0.046000000000000034, "% time spent exploring": 9, "steps": 15586, "episodes": 144}
{"mean 100 episode reward": 0.0, "reward": 0.06800000000000005, "% time spent exploring": 9, "steps": 15695, "episodes": 145}
{"mean 100 episode reward": 0.0, "reward": 0.09700000000000007, "% time spent exploring": 9, "steps": 15804, "episodes": 146}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 15913, "episodes": 147}
{"mean 100 episode reward": 0.0, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 16022, "episodes": 148}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 16131, "episodes": 149}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 16240, "episodes": 150}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 16349, "episodes": 151}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 16458, "episodes": 152}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 9, "steps": 16567, "episodes": 153}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 16676, "episodes": 154}
{"mean 100 episode reward": 0.0, "reward": 0.006, "% time spent exploring": 9, "steps": 16785, "episodes": 155}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 16894, "episodes": 156}
{"mean 100 episode reward": 0.0, "reward": 0.016000000000000007, "% time spent exploring": 9, "steps": 17003, "episodes": 157}
{"mean 100 episode reward": 0.0, "reward": 0.022000000000000013, "% time spent exploring": 9, "steps": 17112, "episodes": 158}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 17221, "episodes": 159}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 9, "steps": 17330, "episodes": 160}
{"mean 100 episode reward": 0.0, "reward": 0.027000000000000017, "% time spent exploring": 9, "steps": 17439, "episodes": 161}
{"mean 100 episode reward": 0.0, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 17548, "episodes": 162}
{"mean 100 episode reward": 0.0, "reward": 0.04500000000000003, "% time spent exploring": 9, "steps": 17657, "episodes": 163}
{"mean 100 episode reward": 0.0, "reward": 0.036000000000000025, "% time spent exploring": 9, "steps": 17766, "episodes": 164}
{"mean 100 episode reward": 0.0, "reward": 0.025000000000000015, "% time spent exploring": 9, "steps": 17875, "episodes": 165}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 17984, "episodes": 166}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 18093, "episodes": 167}
{"mean 100 episode reward": 0.0, "reward": 0.07600000000000005, "% time spent exploring": 9, "steps": 18202, "episodes": 168}
{"mean 100 episode reward": 0.0, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 18311, "episodes": 169}
{"mean 100 episode reward": 0.0, "reward": 0.09800000000000007, "% time spent exploring": 9, "steps": 18420, "episodes": 170}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 9, "steps": 18529, "episodes": 171}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 18638, "episodes": 172}
{"mean 100 episode reward": 0.0, "reward": 0.006, "% time spent exploring": 9, "steps": 18747, "episodes": 173}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 18856, "episodes": 174}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 18965, "episodes": 175}
{"mean 100 episode reward": 0.0, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 19074, "episodes": 176}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 19183, "episodes": 177}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 19292, "episodes": 178}
{"mean 100 episode reward": 0.0, "reward": 0.01800000000000001, "% time spent exploring": 9, "steps": 19401, "episodes": 179}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 19510, "episodes": 180}
{"mean 100 episode reward": 0.0, "reward": 0.009000000000000001, "% time spent exploring": 9, "steps": 19619, "episodes": 181}
{"mean 100 episode reward": 0.0, "reward": 0.013000000000000005, "% time spent exploring": 9, "steps": 19728, "episodes": 182}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 9, "steps": 19837, "episodes": 183}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 19946, "episodes": 184}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 9, "steps": 20055, "episodes": 185}
{"mean 100 episode reward": 0.0, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 20164, "episodes": 186}
{"mean 100 episode reward": 0.0, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 20273, "episodes": 187}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 20382, "episodes": 188}
{"mean 100 episode reward": 0.0, "reward": 0.022000000000000013, "% time spent exploring": 9, "steps": 20491, "episodes": 189}
{"mean 100 episode reward": 0.0, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 20600, "episodes": 190}
{"mean 100 episode reward": 0.0, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 20709, "episodes": 191}
{"mean 100 episode reward": 0.0, "reward": 0.10300000000000008, "% time spent exploring": 9, "steps": 20818, "episodes": 192}
{"mean 100 episode reward": 0.0, "reward": 0.10300000000000008, "% time spent exploring": 9, "steps": 20927, "episodes": 193}
{"mean 100 episode reward": 0.0, "reward": 0.05000000000000004, "% time spent exploring": 9, "steps": 21036, "episodes": 194}
{"mean 100 episode reward": 0.0, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 21145, "episodes": 195}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 21254, "episodes": 196}
{"mean 100 episode reward": 0.0, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 21363, "episodes": 197}
{"mean 100 episode reward": 0.0, "reward": 0.03100000000000002, "% time spent exploring": 9, "steps": 21472, "episodes": 198}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 21581, "episodes": 199}
{"mean 100 episode reward": 0.0, "reward": 0.07100000000000005, "% time spent exploring": 9, "steps": 21690, "episodes": 200}
{"mean 100 episode reward": 0.0, "reward": 0.012000000000000004, "% time spent exploring": 9, "steps": 21799, "episodes": 201}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 9, "steps": 21908, "episodes": 202}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 22017, "episodes": 203}
{"mean 100 episode reward": 0.0, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 22126, "episodes": 204}
{"mean 100 episode reward": 0.0, "reward": 0.008, "% time spent exploring": 9, "steps": 22235, "episodes": 205}
{"mean 100 episode reward": 0.0, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 22344, "episodes": 206}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 22453, "episodes": 207}
{"mean 100 episode reward": 0.0, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 22562, "episodes": 208}
{"mean 100 episode reward": 0.0, "reward": 0.06100000000000005, "% time spent exploring": 9, "steps": 22671, "episodes": 209}
{"mean 100 episode reward": 0.0, "reward": 0.02900000000000002, "% time spent exploring": 9, "steps": 22780, "episodes": 210}
{"mean 100 episode reward": 0.0, "reward": 0.03100000000000002, "% time spent exploring": 9, "steps": 22889, "episodes": 211}
{"mean 100 episode reward": 0.0, "reward": 0.08100000000000006, "% time spent exploring": 9, "steps": 22998, "episodes": 212}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 23107, "episodes": 213}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 23216, "episodes": 214}
{"mean 100 episode reward": 0.0, "reward": 0.015000000000000006, "% time spent exploring": 9, "steps": 23325, "episodes": 215}
{"mean 100 episode reward": 0.0, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 23434, "episodes": 216}
{"mean 100 episode reward": 0.0, "reward": 0.025000000000000015, "% time spent exploring": 9, "steps": 23543, "episodes": 217}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 23652, "episodes": 218}
{"mean 100 episode reward": 0.0, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 23761, "episodes": 219}
{"mean 100 episode reward": 0.0, "reward": 0.07100000000000005, "% time spent exploring": 9, "steps": 23870, "episodes": 220}
{"mean 100 episode reward": 0.0, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 23979, "episodes": 221}
{"mean 100 episode reward": 0.0, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 24088, "episodes": 222}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 24197, "episodes": 223}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 24306, "episodes": 224}
{"mean 100 episode reward": 0.0, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 24415, "episodes": 225}
{"mean 100 episode reward": 0.0, "reward": 0.03000000000000002, "% time spent exploring": 9, "steps": 24524, "episodes": 226}
{"mean 100 episode reward": 0.0, "reward": 0.02900000000000002, "% time spent exploring": 9, "steps": 24633, "episodes": 227}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 24742, "episodes": 228}
{"mean 100 episode reward": 0.0, "reward": 0.008, "% time spent exploring": 9, "steps": 24851, "episodes": 229}
{"mean 100 episode reward": 0.0, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 24960, "episodes": 230}
{"mean 100 episode reward": 0.0, "reward": 0.0, "% time spent exploring": 9, "steps": 25069, "episodes": 231}
{"mean 100 episode reward": 0.0, "reward": 0.004, "% time spent exploring": 9, "steps": 25178, "episodes": 232}
{"mean 100 episode reward": 0.0, "reward": 0.001, "% time spent exploring": 9, "steps": 25287, "episodes": 233}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 25396, "episodes": 234}
{"mean 100 episode reward": 0.0, "reward": 0.003, "% time spent exploring": 9, "steps": 25505, "episodes": 235}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 25614, "episodes": 236}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 25723, "episodes": 237}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 25832, "episodes": 238}
{"mean 100 episode reward": 0.0, "reward": 0.014000000000000005, "% time spent exploring": 9, "steps": 25941, "episodes": 239}
{"mean 100 episode reward": 0.0, "reward": 0.01800000000000001, "% time spent exploring": 9, "steps": 26050, "episodes": 240}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 26159, "episodes": 241}
{"mean 100 episode reward": 0.0, "reward": 0.024000000000000014, "% time spent exploring": 9, "steps": 26268, "episodes": 242}
{"mean 100 episode reward": 0.0, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 26377, "episodes": 243}
{"mean 100 episode reward": 0.0, "reward": 0.06900000000000005, "% time spent exploring": 9, "steps": 26486, "episodes": 244}
{"mean 100 episode reward": 0.0, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 26595, "episodes": 245}
{"mean 100 episode reward": 0.0, "reward": 0.048000000000000036, "% time spent exploring": 9, "steps": 26704, "episodes": 246}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 26813, "episodes": 247}
{"mean 100 episode reward": 0.0, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 26922, "episodes": 248}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 27031, "episodes": 249}
{"mean 100 episode reward": 0.0, "reward": 0.04300000000000003, "% time spent exploring": 9, "steps": 27140, "episodes": 250}
{"mean 100 episode reward": 0.0, "reward": 0.05400000000000004, "% time spent exploring": 9, "steps": 27249, "episodes": 251}
{"mean 100 episode reward": 0.0, "reward": 0.028000000000000018, "% time spent exploring": 9, "steps": 27358, "episodes": 252}
{"mean 100 episode reward": 0.0, "reward": 0.03400000000000002, "% time spent exploring": 9, "steps": 27467, "episodes": 253}
{"mean 100 episode reward": 0.0, "reward": 0.048000000000000036, "% time spent exploring": 9, "steps": 27576, "episodes": 254}
{"mean 100 episode reward": 0.0, "reward": 0.09400000000000007, "% time spent exploring": 9, "steps": 27685, "episodes": 255}
{"mean 100 episode reward": 0.0, "reward": 0.10800000000000008, "% time spent exploring": 9, "steps": 27794, "episodes": 256}
{"mean 100 episode reward": 0.0, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 27903, "episodes": 257}
{"mean 100 episode reward": 0.0, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 28012, "episodes": 258}
{"mean 100 episode reward": 0.0, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 28121, "episodes": 259}
{"mean 100 episode reward": 0.0, "reward": 0.09300000000000007, "% time spent exploring": 9, "steps": 28230, "episodes": 260}
{"mean 100 episode reward": 0.0, "reward": 0.06400000000000004, "% time spent exploring": 9, "steps": 28339, "episodes": 261}
{"mean 100 episode reward": 0.0, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 28448, "episodes": 262}
{"mean 100 episode reward": 0.0, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 28557, "episodes": 263}
{"mean 100 episode reward": 0.0, "reward": 0.06800000000000005, "% time spent exploring": 9, "steps": 28666, "episodes": 264}
{"mean 100 episode reward": 0.0, "reward": 0.04400000000000003, "% time spent exploring": 9, "steps": 28775, "episodes": 265}
{"mean 100 episode reward": 0.0, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 28884, "episodes": 266}
{"mean 100 episode reward": 0.0, "reward": 0.024000000000000014, "% time spent exploring": 9, "steps": 28993, "episodes": 267}
{"mean 100 episode reward": 0.0, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 29102, "episodes": 268}
{"mean 100 episode reward": 0.0, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 29211, "episodes": 269}
{"mean 100 episode reward": 0.0, "reward": 0.09900000000000007, "% time spent exploring": 9, "steps": 29320, "episodes": 270}
{"mean 100 episode reward": 0.0, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 29429, "episodes": 271}
{"mean 100 episode reward": 0.0, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 29538, "episodes": 272}
{"mean 100 episode reward": 0.0, "reward": 0.04400000000000003, "% time spent exploring": 9, "steps": 29647, "episodes": 273}
{"mean 100 episode reward": 0.0, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 29756, "episodes": 274}
{"mean 100 episode reward": 0.0, "reward": 0.03900000000000003, "% time spent exploring": 9, "steps": 29865, "episodes": 275}
{"mean 100 episode reward": 0.0, "reward": 0.048000000000000036, "% time spent exploring": 9, "steps": 29974, "episodes": 276}
{"mean 100 episode reward": 0.0, "reward": 0.06200000000000005, "% time spent exploring": 9, "steps": 30083, "episodes": 277}
{"mean 100 episode reward": 0.0, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 30192, "episodes": 278}
{"mean 100 episode reward": 0.0, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 30301, "episodes": 279}
{"mean 100 episode reward": 0.0, "reward": 0.059000000000000045, "% time spent exploring": 9, "steps": 30410, "episodes": 280}
{"mean 100 episode reward": 0.0, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 30519, "episodes": 281}
{"mean 100 episode reward": 0.0, "reward": 0.011000000000000003, "% time spent exploring": 9, "steps": 30628, "episodes": 282}
{"mean 100 episode reward": 0.0, "reward": 0.008, "% time spent exploring": 9, "steps": 30737, "episodes": 283}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 30846, "episodes": 284}
{"mean 100 episode reward": 0.0, "reward": 0.007, "% time spent exploring": 9, "steps": 30955, "episodes": 285}
{"mean 100 episode reward": 0.0, "reward": 0.015000000000000006, "% time spent exploring": 9, "steps": 31064, "episodes": 286}
{"mean 100 episode reward": 0.0, "reward": 0.016000000000000007, "% time spent exploring": 9, "steps": 31173, "episodes": 287}
{"mean 100 episode reward": 0.0, "reward": 0.008, "% time spent exploring": 9, "steps": 31282, "episodes": 288}
{"mean 100 episode reward": 0.0, "reward": 0.03300000000000002, "% time spent exploring": 9, "steps": 31391, "episodes": 289}
{"mean 100 episode reward": 0.0, "reward": 0.02900000000000002, "% time spent exploring": 9, "steps": 31500, "episodes": 290}
{"mean 100 episode reward": 0.0, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 31609, "episodes": 291}
{"mean 100 episode reward": 0.0, "reward": 0.023000000000000013, "% time spent exploring": 9, "steps": 31718, "episodes": 292}
{"mean 100 episode reward": 0.0, "reward": 0.03900000000000003, "% time spent exploring": 9, "steps": 31827, "episodes": 293}
{"mean 100 episode reward": 0.0, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 31936, "episodes": 294}
{"mean 100 episode reward": 0.0, "reward": 0.04300000000000003, "% time spent exploring": 9, "steps": 32045, "episodes": 295}
{"mean 100 episode reward": 0.0, "reward": 0.09600000000000007, "% time spent exploring": 9, "steps": 32154, "episodes": 296}
{"mean 100 episode reward": 0.0, "reward": 0.057000000000000044, "% time spent exploring": 9, "steps": 32263, "episodes": 297}
{"mean 100 episode reward": 0.0, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 32372, "episodes": 298}
{"mean 100 episode reward": 0.0, "reward": 0.09900000000000007, "% time spent exploring": 9, "steps": 32481, "episodes": 299}
{"mean 100 episode reward": 0.0, "reward": 0.09900000000000007, "% time spent exploring": 9, "steps": 32590, "episodes": 300}
{"mean 100 episode reward": 0.0, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 32699, "episodes": 301}
{"mean 100 episode reward": 0.0, "reward": 0.10000000000000007, "% time spent exploring": 9, "steps": 32808, "episodes": 302}
{"mean 100 episode reward": 0.0, "reward": 0.10500000000000008, "% time spent exploring": 9, "steps": 32917, "episodes": 303}
{"mean 100 episode reward": 0.0, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 33026, "episodes": 304}
{"mean 100 episode reward": 0.0, "reward": 0.10600000000000008, "% time spent exploring": 9, "steps": 33135, "episodes": 305}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 33244, "episodes": 306}
{"mean 100 episode reward": 0.0, "reward": 0.06900000000000005, "% time spent exploring": 9, "steps": 33353, "episodes": 307}
{"mean 100 episode reward": 0.0, "reward": 0.10000000000000007, "% time spent exploring": 9, "steps": 33462, "episodes": 308}
{"mean 100 episode reward": 0.0, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 33571, "episodes": 309}
{"mean 100 episode reward": 0.0, "reward": 0.08900000000000007, "% time spent exploring": 9, "steps": 33680, "episodes": 310}
{"mean 100 episode reward": 0.0, "reward": 0.06500000000000004, "% time spent exploring": 9, "steps": 33789, "episodes": 311}
{"mean 100 episode reward": 0.0, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 33898, "episodes": 312}
{"mean 100 episode reward": 0.0, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 34007, "episodes": 313}
{"mean 100 episode reward": 0.0, "reward": 0.08900000000000007, "% time spent exploring": 9, "steps": 34116, "episodes": 314}
{"mean 100 episode reward": 0.1, "reward": 0.10600000000000008, "% time spent exploring": 9, "steps": 34225, "episodes": 315}
{"mean 100 episode reward": 0.1, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 34334, "episodes": 316}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 34443, "episodes": 317}
{"mean 100 episode reward": 0.1, "reward": 0.07600000000000005, "% time spent exploring": 9, "steps": 34552, "episodes": 318}
{"mean 100 episode reward": 0.1, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 34661, "episodes": 319}
{"mean 100 episode reward": 0.1, "reward": 0.006, "% time spent exploring": 9, "steps": 34770, "episodes": 320}
{"mean 100 episode reward": 0.1, "reward": 0.009000000000000001, "% time spent exploring": 9, "steps": 34879, "episodes": 321}
{"mean 100 episode reward": 0.0, "reward": 0.023000000000000013, "% time spent exploring": 9, "steps": 34988, "episodes": 322}
{"mean 100 episode reward": 0.0, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 35097, "episodes": 323}
{"mean 100 episode reward": 0.1, "reward": 0.02000000000000001, "% time spent exploring": 9, "steps": 35206, "episodes": 324}
{"mean 100 episode reward": 0.0, "reward": 0.005, "% time spent exploring": 9, "steps": 35315, "episodes": 325}
{"mean 100 episode reward": 0.0, "reward": 0.026000000000000016, "% time spent exploring": 9, "steps": 35424, "episodes": 326}
{"mean 100 episode reward": 0.0, "reward": 0.028000000000000018, "% time spent exploring": 9, "steps": 35533, "episodes": 327}
{"mean 100 episode reward": 0.0, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 35642, "episodes": 328}
{"mean 100 episode reward": 0.1, "reward": 0.009000000000000001, "% time spent exploring": 9, "steps": 35751, "episodes": 329}
{"mean 100 episode reward": 0.1, "reward": 0.07300000000000005, "% time spent exploring": 9, "steps": 35860, "episodes": 330}
{"mean 100 episode reward": 0.1, "reward": 0.04500000000000003, "% time spent exploring": 9, "steps": 35969, "episodes": 331}
{"mean 100 episode reward": 0.1, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 36078, "episodes": 332}
{"mean 100 episode reward": 0.1, "reward": 0.09100000000000007, "% time spent exploring": 9, "steps": 36187, "episodes": 333}
{"mean 100 episode reward": 0.1, "reward": 0.09600000000000007, "% time spent exploring": 9, "steps": 36296, "episodes": 334}
{"mean 100 episode reward": 0.1, "reward": 0.10200000000000008, "% time spent exploring": 9, "steps": 36405, "episodes": 335}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 36514, "episodes": 336}
{"mean 100 episode reward": 0.1, "reward": 0.05400000000000004, "% time spent exploring": 9, "steps": 36623, "episodes": 337}
{"mean 100 episode reward": 0.1, "reward": 0.048000000000000036, "% time spent exploring": 9, "steps": 36732, "episodes": 338}
{"mean 100 episode reward": 0.1, "reward": 0.01900000000000001, "% time spent exploring": 9, "steps": 36841, "episodes": 339}
{"mean 100 episode reward": 0.1, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 36950, "episodes": 340}
{"mean 100 episode reward": 0.1, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 37059, "episodes": 341}
{"mean 100 episode reward": 0.1, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 37168, "episodes": 342}
{"mean 100 episode reward": 0.1, "reward": 0.07000000000000005, "% time spent exploring": 9, "steps": 37277, "episodes": 343}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 37386, "episodes": 344}
{"mean 100 episode reward": 0.1, "reward": 0.07100000000000005, "% time spent exploring": 9, "steps": 37495, "episodes": 345}
{"mean 100 episode reward": 0.1, "reward": 0.09300000000000007, "% time spent exploring": 9, "steps": 37604, "episodes": 346}
{"mean 100 episode reward": 0.1, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 37713, "episodes": 347}
{"mean 100 episode reward": 0.1, "reward": 0.046000000000000034, "% time spent exploring": 9, "steps": 37822, "episodes": 348}
{"mean 100 episode reward": 0.1, "reward": 0.07700000000000005, "% time spent exploring": 9, "steps": 37931, "episodes": 349}
{"mean 100 episode reward": 0.1, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 38040, "episodes": 350}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 38149, "episodes": 351}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 38258, "episodes": 352}
{"mean 100 episode reward": 0.1, "reward": 0.06100000000000005, "% time spent exploring": 9, "steps": 38367, "episodes": 353}
{"mean 100 episode reward": 0.1, "reward": 0.07100000000000005, "% time spent exploring": 9, "steps": 38476, "episodes": 354}
{"mean 100 episode reward": 0.1, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 38585, "episodes": 355}
{"mean 100 episode reward": 0.1, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 38694, "episodes": 356}
{"mean 100 episode reward": 0.1, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 38803, "episodes": 357}
{"mean 100 episode reward": 0.1, "reward": 0.08800000000000006, "% time spent exploring": 9, "steps": 38912, "episodes": 358}
{"mean 100 episode reward": 0.1, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 39021, "episodes": 359}
{"mean 100 episode reward": 0.1, "reward": 0.10400000000000008, "% time spent exploring": 9, "steps": 39130, "episodes": 360}
{"mean 100 episode reward": 0.1, "reward": 0.09300000000000007, "% time spent exploring": 9, "steps": 39239, "episodes": 361}
{"mean 100 episode reward": 0.1, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 39348, "episodes": 362}
{"mean 100 episode reward": 0.1, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 39457, "episodes": 363}
{"mean 100 episode reward": 0.1, "reward": 0.06400000000000004, "% time spent exploring": 9, "steps": 39566, "episodes": 364}
{"mean 100 episode reward": 0.1, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 39675, "episodes": 365}
{"mean 100 episode reward": 0.1, "reward": 0.07200000000000005, "% time spent exploring": 9, "steps": 39784, "episodes": 366}
{"mean 100 episode reward": 0.1, "reward": 0.059000000000000045, "% time spent exploring": 9, "steps": 39893, "episodes": 367}
{"mean 100 episode reward": 0.1, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 40002, "episodes": 368}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 40111, "episodes": 369}
{"mean 100 episode reward": 0.1, "reward": 0.10600000000000008, "% time spent exploring": 9, "steps": 40220, "episodes": 370}
{"mean 100 episode reward": 0.1, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 40329, "episodes": 371}
{"mean 100 episode reward": 0.1, "reward": 0.04400000000000003, "% time spent exploring": 9, "steps": 40438, "episodes": 372}
{"mean 100 episode reward": 0.1, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 40547, "episodes": 373}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 40656, "episodes": 374}
{"mean 100 episode reward": 0.1, "reward": 0.07700000000000005, "% time spent exploring": 9, "steps": 40765, "episodes": 375}
{"mean 100 episode reward": 0.1, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 40874, "episodes": 376}
{"mean 100 episode reward": 0.1, "reward": 0.07500000000000005, "% time spent exploring": 9, "steps": 40983, "episodes": 377}
{"mean 100 episode reward": 0.1, "reward": 0.06500000000000004, "% time spent exploring": 9, "steps": 41092, "episodes": 378}
{"mean 100 episode reward": 0.1, "reward": 0.06200000000000005, "% time spent exploring": 9, "steps": 41201, "episodes": 379}
{"mean 100 episode reward": 0.1, "reward": 0.059000000000000045, "% time spent exploring": 9, "steps": 41310, "episodes": 380}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 41419, "episodes": 381}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 41528, "episodes": 382}
{"mean 100 episode reward": 0.1, "reward": 0.017000000000000008, "% time spent exploring": 9, "steps": 41637, "episodes": 383}
{"mean 100 episode reward": 0.1, "reward": 0.05500000000000004, "% time spent exploring": 9, "steps": 41746, "episodes": 384}
{"mean 100 episode reward": 0.1, "reward": 0.035000000000000024, "% time spent exploring": 9, "steps": 41855, "episodes": 385}
{"mean 100 episode reward": 0.1, "reward": 0.059000000000000045, "% time spent exploring": 9, "steps": 41964, "episodes": 386}
{"mean 100 episode reward": 0.1, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 42073, "episodes": 387}
{"mean 100 episode reward": 0.1, "reward": 0.05300000000000004, "% time spent exploring": 9, "steps": 42182, "episodes": 388}
{"mean 100 episode reward": 0.1, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 42291, "episodes": 389}
{"mean 100 episode reward": 0.1, "reward": 0.04200000000000003, "% time spent exploring": 9, "steps": 42400, "episodes": 390}
{"mean 100 episode reward": 0.1, "reward": 0.05400000000000004, "% time spent exploring": 9, "steps": 42509, "episodes": 391}
{"mean 100 episode reward": 0.1, "reward": 0.04200000000000003, "% time spent exploring": 9, "steps": 42618, "episodes": 392}
{"mean 100 episode reward": 0.1, "reward": 0.03200000000000002, "% time spent exploring": 9, "steps": 42727, "episodes": 393}
{"mean 100 episode reward": 0.1, "reward": 0.046000000000000034, "% time spent exploring": 9, "steps": 42836, "episodes": 394}
{"mean 100 episode reward": 0.1, "reward": 0.037000000000000026, "% time spent exploring": 9, "steps": 42945, "episodes": 395}
{"mean 100 episode reward": 0.1, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 43054, "episodes": 396}
{"mean 100 episode reward": 0.1, "reward": 0.08400000000000006, "% time spent exploring": 9, "steps": 43163, "episodes": 397}
{"mean 100 episode reward": 0.1, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 43272, "episodes": 398}
{"mean 100 episode reward": 0.1, "reward": 0.04200000000000003, "% time spent exploring": 9, "steps": 43381, "episodes": 399}
{"mean 100 episode reward": 0.1, "reward": 0.07300000000000005, "% time spent exploring": 9, "steps": 43490, "episodes": 400}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 43599, "episodes": 401}
{"mean 100 episode reward": 0.1, "reward": 0.05500000000000004, "% time spent exploring": 9, "steps": 43708, "episodes": 402}
{"mean 100 episode reward": 0.1, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 43817, "episodes": 403}
{"mean 100 episode reward": 0.1, "reward": 0.05300000000000004, "% time spent exploring": 9, "steps": 43926, "episodes": 404}
{"mean 100 episode reward": 0.1, "reward": 0.04500000000000003, "% time spent exploring": 9, "steps": 44035, "episodes": 405}
{"mean 100 episode reward": 0.1, "reward": 0.07600000000000005, "% time spent exploring": 9, "steps": 44144, "episodes": 406}
{"mean 100 episode reward": 0.1, "reward": 0.09000000000000007, "% time spent exploring": 9, "steps": 44253, "episodes": 407}
{"mean 100 episode reward": 0.1, "reward": 0.02100000000000001, "% time spent exploring": 9, "steps": 44362, "episodes": 408}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 44471, "episodes": 409}
{"mean 100 episode reward": 0.1, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 44580, "episodes": 410}
{"mean 100 episode reward": 0.1, "reward": 0.005, "% time spent exploring": 9, "steps": 44689, "episodes": 411}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 44798, "episodes": 412}
{"mean 100 episode reward": 0.1, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 44907, "episodes": 413}
{"mean 100 episode reward": 0.1, "reward": 0.05500000000000004, "% time spent exploring": 9, "steps": 45016, "episodes": 414}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 45125, "episodes": 415}
{"mean 100 episode reward": 0.1, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 45234, "episodes": 416}
{"mean 100 episode reward": 0.1, "reward": 0.023000000000000013, "% time spent exploring": 9, "steps": 45343, "episodes": 417}
{"mean 100 episode reward": 0.1, "reward": 0.05400000000000004, "% time spent exploring": 9, "steps": 45452, "episodes": 418}
{"mean 100 episode reward": 0.1, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 45561, "episodes": 419}
{"mean 100 episode reward": 0.1, "reward": 0.010000000000000002, "% time spent exploring": 9, "steps": 45670, "episodes": 420}
{"mean 100 episode reward": 0.1, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 45779, "episodes": 421}
{"mean 100 episode reward": 0.1, "reward": 0.057000000000000044, "% time spent exploring": 9, "steps": 45888, "episodes": 422}
{"mean 100 episode reward": 0.1, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 45997, "episodes": 423}
{"mean 100 episode reward": 0.1, "reward": 0.08000000000000006, "% time spent exploring": 9, "steps": 46106, "episodes": 424}
{"mean 100 episode reward": 0.1, "reward": 0.05400000000000004, "% time spent exploring": 9, "steps": 46215, "episodes": 425}
{"mean 100 episode reward": 0.1, "reward": 0.07300000000000005, "% time spent exploring": 9, "steps": 46324, "episodes": 426}
{"mean 100 episode reward": 0.1, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 46433, "episodes": 427}
{"mean 100 episode reward": 0.1, "reward": 0.08500000000000006, "% time spent exploring": 9, "steps": 46542, "episodes": 428}
{"mean 100 episode reward": 0.1, "reward": 0.08600000000000006, "% time spent exploring": 9, "steps": 46651, "episodes": 429}
{"mean 100 episode reward": 0.1, "reward": 0.09100000000000007, "% time spent exploring": 9, "steps": 46760, "episodes": 430}
{"mean 100 episode reward": 0.1, "reward": 0.06800000000000005, "% time spent exploring": 9, "steps": 46869, "episodes": 431}
{"mean 100 episode reward": 0.1, "reward": 0.07000000000000005, "% time spent exploring": 9, "steps": 46978, "episodes": 432}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 47087, "episodes": 433}
{"mean 100 episode reward": 0.1, "reward": 0.10500000000000008, "% time spent exploring": 9, "steps": 47196, "episodes": 434}
{"mean 100 episode reward": 0.1, "reward": 0.08900000000000007, "% time spent exploring": 9, "steps": 47305, "episodes": 435}
{"mean 100 episode reward": 0.1, "reward": 0.09400000000000007, "% time spent exploring": 9, "steps": 47414, "episodes": 436}
{"mean 100 episode reward": 0.1, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 47523, "episodes": 437}
{"mean 100 episode reward": 0.1, "reward": 0.09300000000000007, "% time spent exploring": 9, "steps": 47632, "episodes": 438}
{"mean 100 episode reward": 0.1, "reward": 0.10100000000000008, "% time spent exploring": 9, "steps": 47741, "episodes": 439}
{"mean 100 episode reward": 0.1, "reward": 0.06800000000000005, "% time spent exploring": 9, "steps": 47850, "episodes": 440}
{"mean 100 episode reward": 0.1, "reward": 0.10100000000000008, "% time spent exploring": 9, "steps": 47959, "episodes": 441}
{"mean 100 episode reward": 0.1, "reward": 0.09900000000000007, "% time spent exploring": 9, "steps": 48068, "episodes": 442}
{"mean 100 episode reward": 0.1, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 48177, "episodes": 443}
{"mean 100 episode reward": 0.1, "reward": 0.05500000000000004, "% time spent exploring": 9, "steps": 48286, "episodes": 444}
{"mean 100 episode reward": 0.1, "reward": 0.09600000000000007, "% time spent exploring": 9, "steps": 48395, "episodes": 445}
{"mean 100 episode reward": 0.1, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 48504, "episodes": 446}
{"mean 100 episode reward": 0.1, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 48613, "episodes": 447}
{"mean 100 episode reward": 0.1, "reward": 0.08200000000000006, "% time spent exploring": 9, "steps": 48722, "episodes": 448}
{"mean 100 episode reward": 0.1, "reward": 0.09800000000000007, "% time spent exploring": 9, "steps": 48831, "episodes": 449}
{"mean 100 episode reward": 0.1, "reward": 0.09300000000000007, "% time spent exploring": 9, "steps": 48940, "episodes": 450}
{"mean 100 episode reward": 0.1, "reward": 0.07200000000000005, "% time spent exploring": 9, "steps": 49049, "episodes": 451}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 49158, "episodes": 452}
{"mean 100 episode reward": 0.1, "reward": 0.04100000000000003, "% time spent exploring": 9, "steps": 49267, "episodes": 453}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 49376, "episodes": 454}
{"mean 100 episode reward": 0.1, "reward": 0.06200000000000005, "% time spent exploring": 9, "steps": 49485, "episodes": 455}
{"mean 100 episode reward": 0.1, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 49594, "episodes": 456}
{"mean 100 episode reward": 0.1, "reward": 0.06700000000000005, "% time spent exploring": 9, "steps": 49703, "episodes": 457}
{"mean 100 episode reward": 0.1, "reward": 0.035000000000000024, "% time spent exploring": 9, "steps": 49812, "episodes": 458}
{"mean 100 episode reward": 0.1, "reward": 0.027000000000000017, "% time spent exploring": 9, "steps": 49921, "episodes": 459}
{"mean 100 episode reward": 0.1, "reward": 0.04000000000000003, "% time spent exploring": 9, "steps": 50030, "episodes": 460}
{"mean 100 episode reward": 0.1, "reward": 0.04400000000000003, "% time spent exploring": 9, "steps": 50139, "episodes": 461}
{"mean 100 episode reward": 0.1, "reward": 0.035000000000000024, "% time spent exploring": 9, "steps": 50248, "episodes": 462}
{"mean 100 episode reward": 0.1, "reward": 0.057000000000000044, "% time spent exploring": 9, "steps": 50357, "episodes": 463}
{"mean 100 episode reward": 0.1, "reward": 0.057000000000000044, "% time spent exploring": 9, "steps": 50466, "episodes": 464}
{"mean 100 episode reward": 0.1, "reward": 0.05000000000000004, "% time spent exploring": 9, "steps": 50575, "episodes": 465}
{"mean 100 episode reward": 0.1, "reward": 0.025000000000000015, "% time spent exploring": 9, "steps": 50684, "episodes": 466}
{"mean 100 episode reward": 0.1, "reward": 0.03800000000000003, "% time spent exploring": 9, "steps": 50793, "episodes": 467}
{"mean 100 episode reward": 0.1, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 50902, "episodes": 468}
{"mean 100 episode reward": 0.1, "reward": 0.02900000000000002, "% time spent exploring": 9, "steps": 51011, "episodes": 469}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 51120, "episodes": 470}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 51229, "episodes": 471}
{"mean 100 episode reward": 0.1, "reward": 0.04300000000000003, "% time spent exploring": 9, "steps": 51338, "episodes": 472}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 51447, "episodes": 473}
{"mean 100 episode reward": 0.1, "reward": 0.07500000000000005, "% time spent exploring": 9, "steps": 51556, "episodes": 474}
{"mean 100 episode reward": 0.1, "reward": 0.05200000000000004, "% time spent exploring": 9, "steps": 51665, "episodes": 475}
{"mean 100 episode reward": 0.1, "reward": 0.04200000000000003, "% time spent exploring": 9, "steps": 51774, "episodes": 476}
{"mean 100 episode reward": 0.1, "reward": 0.03000000000000002, "% time spent exploring": 9, "steps": 51883, "episodes": 477}
{"mean 100 episode reward": 0.1, "reward": 0.047000000000000035, "% time spent exploring": 9, "steps": 51992, "episodes": 478}
{"mean 100 episode reward": 0.1, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 52101, "episodes": 479}
{"mean 100 episode reward": 0.1, "reward": 0.06100000000000005, "% time spent exploring": 9, "steps": 52210, "episodes": 480}
{"mean 100 episode reward": 0.1, "reward": 0.06900000000000005, "% time spent exploring": 9, "steps": 52319, "episodes": 481}
{"mean 100 episode reward": 0.1, "reward": 0.07900000000000006, "% time spent exploring": 9, "steps": 52428, "episodes": 482}
{"mean 100 episode reward": 0.1, "reward": 0.04300000000000003, "% time spent exploring": 9, "steps": 52537, "episodes": 483}
{"mean 100 episode reward": 0.1, "reward": 0.036000000000000025, "% time spent exploring": 9, "steps": 52646, "episodes": 484}
{"mean 100 episode reward": 0.1, "reward": 0.04500000000000003, "% time spent exploring": 9, "steps": 52755, "episodes": 485}
{"mean 100 episode reward": 0.1, "reward": 0.04100000000000003, "% time spent exploring": 9, "steps": 52864, "episodes": 486}
{"mean 100 episode reward": 0.1, "reward": 0.05100000000000004, "% time spent exploring": 9, "steps": 52973, "episodes": 487}
{"mean 100 episode reward": 0.1, "reward": 0.06600000000000004, "% time spent exploring": 9, "steps": 53082, "episodes": 488}
{"mean 100 episode reward": 0.1, "reward": 0.10300000000000008, "% time spent exploring": 9, "steps": 53191, "episodes": 489}
{"mean 100 episode reward": 0.1, "reward": 0.03900000000000003, "% time spent exploring": 9, "steps": 53300, "episodes": 490}
{"mean 100 episode reward": 0.1, "reward": 0.06800000000000005, "% time spent exploring": 9, "steps": 53409, "episodes": 491}
{"mean 100 episode reward": 0.1, "reward": 0.08700000000000006, "% time spent exploring": 9, "steps": 53518, "episodes": 492}
{"mean 100 episode reward": 0.1, "reward": 0.059000000000000045, "% time spent exploring": 9, "steps": 53627, "episodes": 493}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 53736, "episodes": 494}
{"mean 100 episode reward": 0.1, "reward": 0.09100000000000007, "% time spent exploring": 9, "steps": 53845, "episodes": 495}
{"mean 100 episode reward": 0.1, "reward": 0.04400000000000003, "% time spent exploring": 9, "steps": 53954, "episodes": 496}
{"mean 100 episode reward": 0.1, "reward": 0.08300000000000006, "% time spent exploring": 9, "steps": 54063, "episodes": 497}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 54172, "episodes": 498}
{"mean 100 episode reward": 0.1, "reward": 0.07300000000000005, "% time spent exploring": 9, "steps": 54281, "episodes": 499}
{"mean 100 episode reward": 0.1, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 54390, "episodes": 500}
{"mean 100 episode reward": 0.1, "reward": 0.058000000000000045, "% time spent exploring": 9, "steps": 54499, "episodes": 501}
{"mean 100 episode reward": 0.1, "reward": 0.08100000000000006, "% time spent exploring": 9, "steps": 54608, "episodes": 502}
{"mean 100 episode reward": 0.1, "reward": 0.07000000000000005, "% time spent exploring": 9, "steps": 54717, "episodes": 503}
{"mean 100 episode reward": 0.1, "reward": 0.06700000000000005, "% time spent exploring": 9, "steps": 54826, "episodes": 504}
{"mean 100 episode reward": 0.1, "reward": 0.06400000000000004, "% time spent exploring": 9, "steps": 54935, "episodes": 505}
{"mean 100 episode reward": 0.1, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 55044, "episodes": 506}
{"mean 100 episode reward": 0.1, "reward": 0.07800000000000006, "% time spent exploring": 9, "steps": 55153, "episodes": 507}
{"mean 100 episode reward": 0.1, "reward": 0.09500000000000007, "% time spent exploring": 9, "steps": 55262, "episodes": 508}
{"mean 100 episode reward": 0.1, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 55371, "episodes": 509}
{"mean 100 episode reward": 0.1, "reward": 0.08100000000000006, "% time spent exploring": 9, "steps": 55480, "episodes": 510}
{"mean 100 episode reward": 0.1, "reward": 0.06300000000000004, "% time spent exploring": 9, "steps": 55589, "episodes": 511}
{"mean 100 episode reward": 0.1, "reward": 0.04900000000000004, "% time spent exploring": 9, "steps": 55698, "episodes": 512}
{"mean 100 episode reward": 0.1, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 55807, "episodes": 513}
{"mean 100 episode reward": 0.1, "reward": 0.04300000000000003, "% time spent exploring": 9, "steps": 55916, "episodes": 514}
{"mean 100 episode reward": 0.1, "reward": 0.060000000000000046, "% time spent exploring": 9, "steps": 56025, "episodes": 515}
{"mean 100 episode reward": 0.1, "reward": 0.05600000000000004, "% time spent exploring": 9, "steps": 56134, "episodes": 516}
{"mean 100 episode reward": 0.1, "reward": 0.07400000000000005, "% time spent exploring": 9, "steps": 56243, "episodes": 517}
