{"episodes": 2, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 99, "steps": 199}
{"episodes": 3, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 99, "steps": 399}
{"episodes": 4, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 98, "steps": 599}
{"episodes": 5, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 98, "steps": 799}
{"episodes": 6, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 98, "steps": 999}
{"episodes": 7, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 97, "steps": 1199}
{"episodes": 8, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 97, "steps": 1399}
{"episodes": 9, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 97, "steps": 1599}
{"episodes": 10, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 96, "steps": 1799}
{"episodes": 11, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 96, "steps": 1999}
{"episodes": 12, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 96, "steps": 2199}
{"episodes": 13, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 95, "steps": 2399}
{"episodes": 14, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 95, "steps": 2599}
{"episodes": 15, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 94, "steps": 2799}
{"episodes": 16, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 94, "steps": 2999}
{"episodes": 17, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 94, "steps": 3199}
{"episodes": 18, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 93, "steps": 3399}
{"episodes": 19, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 93, "steps": 3599}
{"episodes": 20, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 93, "steps": 3799}
{"episodes": 21, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 92, "steps": 3999}
{"episodes": 22, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 92, "steps": 4199}
{"episodes": 23, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 92, "steps": 4399}
{"episodes": 24, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 91, "steps": 4599}
{"episodes": 25, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 91, "steps": 4799}
{"episodes": 26, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 91, "steps": 4999}
{"episodes": 27, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 90, "steps": 5199}
{"episodes": 28, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 90, "steps": 5399}
{"episodes": 29, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 89, "steps": 5599}
{"episodes": 30, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 89, "steps": 5799}
{"episodes": 31, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 89, "steps": 5999}
{"episodes": 32, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 88, "steps": 6199}
{"episodes": 33, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 88, "steps": 6399}
{"episodes": 34, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 88, "steps": 6599}
{"episodes": 35, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 87, "steps": 6799}
{"episodes": 36, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 87, "steps": 6999}
{"episodes": 37, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 87, "steps": 7199}
{"episodes": 38, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 86, "steps": 7399}
{"episodes": 39, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 86, "steps": 7599}
{"episodes": 40, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 85, "steps": 7799}
{"episodes": 41, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 85, "steps": 7999}
{"episodes": 42, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 85, "steps": 8199}
{"episodes": 43, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 84, "steps": 8399}
{"episodes": 44, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 84, "steps": 8599}
{"episodes": 45, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 84, "steps": 8799}
{"episodes": 46, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 83, "steps": 8999}
{"episodes": 47, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 83, "steps": 9199}
{"episodes": 48, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 83, "steps": 9399}
{"episodes": 49, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 82, "steps": 9599}
{"episodes": 50, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 82, "steps": 9799}
{"episodes": 51, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 82, "steps": 9999}
{"episodes": 52, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 81, "steps": 10199}
{"episodes": 53, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 81, "steps": 10399}
{"episodes": 54, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 80, "steps": 10599}
{"episodes": 55, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 80, "steps": 10799}
{"episodes": 56, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 80, "steps": 10999}
{"episodes": 57, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 79, "steps": 11199}
{"episodes": 58, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 79, "steps": 11399}
{"episodes": 59, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 79, "steps": 11599}
{"episodes": 60, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 78, "steps": 11799}
{"episodes": 61, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 78, "steps": 11999}
{"episodes": 62, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 78, "steps": 12199}
{"episodes": 63, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 77, "steps": 12399}
{"episodes": 64, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 77, "steps": 12599}
{"episodes": 65, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 76, "steps": 12799}
{"episodes": 66, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 76, "steps": 12999}
{"episodes": 67, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 76, "steps": 13199}
{"episodes": 68, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 75, "steps": 13399}
{"episodes": 69, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 75, "steps": 13599}
{"episodes": 70, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 75, "steps": 13799}
{"episodes": 71, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 74, "steps": 13999}
{"episodes": 72, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 74, "steps": 14199}
{"episodes": 73, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 74, "steps": 14399}
{"episodes": 74, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 73, "steps": 14599}
{"episodes": 75, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 73, "steps": 14799}
{"episodes": 76, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 73, "steps": 14999}
{"episodes": 77, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 72, "steps": 15199}
{"episodes": 78, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 72, "steps": 15399}
{"episodes": 79, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 71, "steps": 15599}
{"episodes": 80, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 71, "steps": 15799}
{"episodes": 81, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 71, "steps": 15999}
{"episodes": 82, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 70, "steps": 16199}
{"episodes": 83, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 70, "steps": 16399}
{"episodes": 84, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 70, "steps": 16599}
{"episodes": 85, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 69, "steps": 16799}
{"episodes": 86, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 69, "steps": 16999}
{"episodes": 87, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 69, "steps": 17199}
{"episodes": 88, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 68, "steps": 17399}
{"episodes": 89, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 68, "steps": 17599}
{"episodes": 90, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 67, "steps": 17799}
{"episodes": 91, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 67, "steps": 17999}
{"episodes": 92, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 67, "steps": 18199}
{"episodes": 93, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 66, "steps": 18399}
{"episodes": 94, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 66, "steps": 18599}
{"episodes": 95, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 66, "steps": 18799}
{"episodes": 96, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 65, "steps": 18999}
{"episodes": 97, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 65, "steps": 19199}
{"episodes": 98, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 65, "steps": 19399}
{"episodes": 99, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 64, "steps": 19599}
{"episodes": 100, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 64, "steps": 19799}
{"episodes": 101, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 64, "steps": 19999}
{"episodes": 102, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 63, "steps": 20199}
{"episodes": 103, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 63, "steps": 20399}
{"episodes": 104, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 62, "steps": 20599}
{"episodes": 105, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 62, "steps": 20799}
{"episodes": 106, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 62, "steps": 20999}
{"episodes": 107, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 61, "steps": 21199}
{"episodes": 108, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 61, "steps": 21399}
{"episodes": 109, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 61, "steps": 21599}
{"episodes": 110, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 60, "steps": 21799}
{"episodes": 111, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 60, "steps": 21999}
{"episodes": 112, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 60, "steps": 22199}
{"episodes": 113, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 59, "steps": 22399}
{"episodes": 114, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 59, "steps": 22599}
{"episodes": 115, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 58, "steps": 22799}
{"episodes": 116, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 58, "steps": 22999}
{"episodes": 117, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 58, "steps": 23199}
{"episodes": 118, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 57, "steps": 23399}
{"episodes": 119, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 57, "steps": 23599}
{"episodes": 120, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 57, "steps": 23799}
{"episodes": 121, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 56, "steps": 23999}
{"episodes": 122, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 56, "steps": 24199}
{"episodes": 123, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 56, "steps": 24399}
{"episodes": 124, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 55, "steps": 24599}
{"episodes": 125, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 55, "steps": 24799}
{"episodes": 126, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 55, "steps": 24999}
{"episodes": 127, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 54, "steps": 25199}
{"episodes": 128, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 54, "steps": 25399}
{"episodes": 129, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 53, "steps": 25599}
{"episodes": 130, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 53, "steps": 25799}
{"episodes": 131, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 53, "steps": 25999}
{"episodes": 132, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 52, "steps": 26199}
{"episodes": 133, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 52, "steps": 26399}
{"episodes": 134, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 52, "steps": 26599}
{"episodes": 135, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 51, "steps": 26799}
{"episodes": 136, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 51, "steps": 26999}
{"episodes": 137, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 51, "steps": 27199}
{"episodes": 138, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 50, "steps": 27399}
{"episodes": 139, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 50, "steps": 27599}
{"episodes": 140, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 49, "steps": 27799}
{"episodes": 141, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 49, "steps": 27999}
{"episodes": 142, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 49, "steps": 28199}
{"episodes": 143, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 48, "steps": 28399}
{"episodes": 144, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 48, "steps": 28599}
{"episodes": 145, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 48, "steps": 28799}
{"episodes": 146, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 47, "steps": 28999}
{"episodes": 147, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 47, "steps": 29199}
{"episodes": 148, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 47, "steps": 29399}
{"episodes": 149, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 46, "steps": 29599}
{"episodes": 150, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 46, "steps": 29799}
{"episodes": 151, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 46, "steps": 29999}
{"episodes": 152, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 45, "steps": 30199}
{"episodes": 153, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 45, "steps": 30399}
{"episodes": 154, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 44, "steps": 30599}
{"episodes": 155, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 44, "steps": 30799}
{"episodes": 156, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 44, "steps": 30999}
{"episodes": 157, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 43, "steps": 31199}
{"episodes": 158, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 43, "steps": 31399}
{"episodes": 159, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 43, "steps": 31599}
{"episodes": 160, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 42, "steps": 31799}
{"episodes": 161, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 42, "steps": 31999}
{"episodes": 162, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 42, "steps": 32199}
{"episodes": 163, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 41, "steps": 32399}
{"episodes": 164, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 41, "steps": 32599}
{"episodes": 165, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 40, "steps": 32799}
{"episodes": 166, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 40, "steps": 32999}
{"episodes": 167, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 40, "steps": 33199}
{"episodes": 168, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 39, "steps": 33399}
{"episodes": 169, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 39, "steps": 33599}
{"episodes": 170, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 39, "steps": 33799}
{"episodes": 171, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 38, "steps": 33999}
{"episodes": 172, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 38, "steps": 34199}
{"episodes": 173, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 38, "steps": 34399}
{"episodes": 174, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 37, "steps": 34599}
{"episodes": 175, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 37, "steps": 34799}
{"episodes": 176, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 37, "steps": 34999}
{"episodes": 177, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 36, "steps": 35199}
{"episodes": 178, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 36, "steps": 35399}
{"episodes": 179, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 35, "steps": 35599}
{"episodes": 180, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 35, "steps": 35799}
{"episodes": 181, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 35, "steps": 35999}
{"episodes": 182, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 34, "steps": 36199}
{"episodes": 183, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 34, "steps": 36399}
{"episodes": 184, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 34, "steps": 36599}
{"episodes": 185, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 33, "steps": 36799}
{"episodes": 186, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 33, "steps": 36999}
{"episodes": 187, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 33, "steps": 37199}
{"episodes": 188, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 32, "steps": 37399}
{"episodes": 189, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 32, "steps": 37599}
{"episodes": 190, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 31, "steps": 37799}
{"episodes": 191, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 31, "steps": 37999}
{"episodes": 192, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 31, "steps": 38199}
{"episodes": 193, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 30, "steps": 38399}
{"episodes": 194, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 30, "steps": 38599}
{"episodes": 195, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 30, "steps": 38799}
{"episodes": 196, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 29, "steps": 38999}
{"episodes": 197, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 29, "steps": 39199}
{"episodes": 198, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 29, "steps": 39399}
{"episodes": 199, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 28, "steps": 39599}
{"episodes": 200, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 28, "steps": 39799}
{"episodes": 201, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 28, "steps": 39999}
{"episodes": 202, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 27, "steps": 40199}
{"episodes": 203, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 27, "steps": 40399}
{"episodes": 204, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 26, "steps": 40599}
{"episodes": 205, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 26, "steps": 40799}
{"episodes": 206, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 26, "steps": 40999}
{"episodes": 207, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 25, "steps": 41199}
{"episodes": 208, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 25, "steps": 41399}
{"episodes": 209, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 25, "steps": 41599}
{"episodes": 210, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 24, "steps": 41799}
{"episodes": 211, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 24, "steps": 41999}
{"episodes": 212, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 24, "steps": 42199}
{"episodes": 213, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 23, "steps": 42399}
{"episodes": 214, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 23, "steps": 42599}
{"episodes": 215, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 22, "steps": 42799}
{"episodes": 216, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 22, "steps": 42999}
{"episodes": 217, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 22, "steps": 43199}
{"episodes": 218, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 21, "steps": 43399}
{"episodes": 219, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 21, "steps": 43599}
{"episodes": 220, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 21, "steps": 43799}
{"episodes": 221, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 20, "steps": 43999}
{"episodes": 222, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 20, "steps": 44199}
{"episodes": 223, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 20, "steps": 44399}
{"episodes": 224, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 19, "steps": 44599}
{"episodes": 225, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 19, "steps": 44799}
{"episodes": 226, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 19, "steps": 44999}
{"episodes": 227, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 18, "steps": 45199}
{"episodes": 228, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 18, "steps": 45399}
{"episodes": 229, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 17, "steps": 45599}
{"episodes": 230, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 17, "steps": 45799}
{"episodes": 231, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 17, "steps": 45999}
{"episodes": 232, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 16, "steps": 46199}
{"episodes": 233, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 16, "steps": 46399}
{"episodes": 234, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 16, "steps": 46599}
{"episodes": 235, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 15, "steps": 46799}
{"episodes": 236, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 15, "steps": 46999}
{"episodes": 237, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 15, "steps": 47199}
{"episodes": 238, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 14, "steps": 47399}
{"episodes": 239, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 14, "steps": 47599}
{"episodes": 240, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 13, "steps": 47799}
{"episodes": 241, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 13, "steps": 47999}
{"episodes": 242, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 13, "steps": 48199}
{"episodes": 243, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 12, "steps": 48399}
{"episodes": 244, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 12, "steps": 48599}
{"episodes": 245, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 12, "steps": 48799}
{"episodes": 246, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 11, "steps": 48999}
{"episodes": 247, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 11, "steps": 49199}
{"episodes": 248, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 11, "steps": 49399}
{"episodes": 249, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 10, "steps": 49599}
{"episodes": 250, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 10, "steps": 49799}
{"episodes": 251, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 10, "steps": 49999}
{"episodes": 252, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 50199}
{"episodes": 253, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 50399}
{"episodes": 254, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 50599}
{"episodes": 255, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 50799}
{"episodes": 256, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 50999}
{"episodes": 257, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 51199}
{"episodes": 258, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 51399}
{"episodes": 259, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 51599}
{"episodes": 260, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 51799}
{"episodes": 261, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 51999}
{"episodes": 262, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 52199}
{"episodes": 263, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 52399}
{"episodes": 264, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 52599}
{"episodes": 265, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 52799}
{"episodes": 266, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 52999}
{"episodes": 267, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 53199}
{"episodes": 268, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 53399}
{"episodes": 269, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 53599}
{"episodes": 270, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 53799}
{"episodes": 271, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 53999}
{"episodes": 272, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 54199}
{"episodes": 273, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 54399}
{"episodes": 274, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 54599}
{"episodes": 275, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 54799}
{"episodes": 276, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 54999}
{"episodes": 277, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 55199}
{"episodes": 278, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 55399}
{"episodes": 279, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 55599}
{"episodes": 280, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 55799}
{"episodes": 281, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 55999}
{"episodes": 282, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 56199}
{"episodes": 283, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 56399}
{"episodes": 284, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 56599}
{"episodes": 285, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 56799}
{"episodes": 286, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 56999}
{"episodes": 287, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 57199}
{"episodes": 288, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 57399}
{"episodes": 289, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 57599}
{"episodes": 290, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 57799}
{"episodes": 291, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 57999}
{"episodes": 292, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 58199}
{"episodes": 293, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 58399}
{"episodes": 294, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 58599}
{"episodes": 295, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 58799}
{"episodes": 296, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 58999}
{"episodes": 297, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 59199}
{"episodes": 298, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 59399}
{"episodes": 299, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 59599}
{"episodes": 300, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 59799}
{"episodes": 301, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 59999}
{"episodes": 302, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 60199}
{"episodes": 303, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 60399}
{"episodes": 304, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 60599}
{"episodes": 305, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 60799}
{"episodes": 306, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 60999}
{"episodes": 307, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 61199}
{"episodes": 308, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 61399}
{"episodes": 309, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 61599}
{"episodes": 310, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 61799}
{"episodes": 311, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 61999}
{"episodes": 312, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 62199}
{"episodes": 313, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 62399}
{"episodes": 314, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 62599}
{"episodes": 315, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 62799}
{"episodes": 316, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 62999}
{"episodes": 317, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 63199}
{"episodes": 318, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 63399}
{"episodes": 319, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 63599}
{"episodes": 320, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 63799}
{"episodes": 321, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 63999}
{"episodes": 322, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 64199}
{"episodes": 323, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 64399}
{"episodes": 324, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 64599}
{"episodes": 325, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 64799}
{"episodes": 326, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 64999}
{"episodes": 327, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 65199}
{"episodes": 328, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 65399}
{"episodes": 329, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 65599}
{"episodes": 330, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 65799}
{"episodes": 331, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 65999}
{"episodes": 332, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 66199}
{"episodes": 333, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 66399}
{"episodes": 334, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 66599}
{"episodes": 335, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 66799}
{"episodes": 336, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 66999}
{"episodes": 337, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 67199}
{"episodes": 338, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 67399}
{"episodes": 339, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 67599}
{"episodes": 340, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 67799}
{"episodes": 341, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 67999}
{"episodes": 342, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 68199}
{"episodes": 343, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 68399}
{"episodes": 344, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 68599}
{"episodes": 345, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 68799}
{"episodes": 346, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 68999}
{"episodes": 347, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 69199}
{"episodes": 348, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 69399}
{"episodes": 349, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 69599}
{"episodes": 350, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 69799}
{"episodes": 351, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 69999}
{"episodes": 352, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 70199}
{"episodes": 353, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 70399}
{"episodes": 354, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 70599}
{"episodes": 355, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 70799}
{"episodes": 356, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 70999}
{"episodes": 357, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 71199}
{"episodes": 358, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 71399}
{"episodes": 359, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 71599}
{"episodes": 360, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 71799}
{"episodes": 361, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 71999}
{"episodes": 362, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 72199}
{"episodes": 363, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 72399}
{"episodes": 364, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 72599}
{"episodes": 365, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 72799}
{"episodes": 366, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 72999}
{"episodes": 367, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 73199}
{"episodes": 368, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 73399}
{"episodes": 369, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 73599}
{"episodes": 370, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 73799}
{"episodes": 371, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 73999}
{"episodes": 372, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 74199}
{"episodes": 373, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 74399}
{"episodes": 374, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 74599}
{"episodes": 375, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 74799}
{"episodes": 376, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 74999}
{"episodes": 377, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 75199}
{"episodes": 378, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 75399}
{"episodes": 379, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 75599}
{"episodes": 380, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 75799}
{"episodes": 381, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 75999}
{"episodes": 382, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 76199}
{"episodes": 383, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 76399}
{"episodes": 384, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 76599}
{"episodes": 385, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 76799}
{"episodes": 386, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 76999}
{"episodes": 387, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 77199}
{"episodes": 388, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 77399}
{"episodes": 389, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 77599}
{"episodes": 390, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 77799}
{"episodes": 391, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 77999}
{"episodes": 392, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 78199}
{"episodes": 393, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 78399}
{"episodes": 394, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 78599}
{"episodes": 395, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 78799}
{"episodes": 396, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 78999}
{"episodes": 397, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 79199}
{"episodes": 398, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 79399}
{"episodes": 399, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 79599}
{"episodes": 400, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 79799}
{"episodes": 401, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 79999}
{"episodes": 402, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 80199}
{"episodes": 403, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 80399}
{"episodes": 404, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 80599}
{"episodes": 405, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 80799}
{"episodes": 406, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 80999}
{"episodes": 407, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 81199}
{"episodes": 408, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 81399}
{"episodes": 409, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 81599}
{"episodes": 410, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 81799}
{"episodes": 411, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 81999}
{"episodes": 412, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 82199}
{"episodes": 413, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 82399}
{"episodes": 414, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 82599}
{"episodes": 415, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 82799}
{"episodes": 416, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 82999}
{"episodes": 417, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 83199}
{"episodes": 418, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 83399}
{"episodes": 419, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 83599}
{"episodes": 420, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 83799}
{"episodes": 421, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 83999}
{"episodes": 422, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 84199}
{"episodes": 423, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 84399}
{"episodes": 424, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 84599}
{"episodes": 425, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 84799}
{"episodes": 426, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 84999}
{"episodes": 427, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 85199}
{"episodes": 428, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 85399}
{"episodes": 429, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 85599}
{"episodes": 430, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 85799}
{"episodes": 431, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 85999}
{"episodes": 432, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 86199}
{"episodes": 433, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 86399}
{"episodes": 434, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 86599}
{"episodes": 435, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 86799}
{"episodes": 436, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 86999}
{"episodes": 437, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 87199}
{"episodes": 438, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 87399}
{"episodes": 439, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 87599}
{"episodes": 440, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 87799}
{"episodes": 441, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 87999}
{"episodes": 442, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 88199}
{"episodes": 443, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 88399}
{"episodes": 444, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 88599}
{"episodes": 445, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 88799}
{"episodes": 446, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 88999}
{"episodes": 447, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 89199}
{"episodes": 448, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 89399}
{"episodes": 449, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 89599}
{"episodes": 450, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 89799}
{"episodes": 451, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 89999}
{"episodes": 452, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 90199}
{"episodes": 453, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 90399}
{"episodes": 454, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 90599}
{"episodes": 455, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 90799}
{"episodes": 456, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 90999}
{"episodes": 457, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 91199}
{"episodes": 458, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 91399}
{"episodes": 459, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 91599}
{"episodes": 460, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 91799}
{"episodes": 461, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 91999}
{"episodes": 462, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 92199}
{"episodes": 463, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 92399}
{"episodes": 464, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 92599}
{"episodes": 465, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 92799}
{"episodes": 466, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 92999}
{"episodes": 467, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 93199}
{"episodes": 468, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 93399}
{"episodes": 469, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 93599}
{"episodes": 470, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 93799}
{"episodes": 471, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 93999}
{"episodes": 472, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 94199}
{"episodes": 473, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 94399}
{"episodes": 474, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 94599}
{"episodes": 475, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 94799}
{"episodes": 476, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 94999}
{"episodes": 477, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 95199}
{"episodes": 478, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 95399}
{"episodes": 479, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 95599}
{"episodes": 480, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 95799}
{"episodes": 481, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 95999}
{"episodes": 482, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 96199}
{"episodes": 483, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 96399}
{"episodes": 484, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 96599}
{"episodes": 485, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 96799}
{"episodes": 486, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 96999}
{"episodes": 487, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 97199}
{"episodes": 488, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 97399}
{"episodes": 489, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 97599}
{"episodes": 490, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 97799}
{"episodes": 491, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 97999}
{"episodes": 492, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 98199}
{"episodes": 493, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 98399}
{"episodes": 494, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 98599}
{"episodes": 495, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 98799}
{"episodes": 496, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 98999}
{"episodes": 497, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 99199}
{"episodes": 498, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 99399}
{"episodes": 499, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 99599}
{"episodes": 500, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 99799}
{"episodes": 501, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 99999}
{"episodes": 502, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 100199}
{"episodes": 503, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 100399}
{"episodes": 504, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 100599}
{"episodes": 505, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 100799}
{"episodes": 506, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 100999}
{"episodes": 507, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 101199}
{"episodes": 508, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 101399}
{"episodes": 509, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 101599}
{"episodes": 510, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 101799}
{"episodes": 511, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 101999}
{"episodes": 512, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 102199}
{"episodes": 513, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 102399}
{"episodes": 514, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 102599}
{"episodes": 515, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 102799}
{"episodes": 516, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 102999}
{"episodes": 517, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 103199}
{"episodes": 518, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 103399}
{"episodes": 519, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 103599}
{"episodes": 520, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 103799}
{"episodes": 521, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 103999}
{"episodes": 522, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 104199}
{"episodes": 523, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 104399}
{"episodes": 524, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 104599}
{"episodes": 525, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 104799}
{"episodes": 526, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 104999}
{"episodes": 527, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 105199}
{"episodes": 528, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 105399}
{"episodes": 529, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 105599}
{"episodes": 530, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 105799}
{"episodes": 531, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 105999}
{"episodes": 532, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 106199}
{"episodes": 533, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 106399}
{"episodes": 534, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 106599}
{"episodes": 535, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 106799}
{"episodes": 536, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 106999}
{"episodes": 537, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 107199}
{"episodes": 538, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 107399}
{"episodes": 539, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 107599}
{"episodes": 540, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 107799}
{"episodes": 541, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 107999}
{"episodes": 542, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 108199}
{"episodes": 543, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 108399}
{"episodes": 544, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 108599}
{"episodes": 545, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 108799}
{"episodes": 546, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 108999}
{"episodes": 547, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 109199}
{"episodes": 548, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 109399}
{"episodes": 549, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 109599}
{"episodes": 550, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 109799}
{"episodes": 551, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 109999}
{"episodes": 552, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 110199}
{"episodes": 553, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 110399}
{"episodes": 554, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 110599}
{"episodes": 555, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 110799}
{"episodes": 556, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 110999}
{"episodes": 557, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 111199}
{"episodes": 558, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 111399}
{"episodes": 559, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 111599}
{"episodes": 560, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 111799}
{"episodes": 561, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 111999}
{"episodes": 562, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 112199}
{"episodes": 563, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 112399}
{"episodes": 564, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 112599}
{"episodes": 565, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 112799}
{"episodes": 566, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 112999}
{"episodes": 567, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 113199}
{"episodes": 568, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 113399}
{"episodes": 569, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 113599}
{"episodes": 570, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 113799}
{"episodes": 571, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 113999}
{"episodes": 572, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 114199}
{"episodes": 573, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 114399}
{"episodes": 574, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 114599}
{"episodes": 575, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 114799}
{"episodes": 576, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 114999}
{"episodes": 577, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 115199}
{"episodes": 578, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 115399}
{"episodes": 579, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 115599}
{"episodes": 580, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 115799}
{"episodes": 581, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 115999}
{"episodes": 582, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 116199}
{"episodes": 583, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 116399}
{"episodes": 584, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 116599}
{"episodes": 585, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 116799}
{"episodes": 586, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 116999}
{"episodes": 587, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 117199}
{"episodes": 588, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 117399}
{"episodes": 589, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 117599}
{"episodes": 590, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 117799}
{"episodes": 591, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 117999}
{"episodes": 592, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 118199}
{"episodes": 593, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 118399}
{"episodes": 594, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 118599}
{"episodes": 595, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 118799}
{"episodes": 596, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 118999}
{"episodes": 597, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 119199}
{"episodes": 598, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 119399}
{"episodes": 599, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 119599}
{"episodes": 600, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 119799}
{"episodes": 601, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 119999}
{"episodes": 602, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 120199}
{"episodes": 603, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 120399}
{"episodes": 604, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 120599}
{"episodes": 605, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 120799}
{"episodes": 606, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 120999}
{"episodes": 607, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 121199}
{"episodes": 608, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 121399}
{"episodes": 609, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 121599}
{"episodes": 610, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 121799}
{"episodes": 611, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 121999}
{"episodes": 612, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 122199}
{"episodes": 613, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 122399}
{"episodes": 614, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 122599}
{"episodes": 615, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 122799}
{"episodes": 616, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 122999}
{"episodes": 617, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 123199}
{"episodes": 618, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 123399}
{"episodes": 619, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 123599}
{"episodes": 620, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 123799}
{"episodes": 621, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 123999}
{"episodes": 622, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 124199}
{"episodes": 623, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 124399}
{"episodes": 624, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 124599}
{"episodes": 625, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 124799}
{"episodes": 626, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 124999}
{"episodes": 627, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 125199}
{"episodes": 628, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 125399}
{"episodes": 629, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 125599}
{"episodes": 630, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 125799}
{"episodes": 631, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 125999}
{"episodes": 632, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 126199}
{"episodes": 633, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 126399}
{"episodes": 634, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 126599}
{"episodes": 635, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 126799}
{"episodes": 636, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 126999}
{"episodes": 637, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 127199}
{"episodes": 638, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 127399}
{"episodes": 639, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 127599}
{"episodes": 640, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 127799}
{"episodes": 641, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 127999}
{"episodes": 642, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 128199}
{"episodes": 643, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 128399}
{"episodes": 644, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 128599}
{"episodes": 645, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 128799}
{"episodes": 646, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 128999}
{"episodes": 647, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 129199}
{"episodes": 648, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 129399}
{"episodes": 649, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 129599}
{"episodes": 650, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 129799}
{"episodes": 651, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 129999}
{"episodes": 652, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 130199}
{"episodes": 653, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 130399}
{"episodes": 654, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 130599}
{"episodes": 655, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 130799}
{"episodes": 656, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 130999}
{"episodes": 657, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 131199}
{"episodes": 658, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 131399}
{"episodes": 659, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 131599}
{"episodes": 660, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 131799}
{"episodes": 661, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 131999}
{"episodes": 662, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 132199}
{"episodes": 663, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 132399}
{"episodes": 664, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 132599}
{"episodes": 665, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 132799}
{"episodes": 666, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 132999}
{"episodes": 667, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 133199}
{"episodes": 668, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 133399}
{"episodes": 669, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 133599}
{"episodes": 670, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 133799}
{"episodes": 671, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 133999}
{"episodes": 672, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 134199}
{"episodes": 673, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 134399}
{"episodes": 674, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 134599}
{"episodes": 675, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 134799}
{"episodes": 676, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 134999}
{"episodes": 677, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 135199}
{"episodes": 678, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 135399}
{"episodes": 679, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 135599}
{"episodes": 680, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 135799}
{"episodes": 681, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 135999}
{"episodes": 682, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 136199}
{"episodes": 683, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 136399}
{"episodes": 684, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 136599}
{"episodes": 685, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 136799}
{"episodes": 686, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 136999}
{"episodes": 687, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 137199}
{"episodes": 688, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 137399}
{"episodes": 689, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 137599}
{"episodes": 690, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 137799}
{"episodes": 691, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 137999}
{"episodes": 692, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 138199}
{"episodes": 693, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 138399}
{"episodes": 694, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 138599}
{"episodes": 695, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 138799}
{"episodes": 696, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 138999}
{"episodes": 697, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 139199}
{"episodes": 698, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 139399}
{"episodes": 699, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 139599}
{"episodes": 700, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 139799}
{"episodes": 701, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 139999}
{"episodes": 702, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 140199}
{"episodes": 703, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 140399}
{"episodes": 704, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 140599}
{"episodes": 705, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 140799}
{"episodes": 706, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 140999}
{"episodes": 707, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 141199}
{"episodes": 708, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 141399}
{"episodes": 709, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 141599}
{"episodes": 710, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 141799}
{"episodes": 711, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 141999}
{"episodes": 712, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 142199}
{"episodes": 713, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 142399}
{"episodes": 714, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 142599}
{"episodes": 715, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 142799}
{"episodes": 716, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 142999}
{"episodes": 717, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 143199}
{"episodes": 718, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 143399}
{"episodes": 719, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 143599}
{"episodes": 720, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 143799}
{"episodes": 721, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 143999}
{"episodes": 722, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 144199}
{"episodes": 723, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 144399}
{"episodes": 724, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 144599}
{"episodes": 725, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 144799}
{"episodes": 726, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 144999}
{"episodes": 727, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 145199}
{"episodes": 728, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 145399}
{"episodes": 729, "mean 100 episode reward": -200.0, "reward": -198.0, "% time spent exploring": 9, "steps": 145597}
{"episodes": 730, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 145797}
{"episodes": 731, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 145997}
{"episodes": 732, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 146197}
{"episodes": 733, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 146397}
{"episodes": 734, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 146597}
{"episodes": 735, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 146797}
{"episodes": 736, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 146997}
{"episodes": 737, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 147197}
{"episodes": 738, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 147397}
{"episodes": 739, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 147597}
{"episodes": 740, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 147797}
{"episodes": 741, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 147997}
{"episodes": 742, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 148197}
{"episodes": 743, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 148397}
{"episodes": 744, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 148597}
{"episodes": 745, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 148797}
{"episodes": 746, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 148997}
{"episodes": 747, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 149197}
{"episodes": 748, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 149397}
{"episodes": 749, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 149597}
{"episodes": 750, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 149797}
{"episodes": 751, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 149997}
{"episodes": 752, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 150197}
{"episodes": 753, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 150397}
{"episodes": 754, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 150597}
{"episodes": 755, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 150797}
{"episodes": 756, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 150997}
{"episodes": 757, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 151197}
{"episodes": 758, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 151397}
{"episodes": 759, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 151597}
{"episodes": 760, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 151797}
{"episodes": 761, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 151997}
{"episodes": 762, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 152197}
{"episodes": 763, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 152397}
{"episodes": 764, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 152597}
{"episodes": 765, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 152797}
{"episodes": 766, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 152997}
{"episodes": 767, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 153197}
{"episodes": 768, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 153397}
{"episodes": 769, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 153597}
{"episodes": 770, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 153797}
{"episodes": 771, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 153997}
{"episodes": 772, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 154197}
{"episodes": 773, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 154397}
{"episodes": 774, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 154597}
{"episodes": 775, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 154797}
{"episodes": 776, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 154997}
{"episodes": 777, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 155197}
{"episodes": 778, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 155397}
{"episodes": 779, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 155597}
{"episodes": 780, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 155797}
{"episodes": 781, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 155997}
{"episodes": 782, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 156197}
{"episodes": 783, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 156397}
{"episodes": 784, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 156597}
{"episodes": 785, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 156797}
{"episodes": 786, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 156997}
{"episodes": 787, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 157197}
{"episodes": 788, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 157397}
{"episodes": 789, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 157597}
{"episodes": 790, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 157797}
{"episodes": 791, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 157997}
{"episodes": 792, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 158197}
{"episodes": 793, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 158397}
{"episodes": 794, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 158597}
{"episodes": 795, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 158797}
{"episodes": 796, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 158997}
{"episodes": 797, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 159197}
{"episodes": 798, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 159397}
{"episodes": 799, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 159597}
{"episodes": 800, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 159797}
{"episodes": 801, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 159997}
{"episodes": 802, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 160197}
{"episodes": 803, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 160397}
{"episodes": 804, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 160597}
{"episodes": 805, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 160797}
{"episodes": 806, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 160997}
{"episodes": 807, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 161197}
{"episodes": 808, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 161397}
{"episodes": 809, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 161597}
{"episodes": 810, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 161797}
{"episodes": 811, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 161997}
{"episodes": 812, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 162197}
{"episodes": 813, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 162397}
{"episodes": 814, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 162597}
{"episodes": 815, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 162797}
{"episodes": 816, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 162997}
{"episodes": 817, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 163197}
{"episodes": 818, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 163397}
{"episodes": 819, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 163597}
{"episodes": 820, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 163797}
{"episodes": 821, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 163997}
{"episodes": 822, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 164197}
{"episodes": 823, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 164397}
{"episodes": 824, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 164597}
{"episodes": 825, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 164797}
{"episodes": 826, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 164997}
{"episodes": 827, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 165197}
{"episodes": 828, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 165397}
{"episodes": 829, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 165597}
{"episodes": 830, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 165797}
{"episodes": 831, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 165997}
{"episodes": 832, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 166197}
{"episodes": 833, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 166397}
{"episodes": 834, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 166597}
{"episodes": 835, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 166797}
{"episodes": 836, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 166997}
{"episodes": 837, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 167197}
{"episodes": 838, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 167397}
{"episodes": 839, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 167597}
{"episodes": 840, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 167797}
{"episodes": 841, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 167997}
{"episodes": 842, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 168197}
{"episodes": 843, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 168397}
{"episodes": 844, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 168597}
{"episodes": 845, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 168797}
{"episodes": 846, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 168997}
{"episodes": 847, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 169197}
{"episodes": 848, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 169397}
{"episodes": 849, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 169597}
{"episodes": 850, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 169797}
{"episodes": 851, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 169997}
{"episodes": 852, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 170197}
{"episodes": 853, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 170397}
{"episodes": 854, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 170597}
{"episodes": 855, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 170797}
{"episodes": 856, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 170997}
{"episodes": 857, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 171197}
{"episodes": 858, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 171397}
{"episodes": 859, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 171597}
{"episodes": 860, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 171797}
{"episodes": 861, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 171997}
{"episodes": 862, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 172197}
{"episodes": 863, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 172397}
{"episodes": 864, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 172597}
{"episodes": 865, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 172797}
{"episodes": 866, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 172997}
{"episodes": 867, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 173197}
{"episodes": 868, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 173397}
{"episodes": 869, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 173597}
{"episodes": 870, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 173797}
{"episodes": 871, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 173997}
{"episodes": 872, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 174197}
{"episodes": 873, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 174397}
{"episodes": 874, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 174597}
{"episodes": 875, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 174797}
{"episodes": 876, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 174997}
{"episodes": 877, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 175197}
{"episodes": 878, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 175397}
{"episodes": 879, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 175597}
{"episodes": 880, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 175797}
{"episodes": 881, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 175997}
{"episodes": 882, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 176197}
{"episodes": 883, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 176397}
{"episodes": 884, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 176597}
{"episodes": 885, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 176797}
{"episodes": 886, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 176997}
{"episodes": 887, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 177197}
{"episodes": 888, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 177397}
{"episodes": 889, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 177597}
{"episodes": 890, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 177797}
{"episodes": 891, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 177997}
{"episodes": 892, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 178197}
{"episodes": 893, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 178397}
{"episodes": 894, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 178597}
{"episodes": 895, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 178797}
{"episodes": 896, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 178997}
{"episodes": 897, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 179197}
{"episodes": 898, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 179397}
{"episodes": 899, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 179597}
{"episodes": 900, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 179797}
{"episodes": 901, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 179997}
{"episodes": 902, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 180197}
{"episodes": 903, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 180397}
{"episodes": 904, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 180597}
{"episodes": 905, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 180797}
{"episodes": 906, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 180997}
{"episodes": 907, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 181197}
{"episodes": 908, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 181397}
{"episodes": 909, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 181597}
{"episodes": 910, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 181797}
{"episodes": 911, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 181997}
{"episodes": 912, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 182197}
{"episodes": 913, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 182397}
{"episodes": 914, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 182597}
{"episodes": 915, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 182797}
{"episodes": 916, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 182997}
{"episodes": 917, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 183197}
{"episodes": 918, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 183397}
{"episodes": 919, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 183597}
{"episodes": 920, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 183797}
{"episodes": 921, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 183997}
{"episodes": 922, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 184197}
{"episodes": 923, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 184397}
{"episodes": 924, "mean 100 episode reward": -200.0, "reward": -200.0, "% time spent exploring": 9, "steps": 184597}
