{"reward": -200.0, "% time spent exploring": 10, "episodes": 2, "mean 100 episode reward": -200.0, "steps": 199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 3, "mean 100 episode reward": -200.0, "steps": 399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 4, "mean 100 episode reward": -200.0, "steps": 599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 5, "mean 100 episode reward": -200.0, "steps": 799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 6, "mean 100 episode reward": -200.0, "steps": 999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 7, "mean 100 episode reward": -200.0, "steps": 1199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 8, "mean 100 episode reward": -200.0, "steps": 1399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 9, "mean 100 episode reward": -200.0, "steps": 1599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 10, "mean 100 episode reward": -200.0, "steps": 1799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 11, "mean 100 episode reward": -200.0, "steps": 1999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 12, "mean 100 episode reward": -200.0, "steps": 2199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 13, "mean 100 episode reward": -200.0, "steps": 2399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 14, "mean 100 episode reward": -200.0, "steps": 2599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 15, "mean 100 episode reward": -200.0, "steps": 2799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 16, "mean 100 episode reward": -200.0, "steps": 2999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 17, "mean 100 episode reward": -200.0, "steps": 3199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 18, "mean 100 episode reward": -200.0, "steps": 3399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 19, "mean 100 episode reward": -200.0, "steps": 3599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 20, "mean 100 episode reward": -200.0, "steps": 3799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 21, "mean 100 episode reward": -200.0, "steps": 3999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 22, "mean 100 episode reward": -200.0, "steps": 4199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 23, "mean 100 episode reward": -200.0, "steps": 4399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 24, "mean 100 episode reward": -200.0, "steps": 4599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 25, "mean 100 episode reward": -200.0, "steps": 4799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 26, "mean 100 episode reward": -200.0, "steps": 4999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 27, "mean 100 episode reward": -200.0, "steps": 5199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 28, "mean 100 episode reward": -200.0, "steps": 5399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 29, "mean 100 episode reward": -200.0, "steps": 5599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 30, "mean 100 episode reward": -200.0, "steps": 5799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 31, "mean 100 episode reward": -200.0, "steps": 5999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 32, "mean 100 episode reward": -200.0, "steps": 6199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 33, "mean 100 episode reward": -200.0, "steps": 6399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 34, "mean 100 episode reward": -200.0, "steps": 6599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 35, "mean 100 episode reward": -200.0, "steps": 6799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 36, "mean 100 episode reward": -200.0, "steps": 6999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 37, "mean 100 episode reward": -200.0, "steps": 7199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 38, "mean 100 episode reward": -200.0, "steps": 7399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 39, "mean 100 episode reward": -200.0, "steps": 7599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 40, "mean 100 episode reward": -200.0, "steps": 7799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 41, "mean 100 episode reward": -200.0, "steps": 7999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 42, "mean 100 episode reward": -200.0, "steps": 8199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 43, "mean 100 episode reward": -200.0, "steps": 8399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 44, "mean 100 episode reward": -200.0, "steps": 8599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 45, "mean 100 episode reward": -200.0, "steps": 8799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 46, "mean 100 episode reward": -200.0, "steps": 8999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 47, "mean 100 episode reward": -200.0, "steps": 9199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 48, "mean 100 episode reward": -200.0, "steps": 9399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 49, "mean 100 episode reward": -200.0, "steps": 9599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 50, "mean 100 episode reward": -200.0, "steps": 9799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 51, "mean 100 episode reward": -200.0, "steps": 9999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 52, "mean 100 episode reward": -200.0, "steps": 10199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 53, "mean 100 episode reward": -200.0, "steps": 10399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 54, "mean 100 episode reward": -200.0, "steps": 10599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 55, "mean 100 episode reward": -200.0, "steps": 10799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 56, "mean 100 episode reward": -200.0, "steps": 10999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 57, "mean 100 episode reward": -200.0, "steps": 11199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 58, "mean 100 episode reward": -200.0, "steps": 11399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 59, "mean 100 episode reward": -200.0, "steps": 11599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 60, "mean 100 episode reward": -200.0, "steps": 11799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 61, "mean 100 episode reward": -200.0, "steps": 11999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 62, "mean 100 episode reward": -200.0, "steps": 12199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 63, "mean 100 episode reward": -200.0, "steps": 12399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 64, "mean 100 episode reward": -200.0, "steps": 12599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 65, "mean 100 episode reward": -200.0, "steps": 12799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 66, "mean 100 episode reward": -200.0, "steps": 12999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 67, "mean 100 episode reward": -200.0, "steps": 13199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 68, "mean 100 episode reward": -200.0, "steps": 13399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 69, "mean 100 episode reward": -200.0, "steps": 13599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 70, "mean 100 episode reward": -200.0, "steps": 13799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 71, "mean 100 episode reward": -200.0, "steps": 13999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 72, "mean 100 episode reward": -200.0, "steps": 14199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 73, "mean 100 episode reward": -200.0, "steps": 14399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 74, "mean 100 episode reward": -200.0, "steps": 14599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 75, "mean 100 episode reward": -200.0, "steps": 14799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 76, "mean 100 episode reward": -200.0, "steps": 14999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 77, "mean 100 episode reward": -200.0, "steps": 15199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 78, "mean 100 episode reward": -200.0, "steps": 15399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 79, "mean 100 episode reward": -200.0, "steps": 15599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 80, "mean 100 episode reward": -200.0, "steps": 15799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 81, "mean 100 episode reward": -200.0, "steps": 15999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 82, "mean 100 episode reward": -200.0, "steps": 16199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 83, "mean 100 episode reward": -200.0, "steps": 16399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 84, "mean 100 episode reward": -200.0, "steps": 16599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 85, "mean 100 episode reward": -200.0, "steps": 16799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 86, "mean 100 episode reward": -200.0, "steps": 16999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 87, "mean 100 episode reward": -200.0, "steps": 17199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 88, "mean 100 episode reward": -200.0, "steps": 17399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 89, "mean 100 episode reward": -200.0, "steps": 17599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 90, "mean 100 episode reward": -200.0, "steps": 17799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 91, "mean 100 episode reward": -200.0, "steps": 17999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 92, "mean 100 episode reward": -200.0, "steps": 18199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 93, "mean 100 episode reward": -200.0, "steps": 18399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 94, "mean 100 episode reward": -200.0, "steps": 18599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 95, "mean 100 episode reward": -200.0, "steps": 18799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 96, "mean 100 episode reward": -200.0, "steps": 18999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 97, "mean 100 episode reward": -200.0, "steps": 19199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 98, "mean 100 episode reward": -200.0, "steps": 19399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 99, "mean 100 episode reward": -200.0, "steps": 19599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 100, "mean 100 episode reward": -200.0, "steps": 19799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 101, "mean 100 episode reward": -200.0, "steps": 19999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 102, "mean 100 episode reward": -200.0, "steps": 20199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 103, "mean 100 episode reward": -200.0, "steps": 20399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 104, "mean 100 episode reward": -200.0, "steps": 20599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 105, "mean 100 episode reward": -200.0, "steps": 20799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 106, "mean 100 episode reward": -200.0, "steps": 20999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 107, "mean 100 episode reward": -200.0, "steps": 21199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 108, "mean 100 episode reward": -200.0, "steps": 21399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 109, "mean 100 episode reward": -200.0, "steps": 21599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 110, "mean 100 episode reward": -200.0, "steps": 21799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 111, "mean 100 episode reward": -200.0, "steps": 21999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 112, "mean 100 episode reward": -200.0, "steps": 22199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 113, "mean 100 episode reward": -200.0, "steps": 22399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 114, "mean 100 episode reward": -200.0, "steps": 22599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 115, "mean 100 episode reward": -200.0, "steps": 22799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 116, "mean 100 episode reward": -200.0, "steps": 22999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 117, "mean 100 episode reward": -200.0, "steps": 23199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 118, "mean 100 episode reward": -200.0, "steps": 23399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 119, "mean 100 episode reward": -200.0, "steps": 23599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 120, "mean 100 episode reward": -200.0, "steps": 23799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 121, "mean 100 episode reward": -200.0, "steps": 23999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 122, "mean 100 episode reward": -200.0, "steps": 24199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 123, "mean 100 episode reward": -200.0, "steps": 24399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 124, "mean 100 episode reward": -200.0, "steps": 24599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 125, "mean 100 episode reward": -200.0, "steps": 24799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 126, "mean 100 episode reward": -200.0, "steps": 24999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 127, "mean 100 episode reward": -200.0, "steps": 25199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 128, "mean 100 episode reward": -200.0, "steps": 25399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 129, "mean 100 episode reward": -200.0, "steps": 25599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 130, "mean 100 episode reward": -200.0, "steps": 25799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 131, "mean 100 episode reward": -200.0, "steps": 25999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 132, "mean 100 episode reward": -200.0, "steps": 26199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 133, "mean 100 episode reward": -200.0, "steps": 26399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 134, "mean 100 episode reward": -200.0, "steps": 26599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 135, "mean 100 episode reward": -200.0, "steps": 26799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 136, "mean 100 episode reward": -200.0, "steps": 26999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 137, "mean 100 episode reward": -200.0, "steps": 27199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 138, "mean 100 episode reward": -200.0, "steps": 27399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 139, "mean 100 episode reward": -200.0, "steps": 27599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 140, "mean 100 episode reward": -200.0, "steps": 27799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 141, "mean 100 episode reward": -200.0, "steps": 27999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 142, "mean 100 episode reward": -200.0, "steps": 28199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 143, "mean 100 episode reward": -200.0, "steps": 28399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 144, "mean 100 episode reward": -200.0, "steps": 28599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 145, "mean 100 episode reward": -200.0, "steps": 28799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 146, "mean 100 episode reward": -200.0, "steps": 28999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 147, "mean 100 episode reward": -200.0, "steps": 29199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 148, "mean 100 episode reward": -200.0, "steps": 29399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 149, "mean 100 episode reward": -200.0, "steps": 29599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 150, "mean 100 episode reward": -200.0, "steps": 29799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 151, "mean 100 episode reward": -200.0, "steps": 29999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 152, "mean 100 episode reward": -200.0, "steps": 30199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 153, "mean 100 episode reward": -200.0, "steps": 30399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 154, "mean 100 episode reward": -200.0, "steps": 30599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 155, "mean 100 episode reward": -200.0, "steps": 30799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 156, "mean 100 episode reward": -200.0, "steps": 30999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 157, "mean 100 episode reward": -200.0, "steps": 31199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 158, "mean 100 episode reward": -200.0, "steps": 31399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 159, "mean 100 episode reward": -200.0, "steps": 31599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 160, "mean 100 episode reward": -200.0, "steps": 31799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 161, "mean 100 episode reward": -200.0, "steps": 31999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 162, "mean 100 episode reward": -200.0, "steps": 32199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 163, "mean 100 episode reward": -200.0, "steps": 32399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 164, "mean 100 episode reward": -200.0, "steps": 32599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 165, "mean 100 episode reward": -200.0, "steps": 32799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 166, "mean 100 episode reward": -200.0, "steps": 32999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 167, "mean 100 episode reward": -200.0, "steps": 33199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 168, "mean 100 episode reward": -200.0, "steps": 33399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 169, "mean 100 episode reward": -200.0, "steps": 33599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 170, "mean 100 episode reward": -200.0, "steps": 33799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 171, "mean 100 episode reward": -200.0, "steps": 33999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 172, "mean 100 episode reward": -200.0, "steps": 34199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 173, "mean 100 episode reward": -200.0, "steps": 34399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 174, "mean 100 episode reward": -200.0, "steps": 34599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 175, "mean 100 episode reward": -200.0, "steps": 34799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 176, "mean 100 episode reward": -200.0, "steps": 34999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 177, "mean 100 episode reward": -200.0, "steps": 35199}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 178, "mean 100 episode reward": -200.0, "steps": 35399}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 179, "mean 100 episode reward": -200.0, "steps": 35599}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 180, "mean 100 episode reward": -200.0, "steps": 35799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 181, "mean 100 episode reward": -200.0, "steps": 35999}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 182, "mean 100 episode reward": -200.0, "steps": 36199}
