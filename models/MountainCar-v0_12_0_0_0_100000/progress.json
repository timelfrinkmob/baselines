{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 2, "steps": 199, "% time spent exploring": 98}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 3, "steps": 399, "% time spent exploring": 96}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 4, "steps": 599, "% time spent exploring": 94}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 5, "steps": 799, "% time spent exploring": 92}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 6, "steps": 999, "% time spent exploring": 91}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 7, "steps": 1199, "% time spent exploring": 89}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 8, "steps": 1399, "% time spent exploring": 87}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 9, "steps": 1599, "% time spent exploring": 85}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 10, "steps": 1799, "% time spent exploring": 83}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 11, "steps": 1999, "% time spent exploring": 82}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 12, "steps": 2199, "% time spent exploring": 80}
{"reward": -200.0, "mean 100 episode reward": -200.0, "episodes": 13, "steps": 2399, "% time spent exploring": 78}
