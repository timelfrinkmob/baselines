{"reward": -200.0, "% time spent exploring": 98, "episodes": 2, "mean 100 episode reward": -200.0, "steps": 199}
{"reward": -200.0, "% time spent exploring": 96, "episodes": 3, "mean 100 episode reward": -200.0, "steps": 399}
{"reward": -200.0, "% time spent exploring": 94, "episodes": 4, "mean 100 episode reward": -200.0, "steps": 599}
{"reward": -200.0, "% time spent exploring": 92, "episodes": 5, "mean 100 episode reward": -200.0, "steps": 799}
{"reward": -200.0, "% time spent exploring": 91, "episodes": 6, "mean 100 episode reward": -200.0, "steps": 999}
{"reward": -200.0, "% time spent exploring": 89, "episodes": 7, "mean 100 episode reward": -200.0, "steps": 1199}
{"reward": -200.0, "% time spent exploring": 87, "episodes": 8, "mean 100 episode reward": -200.0, "steps": 1399}
{"reward": -200.0, "% time spent exploring": 85, "episodes": 9, "mean 100 episode reward": -200.0, "steps": 1599}
{"reward": -200.0, "% time spent exploring": 83, "episodes": 10, "mean 100 episode reward": -200.0, "steps": 1799}
{"reward": -200.0, "% time spent exploring": 82, "episodes": 11, "mean 100 episode reward": -200.0, "steps": 1999}
{"reward": -200.0, "% time spent exploring": 80, "episodes": 12, "mean 100 episode reward": -200.0, "steps": 2199}
{"reward": -200.0, "% time spent exploring": 78, "episodes": 13, "mean 100 episode reward": -200.0, "steps": 2399}
{"reward": -200.0, "% time spent exploring": 76, "episodes": 14, "mean 100 episode reward": -200.0, "steps": 2599}
{"reward": -200.0, "% time spent exploring": 74, "episodes": 15, "mean 100 episode reward": -200.0, "steps": 2799}
{"reward": -200.0, "% time spent exploring": 73, "episodes": 16, "mean 100 episode reward": -200.0, "steps": 2999}
{"reward": -200.0, "% time spent exploring": 71, "episodes": 17, "mean 100 episode reward": -200.0, "steps": 3199}
{"reward": -200.0, "% time spent exploring": 69, "episodes": 18, "mean 100 episode reward": -200.0, "steps": 3399}
{"reward": -200.0, "% time spent exploring": 67, "episodes": 19, "mean 100 episode reward": -200.0, "steps": 3599}
{"reward": -200.0, "% time spent exploring": 65, "episodes": 20, "mean 100 episode reward": -200.0, "steps": 3799}
{"reward": -200.0, "% time spent exploring": 64, "episodes": 21, "mean 100 episode reward": -200.0, "steps": 3999}
{"reward": -200.0, "% time spent exploring": 62, "episodes": 22, "mean 100 episode reward": -200.0, "steps": 4199}
{"reward": -200.0, "% time spent exploring": 60, "episodes": 23, "mean 100 episode reward": -200.0, "steps": 4399}
{"reward": -200.0, "% time spent exploring": 58, "episodes": 24, "mean 100 episode reward": -200.0, "steps": 4599}
{"reward": -200.0, "% time spent exploring": 56, "episodes": 25, "mean 100 episode reward": -200.0, "steps": 4799}
{"reward": -200.0, "% time spent exploring": 55, "episodes": 26, "mean 100 episode reward": -200.0, "steps": 4999}
{"reward": -200.0, "% time spent exploring": 53, "episodes": 27, "mean 100 episode reward": -200.0, "steps": 5199}
{"reward": -200.0, "% time spent exploring": 51, "episodes": 28, "mean 100 episode reward": -200.0, "steps": 5399}
{"reward": -200.0, "% time spent exploring": 49, "episodes": 29, "mean 100 episode reward": -200.0, "steps": 5599}
{"reward": -200.0, "% time spent exploring": 47, "episodes": 30, "mean 100 episode reward": -200.0, "steps": 5799}
{"reward": -200.0, "% time spent exploring": 46, "episodes": 31, "mean 100 episode reward": -200.0, "steps": 5999}
{"reward": -200.0, "% time spent exploring": 44, "episodes": 32, "mean 100 episode reward": -200.0, "steps": 6199}
{"reward": -200.0, "% time spent exploring": 42, "episodes": 33, "mean 100 episode reward": -200.0, "steps": 6399}
{"reward": -200.0, "% time spent exploring": 40, "episodes": 34, "mean 100 episode reward": -200.0, "steps": 6599}
{"reward": -200.0, "% time spent exploring": 38, "episodes": 35, "mean 100 episode reward": -200.0, "steps": 6799}
{"reward": -200.0, "% time spent exploring": 37, "episodes": 36, "mean 100 episode reward": -200.0, "steps": 6999}
{"reward": -200.0, "% time spent exploring": 35, "episodes": 37, "mean 100 episode reward": -200.0, "steps": 7199}
{"reward": -200.0, "% time spent exploring": 33, "episodes": 38, "mean 100 episode reward": -200.0, "steps": 7399}
{"reward": -200.0, "% time spent exploring": 31, "episodes": 39, "mean 100 episode reward": -200.0, "steps": 7599}
{"reward": -200.0, "% time spent exploring": 29, "episodes": 40, "mean 100 episode reward": -200.0, "steps": 7799}
{"reward": -200.0, "% time spent exploring": 28, "episodes": 41, "mean 100 episode reward": -200.0, "steps": 7999}
{"reward": -200.0, "% time spent exploring": 26, "episodes": 42, "mean 100 episode reward": -200.0, "steps": 8199}
{"reward": -200.0, "% time spent exploring": 24, "episodes": 43, "mean 100 episode reward": -200.0, "steps": 8399}
{"reward": -200.0, "% time spent exploring": 22, "episodes": 44, "mean 100 episode reward": -200.0, "steps": 8599}
{"reward": -200.0, "% time spent exploring": 20, "episodes": 45, "mean 100 episode reward": -200.0, "steps": 8799}
{"reward": -200.0, "% time spent exploring": 19, "episodes": 46, "mean 100 episode reward": -200.0, "steps": 8999}
{"reward": -200.0, "% time spent exploring": 17, "episodes": 47, "mean 100 episode reward": -200.0, "steps": 9199}
{"reward": -200.0, "% time spent exploring": 15, "episodes": 48, "mean 100 episode reward": -200.0, "steps": 9399}
{"reward": -200.0, "% time spent exploring": 13, "episodes": 49, "mean 100 episode reward": -200.0, "steps": 9599}
{"reward": -200.0, "% time spent exploring": 11, "episodes": 50, "mean 100 episode reward": -200.0, "steps": 9799}
{"reward": -200.0, "% time spent exploring": 10, "episodes": 51, "mean 100 episode reward": -200.0, "steps": 9999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 52, "mean 100 episode reward": -200.0, "steps": 10199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 53, "mean 100 episode reward": -200.0, "steps": 10399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 54, "mean 100 episode reward": -200.0, "steps": 10599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 55, "mean 100 episode reward": -200.0, "steps": 10799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 56, "mean 100 episode reward": -200.0, "steps": 10999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 57, "mean 100 episode reward": -200.0, "steps": 11199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 58, "mean 100 episode reward": -200.0, "steps": 11399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 59, "mean 100 episode reward": -200.0, "steps": 11599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 60, "mean 100 episode reward": -200.0, "steps": 11799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 61, "mean 100 episode reward": -200.0, "steps": 11999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 62, "mean 100 episode reward": -200.0, "steps": 12199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 63, "mean 100 episode reward": -200.0, "steps": 12399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 64, "mean 100 episode reward": -200.0, "steps": 12599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 65, "mean 100 episode reward": -200.0, "steps": 12799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 66, "mean 100 episode reward": -200.0, "steps": 12999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 67, "mean 100 episode reward": -200.0, "steps": 13199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 68, "mean 100 episode reward": -200.0, "steps": 13399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 69, "mean 100 episode reward": -200.0, "steps": 13599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 70, "mean 100 episode reward": -200.0, "steps": 13799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 71, "mean 100 episode reward": -200.0, "steps": 13999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 72, "mean 100 episode reward": -200.0, "steps": 14199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 73, "mean 100 episode reward": -200.0, "steps": 14399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 74, "mean 100 episode reward": -200.0, "steps": 14599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 75, "mean 100 episode reward": -200.0, "steps": 14799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 76, "mean 100 episode reward": -200.0, "steps": 14999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 77, "mean 100 episode reward": -200.0, "steps": 15199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 78, "mean 100 episode reward": -200.0, "steps": 15399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 79, "mean 100 episode reward": -200.0, "steps": 15599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 80, "mean 100 episode reward": -200.0, "steps": 15799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 81, "mean 100 episode reward": -200.0, "steps": 15999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 82, "mean 100 episode reward": -200.0, "steps": 16199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 83, "mean 100 episode reward": -200.0, "steps": 16399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 84, "mean 100 episode reward": -200.0, "steps": 16599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 85, "mean 100 episode reward": -200.0, "steps": 16799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 86, "mean 100 episode reward": -200.0, "steps": 16999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 87, "mean 100 episode reward": -200.0, "steps": 17199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 88, "mean 100 episode reward": -200.0, "steps": 17399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 89, "mean 100 episode reward": -200.0, "steps": 17599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 90, "mean 100 episode reward": -200.0, "steps": 17799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 91, "mean 100 episode reward": -200.0, "steps": 17999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 92, "mean 100 episode reward": -200.0, "steps": 18199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 93, "mean 100 episode reward": -200.0, "steps": 18399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 94, "mean 100 episode reward": -200.0, "steps": 18599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 95, "mean 100 episode reward": -200.0, "steps": 18799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 96, "mean 100 episode reward": -200.0, "steps": 18999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 97, "mean 100 episode reward": -200.0, "steps": 19199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 98, "mean 100 episode reward": -200.0, "steps": 19399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 99, "mean 100 episode reward": -200.0, "steps": 19599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 100, "mean 100 episode reward": -200.0, "steps": 19799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 101, "mean 100 episode reward": -200.0, "steps": 19999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 102, "mean 100 episode reward": -200.0, "steps": 20199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 103, "mean 100 episode reward": -200.0, "steps": 20399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 104, "mean 100 episode reward": -200.0, "steps": 20599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 105, "mean 100 episode reward": -200.0, "steps": 20799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 106, "mean 100 episode reward": -200.0, "steps": 20999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 107, "mean 100 episode reward": -200.0, "steps": 21199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 108, "mean 100 episode reward": -200.0, "steps": 21399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 109, "mean 100 episode reward": -200.0, "steps": 21599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 110, "mean 100 episode reward": -200.0, "steps": 21799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 111, "mean 100 episode reward": -200.0, "steps": 21999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 112, "mean 100 episode reward": -200.0, "steps": 22199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 113, "mean 100 episode reward": -200.0, "steps": 22399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 114, "mean 100 episode reward": -200.0, "steps": 22599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 115, "mean 100 episode reward": -200.0, "steps": 22799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 116, "mean 100 episode reward": -200.0, "steps": 22999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 117, "mean 100 episode reward": -200.0, "steps": 23199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 118, "mean 100 episode reward": -200.0, "steps": 23399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 119, "mean 100 episode reward": -200.0, "steps": 23599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 120, "mean 100 episode reward": -200.0, "steps": 23799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 121, "mean 100 episode reward": -200.0, "steps": 23999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 122, "mean 100 episode reward": -200.0, "steps": 24199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 123, "mean 100 episode reward": -200.0, "steps": 24399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 124, "mean 100 episode reward": -200.0, "steps": 24599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 125, "mean 100 episode reward": -200.0, "steps": 24799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 126, "mean 100 episode reward": -200.0, "steps": 24999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 127, "mean 100 episode reward": -200.0, "steps": 25199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 128, "mean 100 episode reward": -200.0, "steps": 25399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 129, "mean 100 episode reward": -200.0, "steps": 25599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 130, "mean 100 episode reward": -200.0, "steps": 25799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 131, "mean 100 episode reward": -200.0, "steps": 25999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 132, "mean 100 episode reward": -200.0, "steps": 26199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 133, "mean 100 episode reward": -200.0, "steps": 26399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 134, "mean 100 episode reward": -200.0, "steps": 26599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 135, "mean 100 episode reward": -200.0, "steps": 26799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 136, "mean 100 episode reward": -200.0, "steps": 26999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 137, "mean 100 episode reward": -200.0, "steps": 27199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 138, "mean 100 episode reward": -200.0, "steps": 27399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 139, "mean 100 episode reward": -200.0, "steps": 27599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 140, "mean 100 episode reward": -200.0, "steps": 27799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 141, "mean 100 episode reward": -200.0, "steps": 27999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 142, "mean 100 episode reward": -200.0, "steps": 28199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 143, "mean 100 episode reward": -200.0, "steps": 28399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 144, "mean 100 episode reward": -200.0, "steps": 28599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 145, "mean 100 episode reward": -200.0, "steps": 28799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 146, "mean 100 episode reward": -200.0, "steps": 28999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 147, "mean 100 episode reward": -200.0, "steps": 29199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 148, "mean 100 episode reward": -200.0, "steps": 29399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 149, "mean 100 episode reward": -200.0, "steps": 29599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 150, "mean 100 episode reward": -200.0, "steps": 29799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 151, "mean 100 episode reward": -200.0, "steps": 29999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 152, "mean 100 episode reward": -200.0, "steps": 30199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 153, "mean 100 episode reward": -200.0, "steps": 30399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 154, "mean 100 episode reward": -200.0, "steps": 30599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 155, "mean 100 episode reward": -200.0, "steps": 30799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 156, "mean 100 episode reward": -200.0, "steps": 30999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 157, "mean 100 episode reward": -200.0, "steps": 31199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 158, "mean 100 episode reward": -200.0, "steps": 31399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 159, "mean 100 episode reward": -200.0, "steps": 31599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 160, "mean 100 episode reward": -200.0, "steps": 31799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 161, "mean 100 episode reward": -200.0, "steps": 31999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 162, "mean 100 episode reward": -200.0, "steps": 32199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 163, "mean 100 episode reward": -200.0, "steps": 32399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 164, "mean 100 episode reward": -200.0, "steps": 32599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 165, "mean 100 episode reward": -200.0, "steps": 32799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 166, "mean 100 episode reward": -200.0, "steps": 32999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 167, "mean 100 episode reward": -200.0, "steps": 33199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 168, "mean 100 episode reward": -200.0, "steps": 33399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 169, "mean 100 episode reward": -200.0, "steps": 33599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 170, "mean 100 episode reward": -200.0, "steps": 33799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 171, "mean 100 episode reward": -200.0, "steps": 33999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 172, "mean 100 episode reward": -200.0, "steps": 34199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 173, "mean 100 episode reward": -200.0, "steps": 34399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 174, "mean 100 episode reward": -200.0, "steps": 34599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 175, "mean 100 episode reward": -200.0, "steps": 34799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 176, "mean 100 episode reward": -200.0, "steps": 34999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 177, "mean 100 episode reward": -200.0, "steps": 35199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 178, "mean 100 episode reward": -200.0, "steps": 35399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 179, "mean 100 episode reward": -200.0, "steps": 35599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 180, "mean 100 episode reward": -200.0, "steps": 35799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 181, "mean 100 episode reward": -200.0, "steps": 35999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 182, "mean 100 episode reward": -200.0, "steps": 36199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 183, "mean 100 episode reward": -200.0, "steps": 36399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 184, "mean 100 episode reward": -200.0, "steps": 36599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 185, "mean 100 episode reward": -200.0, "steps": 36799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 186, "mean 100 episode reward": -200.0, "steps": 36999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 187, "mean 100 episode reward": -200.0, "steps": 37199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 188, "mean 100 episode reward": -200.0, "steps": 37399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 189, "mean 100 episode reward": -200.0, "steps": 37599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 190, "mean 100 episode reward": -200.0, "steps": 37799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 191, "mean 100 episode reward": -200.0, "steps": 37999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 192, "mean 100 episode reward": -200.0, "steps": 38199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 193, "mean 100 episode reward": -200.0, "steps": 38399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 194, "mean 100 episode reward": -200.0, "steps": 38599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 195, "mean 100 episode reward": -200.0, "steps": 38799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 196, "mean 100 episode reward": -200.0, "steps": 38999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 197, "mean 100 episode reward": -200.0, "steps": 39199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 198, "mean 100 episode reward": -200.0, "steps": 39399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 199, "mean 100 episode reward": -200.0, "steps": 39599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 200, "mean 100 episode reward": -200.0, "steps": 39799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 201, "mean 100 episode reward": -200.0, "steps": 39999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 202, "mean 100 episode reward": -200.0, "steps": 40199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 203, "mean 100 episode reward": -200.0, "steps": 40399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 204, "mean 100 episode reward": -200.0, "steps": 40599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 205, "mean 100 episode reward": -200.0, "steps": 40799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 206, "mean 100 episode reward": -200.0, "steps": 40999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 207, "mean 100 episode reward": -200.0, "steps": 41199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 208, "mean 100 episode reward": -200.0, "steps": 41399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 209, "mean 100 episode reward": -200.0, "steps": 41599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 210, "mean 100 episode reward": -200.0, "steps": 41799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 211, "mean 100 episode reward": -200.0, "steps": 41999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 212, "mean 100 episode reward": -200.0, "steps": 42199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 213, "mean 100 episode reward": -200.0, "steps": 42399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 214, "mean 100 episode reward": -200.0, "steps": 42599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 215, "mean 100 episode reward": -200.0, "steps": 42799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 216, "mean 100 episode reward": -200.0, "steps": 42999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 217, "mean 100 episode reward": -200.0, "steps": 43199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 218, "mean 100 episode reward": -200.0, "steps": 43399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 219, "mean 100 episode reward": -200.0, "steps": 43599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 220, "mean 100 episode reward": -200.0, "steps": 43799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 221, "mean 100 episode reward": -200.0, "steps": 43999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 222, "mean 100 episode reward": -200.0, "steps": 44199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 223, "mean 100 episode reward": -200.0, "steps": 44399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 224, "mean 100 episode reward": -200.0, "steps": 44599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 225, "mean 100 episode reward": -200.0, "steps": 44799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 226, "mean 100 episode reward": -200.0, "steps": 44999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 227, "mean 100 episode reward": -200.0, "steps": 45199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 228, "mean 100 episode reward": -200.0, "steps": 45399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 229, "mean 100 episode reward": -200.0, "steps": 45599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 230, "mean 100 episode reward": -200.0, "steps": 45799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 231, "mean 100 episode reward": -200.0, "steps": 45999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 232, "mean 100 episode reward": -200.0, "steps": 46199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 233, "mean 100 episode reward": -200.0, "steps": 46399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 234, "mean 100 episode reward": -200.0, "steps": 46599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 235, "mean 100 episode reward": -200.0, "steps": 46799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 236, "mean 100 episode reward": -200.0, "steps": 46999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 237, "mean 100 episode reward": -200.0, "steps": 47199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 238, "mean 100 episode reward": -200.0, "steps": 47399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 239, "mean 100 episode reward": -200.0, "steps": 47599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 240, "mean 100 episode reward": -200.0, "steps": 47799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 241, "mean 100 episode reward": -200.0, "steps": 47999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 242, "mean 100 episode reward": -200.0, "steps": 48199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 243, "mean 100 episode reward": -200.0, "steps": 48399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 244, "mean 100 episode reward": -200.0, "steps": 48599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 245, "mean 100 episode reward": -200.0, "steps": 48799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 246, "mean 100 episode reward": -200.0, "steps": 48999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 247, "mean 100 episode reward": -200.0, "steps": 49199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 248, "mean 100 episode reward": -200.0, "steps": 49399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 249, "mean 100 episode reward": -200.0, "steps": 49599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 250, "mean 100 episode reward": -200.0, "steps": 49799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 251, "mean 100 episode reward": -200.0, "steps": 49999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 252, "mean 100 episode reward": -200.0, "steps": 50199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 253, "mean 100 episode reward": -200.0, "steps": 50399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 254, "mean 100 episode reward": -200.0, "steps": 50599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 255, "mean 100 episode reward": -200.0, "steps": 50799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 256, "mean 100 episode reward": -200.0, "steps": 50999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 257, "mean 100 episode reward": -200.0, "steps": 51199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 258, "mean 100 episode reward": -200.0, "steps": 51399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 259, "mean 100 episode reward": -200.0, "steps": 51599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 260, "mean 100 episode reward": -200.0, "steps": 51799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 261, "mean 100 episode reward": -200.0, "steps": 51999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 262, "mean 100 episode reward": -200.0, "steps": 52199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 263, "mean 100 episode reward": -200.0, "steps": 52399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 264, "mean 100 episode reward": -200.0, "steps": 52599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 265, "mean 100 episode reward": -200.0, "steps": 52799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 266, "mean 100 episode reward": -200.0, "steps": 52999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 267, "mean 100 episode reward": -200.0, "steps": 53199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 268, "mean 100 episode reward": -200.0, "steps": 53399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 269, "mean 100 episode reward": -200.0, "steps": 53599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 270, "mean 100 episode reward": -200.0, "steps": 53799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 271, "mean 100 episode reward": -200.0, "steps": 53999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 272, "mean 100 episode reward": -200.0, "steps": 54199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 273, "mean 100 episode reward": -200.0, "steps": 54399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 274, "mean 100 episode reward": -200.0, "steps": 54599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 275, "mean 100 episode reward": -200.0, "steps": 54799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 276, "mean 100 episode reward": -200.0, "steps": 54999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 277, "mean 100 episode reward": -200.0, "steps": 55199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 278, "mean 100 episode reward": -200.0, "steps": 55399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 279, "mean 100 episode reward": -200.0, "steps": 55599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 280, "mean 100 episode reward": -200.0, "steps": 55799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 281, "mean 100 episode reward": -200.0, "steps": 55999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 282, "mean 100 episode reward": -200.0, "steps": 56199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 283, "mean 100 episode reward": -200.0, "steps": 56399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 284, "mean 100 episode reward": -200.0, "steps": 56599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 285, "mean 100 episode reward": -200.0, "steps": 56799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 286, "mean 100 episode reward": -200.0, "steps": 56999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 287, "mean 100 episode reward": -200.0, "steps": 57199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 288, "mean 100 episode reward": -200.0, "steps": 57399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 289, "mean 100 episode reward": -200.0, "steps": 57599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 290, "mean 100 episode reward": -200.0, "steps": 57799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 291, "mean 100 episode reward": -200.0, "steps": 57999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 292, "mean 100 episode reward": -200.0, "steps": 58199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 293, "mean 100 episode reward": -200.0, "steps": 58399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 294, "mean 100 episode reward": -200.0, "steps": 58599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 295, "mean 100 episode reward": -200.0, "steps": 58799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 296, "mean 100 episode reward": -200.0, "steps": 58999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 297, "mean 100 episode reward": -200.0, "steps": 59199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 298, "mean 100 episode reward": -200.0, "steps": 59399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 299, "mean 100 episode reward": -200.0, "steps": 59599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 300, "mean 100 episode reward": -200.0, "steps": 59799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 301, "mean 100 episode reward": -200.0, "steps": 59999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 302, "mean 100 episode reward": -200.0, "steps": 60199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 303, "mean 100 episode reward": -200.0, "steps": 60399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 304, "mean 100 episode reward": -200.0, "steps": 60599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 305, "mean 100 episode reward": -200.0, "steps": 60799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 306, "mean 100 episode reward": -200.0, "steps": 60999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 307, "mean 100 episode reward": -200.0, "steps": 61199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 308, "mean 100 episode reward": -200.0, "steps": 61399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 309, "mean 100 episode reward": -200.0, "steps": 61599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 310, "mean 100 episode reward": -200.0, "steps": 61799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 311, "mean 100 episode reward": -200.0, "steps": 61999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 312, "mean 100 episode reward": -200.0, "steps": 62199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 313, "mean 100 episode reward": -200.0, "steps": 62399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 314, "mean 100 episode reward": -200.0, "steps": 62599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 315, "mean 100 episode reward": -200.0, "steps": 62799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 316, "mean 100 episode reward": -200.0, "steps": 62999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 317, "mean 100 episode reward": -200.0, "steps": 63199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 318, "mean 100 episode reward": -200.0, "steps": 63399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 319, "mean 100 episode reward": -200.0, "steps": 63599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 320, "mean 100 episode reward": -200.0, "steps": 63799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 321, "mean 100 episode reward": -200.0, "steps": 63999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 322, "mean 100 episode reward": -200.0, "steps": 64199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 323, "mean 100 episode reward": -200.0, "steps": 64399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 324, "mean 100 episode reward": -200.0, "steps": 64599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 325, "mean 100 episode reward": -200.0, "steps": 64799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 326, "mean 100 episode reward": -200.0, "steps": 64999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 327, "mean 100 episode reward": -200.0, "steps": 65199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 328, "mean 100 episode reward": -200.0, "steps": 65399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 329, "mean 100 episode reward": -200.0, "steps": 65599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 330, "mean 100 episode reward": -200.0, "steps": 65799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 331, "mean 100 episode reward": -200.0, "steps": 65999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 332, "mean 100 episode reward": -200.0, "steps": 66199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 333, "mean 100 episode reward": -200.0, "steps": 66399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 334, "mean 100 episode reward": -200.0, "steps": 66599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 335, "mean 100 episode reward": -200.0, "steps": 66799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 336, "mean 100 episode reward": -200.0, "steps": 66999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 337, "mean 100 episode reward": -200.0, "steps": 67199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 338, "mean 100 episode reward": -200.0, "steps": 67399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 339, "mean 100 episode reward": -200.0, "steps": 67599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 340, "mean 100 episode reward": -200.0, "steps": 67799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 341, "mean 100 episode reward": -200.0, "steps": 67999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 342, "mean 100 episode reward": -200.0, "steps": 68199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 343, "mean 100 episode reward": -200.0, "steps": 68399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 344, "mean 100 episode reward": -200.0, "steps": 68599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 345, "mean 100 episode reward": -200.0, "steps": 68799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 346, "mean 100 episode reward": -200.0, "steps": 68999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 347, "mean 100 episode reward": -200.0, "steps": 69199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 348, "mean 100 episode reward": -200.0, "steps": 69399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 349, "mean 100 episode reward": -200.0, "steps": 69599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 350, "mean 100 episode reward": -200.0, "steps": 69799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 351, "mean 100 episode reward": -200.0, "steps": 69999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 352, "mean 100 episode reward": -200.0, "steps": 70199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 353, "mean 100 episode reward": -200.0, "steps": 70399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 354, "mean 100 episode reward": -200.0, "steps": 70599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 355, "mean 100 episode reward": -200.0, "steps": 70799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 356, "mean 100 episode reward": -200.0, "steps": 70999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 357, "mean 100 episode reward": -200.0, "steps": 71199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 358, "mean 100 episode reward": -200.0, "steps": 71399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 359, "mean 100 episode reward": -200.0, "steps": 71599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 360, "mean 100 episode reward": -200.0, "steps": 71799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 361, "mean 100 episode reward": -200.0, "steps": 71999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 362, "mean 100 episode reward": -200.0, "steps": 72199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 363, "mean 100 episode reward": -200.0, "steps": 72399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 364, "mean 100 episode reward": -200.0, "steps": 72599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 365, "mean 100 episode reward": -200.0, "steps": 72799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 366, "mean 100 episode reward": -200.0, "steps": 72999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 367, "mean 100 episode reward": -200.0, "steps": 73199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 368, "mean 100 episode reward": -200.0, "steps": 73399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 369, "mean 100 episode reward": -200.0, "steps": 73599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 370, "mean 100 episode reward": -200.0, "steps": 73799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 371, "mean 100 episode reward": -200.0, "steps": 73999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 372, "mean 100 episode reward": -200.0, "steps": 74199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 373, "mean 100 episode reward": -200.0, "steps": 74399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 374, "mean 100 episode reward": -200.0, "steps": 74599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 375, "mean 100 episode reward": -200.0, "steps": 74799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 376, "mean 100 episode reward": -200.0, "steps": 74999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 377, "mean 100 episode reward": -200.0, "steps": 75199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 378, "mean 100 episode reward": -200.0, "steps": 75399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 379, "mean 100 episode reward": -200.0, "steps": 75599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 380, "mean 100 episode reward": -200.0, "steps": 75799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 381, "mean 100 episode reward": -200.0, "steps": 75999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 382, "mean 100 episode reward": -200.0, "steps": 76199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 383, "mean 100 episode reward": -200.0, "steps": 76399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 384, "mean 100 episode reward": -200.0, "steps": 76599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 385, "mean 100 episode reward": -200.0, "steps": 76799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 386, "mean 100 episode reward": -200.0, "steps": 76999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 387, "mean 100 episode reward": -200.0, "steps": 77199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 388, "mean 100 episode reward": -200.0, "steps": 77399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 389, "mean 100 episode reward": -200.0, "steps": 77599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 390, "mean 100 episode reward": -200.0, "steps": 77799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 391, "mean 100 episode reward": -200.0, "steps": 77999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 392, "mean 100 episode reward": -200.0, "steps": 78199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 393, "mean 100 episode reward": -200.0, "steps": 78399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 394, "mean 100 episode reward": -200.0, "steps": 78599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 395, "mean 100 episode reward": -200.0, "steps": 78799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 396, "mean 100 episode reward": -200.0, "steps": 78999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 397, "mean 100 episode reward": -200.0, "steps": 79199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 398, "mean 100 episode reward": -200.0, "steps": 79399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 399, "mean 100 episode reward": -200.0, "steps": 79599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 400, "mean 100 episode reward": -200.0, "steps": 79799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 401, "mean 100 episode reward": -200.0, "steps": 79999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 402, "mean 100 episode reward": -200.0, "steps": 80199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 403, "mean 100 episode reward": -200.0, "steps": 80399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 404, "mean 100 episode reward": -200.0, "steps": 80599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 405, "mean 100 episode reward": -200.0, "steps": 80799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 406, "mean 100 episode reward": -200.0, "steps": 80999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 407, "mean 100 episode reward": -200.0, "steps": 81199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 408, "mean 100 episode reward": -200.0, "steps": 81399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 409, "mean 100 episode reward": -200.0, "steps": 81599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 410, "mean 100 episode reward": -200.0, "steps": 81799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 411, "mean 100 episode reward": -200.0, "steps": 81999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 412, "mean 100 episode reward": -200.0, "steps": 82199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 413, "mean 100 episode reward": -200.0, "steps": 82399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 414, "mean 100 episode reward": -200.0, "steps": 82599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 415, "mean 100 episode reward": -200.0, "steps": 82799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 416, "mean 100 episode reward": -200.0, "steps": 82999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 417, "mean 100 episode reward": -200.0, "steps": 83199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 418, "mean 100 episode reward": -200.0, "steps": 83399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 419, "mean 100 episode reward": -200.0, "steps": 83599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 420, "mean 100 episode reward": -200.0, "steps": 83799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 421, "mean 100 episode reward": -200.0, "steps": 83999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 422, "mean 100 episode reward": -200.0, "steps": 84199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 423, "mean 100 episode reward": -200.0, "steps": 84399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 424, "mean 100 episode reward": -200.0, "steps": 84599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 425, "mean 100 episode reward": -200.0, "steps": 84799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 426, "mean 100 episode reward": -200.0, "steps": 84999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 427, "mean 100 episode reward": -200.0, "steps": 85199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 428, "mean 100 episode reward": -200.0, "steps": 85399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 429, "mean 100 episode reward": -200.0, "steps": 85599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 430, "mean 100 episode reward": -200.0, "steps": 85799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 431, "mean 100 episode reward": -200.0, "steps": 85999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 432, "mean 100 episode reward": -200.0, "steps": 86199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 433, "mean 100 episode reward": -200.0, "steps": 86399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 434, "mean 100 episode reward": -200.0, "steps": 86599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 435, "mean 100 episode reward": -200.0, "steps": 86799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 436, "mean 100 episode reward": -200.0, "steps": 86999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 437, "mean 100 episode reward": -200.0, "steps": 87199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 438, "mean 100 episode reward": -200.0, "steps": 87399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 439, "mean 100 episode reward": -200.0, "steps": 87599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 440, "mean 100 episode reward": -200.0, "steps": 87799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 441, "mean 100 episode reward": -200.0, "steps": 87999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 442, "mean 100 episode reward": -200.0, "steps": 88199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 443, "mean 100 episode reward": -200.0, "steps": 88399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 444, "mean 100 episode reward": -200.0, "steps": 88599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 445, "mean 100 episode reward": -200.0, "steps": 88799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 446, "mean 100 episode reward": -200.0, "steps": 88999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 447, "mean 100 episode reward": -200.0, "steps": 89199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 448, "mean 100 episode reward": -200.0, "steps": 89399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 449, "mean 100 episode reward": -200.0, "steps": 89599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 450, "mean 100 episode reward": -200.0, "steps": 89799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 451, "mean 100 episode reward": -200.0, "steps": 89999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 452, "mean 100 episode reward": -200.0, "steps": 90199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 453, "mean 100 episode reward": -200.0, "steps": 90399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 454, "mean 100 episode reward": -200.0, "steps": 90599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 455, "mean 100 episode reward": -200.0, "steps": 90799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 456, "mean 100 episode reward": -200.0, "steps": 90999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 457, "mean 100 episode reward": -200.0, "steps": 91199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 458, "mean 100 episode reward": -200.0, "steps": 91399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 459, "mean 100 episode reward": -200.0, "steps": 91599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 460, "mean 100 episode reward": -200.0, "steps": 91799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 461, "mean 100 episode reward": -200.0, "steps": 91999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 462, "mean 100 episode reward": -200.0, "steps": 92199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 463, "mean 100 episode reward": -200.0, "steps": 92399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 464, "mean 100 episode reward": -200.0, "steps": 92599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 465, "mean 100 episode reward": -200.0, "steps": 92799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 466, "mean 100 episode reward": -200.0, "steps": 92999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 467, "mean 100 episode reward": -200.0, "steps": 93199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 468, "mean 100 episode reward": -200.0, "steps": 93399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 469, "mean 100 episode reward": -200.0, "steps": 93599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 470, "mean 100 episode reward": -200.0, "steps": 93799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 471, "mean 100 episode reward": -200.0, "steps": 93999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 472, "mean 100 episode reward": -200.0, "steps": 94199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 473, "mean 100 episode reward": -200.0, "steps": 94399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 474, "mean 100 episode reward": -200.0, "steps": 94599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 475, "mean 100 episode reward": -200.0, "steps": 94799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 476, "mean 100 episode reward": -200.0, "steps": 94999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 477, "mean 100 episode reward": -200.0, "steps": 95199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 478, "mean 100 episode reward": -200.0, "steps": 95399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 479, "mean 100 episode reward": -200.0, "steps": 95599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 480, "mean 100 episode reward": -200.0, "steps": 95799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 481, "mean 100 episode reward": -200.0, "steps": 95999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 482, "mean 100 episode reward": -200.0, "steps": 96199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 483, "mean 100 episode reward": -200.0, "steps": 96399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 484, "mean 100 episode reward": -200.0, "steps": 96599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 485, "mean 100 episode reward": -200.0, "steps": 96799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 486, "mean 100 episode reward": -200.0, "steps": 96999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 487, "mean 100 episode reward": -200.0, "steps": 97199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 488, "mean 100 episode reward": -200.0, "steps": 97399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 489, "mean 100 episode reward": -200.0, "steps": 97599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 490, "mean 100 episode reward": -200.0, "steps": 97799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 491, "mean 100 episode reward": -200.0, "steps": 97999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 492, "mean 100 episode reward": -200.0, "steps": 98199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 493, "mean 100 episode reward": -200.0, "steps": 98399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 494, "mean 100 episode reward": -200.0, "steps": 98599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 495, "mean 100 episode reward": -200.0, "steps": 98799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 496, "mean 100 episode reward": -200.0, "steps": 98999}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 497, "mean 100 episode reward": -200.0, "steps": 99199}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 498, "mean 100 episode reward": -200.0, "steps": 99399}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 499, "mean 100 episode reward": -200.0, "steps": 99599}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 500, "mean 100 episode reward": -200.0, "steps": 99799}
{"reward": -200.0, "% time spent exploring": 9, "episodes": 501, "mean 100 episode reward": -200.0, "steps": 99999}
