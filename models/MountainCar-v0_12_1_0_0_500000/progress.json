{"episodes": 2, "head": 6, "mean 100 episode reward": -200.0, "steps": 199, "reward": -200.0, "% time spent exploring": 99}
{"episodes": 3, "head": 1, "mean 100 episode reward": -200.0, "steps": 399, "reward": -200.0, "% time spent exploring": 99}
{"episodes": 4, "head": 2, "mean 100 episode reward": -200.0, "steps": 599, "reward": -200.0, "% time spent exploring": 98}
{"episodes": 5, "head": 3, "mean 100 episode reward": -200.0, "steps": 799, "reward": -200.0, "% time spent exploring": 98}
{"episodes": 6, "head": 3, "mean 100 episode reward": -200.0, "steps": 999, "reward": -200.0, "% time spent exploring": 98}
{"episodes": 7, "head": 0, "mean 100 episode reward": -200.0, "steps": 1199, "reward": -200.0, "% time spent exploring": 97}
{"episodes": 8, "head": 6, "mean 100 episode reward": -200.0, "steps": 1399, "reward": -200.0, "% time spent exploring": 97}
{"episodes": 9, "head": 1, "mean 100 episode reward": -200.0, "steps": 1599, "reward": -200.0, "% time spent exploring": 97}
{"episodes": 10, "head": 4, "mean 100 episode reward": -200.0, "steps": 1799, "reward": -200.0, "% time spent exploring": 96}
{"episodes": 11, "head": 5, "mean 100 episode reward": -200.0, "steps": 1999, "reward": -200.0, "% time spent exploring": 96}
{"episodes": 12, "head": 9, "mean 100 episode reward": -200.0, "steps": 2199, "reward": -200.0, "% time spent exploring": 96}
{"episodes": 13, "head": 2, "mean 100 episode reward": -200.0, "steps": 2399, "reward": -200.0, "% time spent exploring": 95}
{"episodes": 14, "head": 6, "mean 100 episode reward": -200.0, "steps": 2599, "reward": -200.0, "% time spent exploring": 95}
{"episodes": 15, "head": 0, "mean 100 episode reward": -200.0, "steps": 2799, "reward": -200.0, "% time spent exploring": 94}
{"episodes": 16, "head": 5, "mean 100 episode reward": -200.0, "steps": 2999, "reward": -200.0, "% time spent exploring": 94}
{"episodes": 17, "head": 8, "mean 100 episode reward": -200.0, "steps": 3199, "reward": -200.0, "% time spent exploring": 94}
{"episodes": 18, "head": 2, "mean 100 episode reward": -200.0, "steps": 3399, "reward": -200.0, "% time spent exploring": 93}
{"episodes": 19, "head": 9, "mean 100 episode reward": -200.0, "steps": 3599, "reward": -200.0, "% time spent exploring": 93}
{"episodes": 20, "head": 3, "mean 100 episode reward": -200.0, "steps": 3799, "reward": -200.0, "% time spent exploring": 93}
{"episodes": 21, "head": 4, "mean 100 episode reward": -200.0, "steps": 3999, "reward": -200.0, "% time spent exploring": 92}
{"episodes": 22, "head": 3, "mean 100 episode reward": -200.0, "steps": 4199, "reward": -200.0, "% time spent exploring": 92}
{"episodes": 23, "head": 1, "mean 100 episode reward": -200.0, "steps": 4399, "reward": -200.0, "% time spent exploring": 92}
{"episodes": 24, "head": 7, "mean 100 episode reward": -200.0, "steps": 4599, "reward": -200.0, "% time spent exploring": 91}
{"episodes": 25, "head": 0, "mean 100 episode reward": -200.0, "steps": 4799, "reward": -200.0, "% time spent exploring": 91}
{"episodes": 26, "head": 2, "mean 100 episode reward": -200.0, "steps": 4999, "reward": -200.0, "% time spent exploring": 91}
{"episodes": 27, "head": 6, "mean 100 episode reward": -200.0, "steps": 5199, "reward": -200.0, "% time spent exploring": 90}
{"episodes": 28, "head": 2, "mean 100 episode reward": -200.0, "steps": 5399, "reward": -200.0, "% time spent exploring": 90}
{"episodes": 29, "head": 0, "mean 100 episode reward": -200.0, "steps": 5599, "reward": -200.0, "% time spent exploring": 89}
{"episodes": 30, "head": 4, "mean 100 episode reward": -200.0, "steps": 5799, "reward": -200.0, "% time spent exploring": 89}
{"episodes": 31, "head": 6, "mean 100 episode reward": -200.0, "steps": 5999, "reward": -200.0, "% time spent exploring": 89}
{"episodes": 32, "head": 9, "mean 100 episode reward": -200.0, "steps": 6199, "reward": -200.0, "% time spent exploring": 88}
{"episodes": 33, "head": 0, "mean 100 episode reward": -200.0, "steps": 6399, "reward": -200.0, "% time spent exploring": 88}
{"episodes": 34, "head": 0, "mean 100 episode reward": -200.0, "steps": 6599, "reward": -200.0, "% time spent exploring": 88}
{"episodes": 35, "head": 9, "mean 100 episode reward": -200.0, "steps": 6799, "reward": -200.0, "% time spent exploring": 87}
{"episodes": 36, "head": 8, "mean 100 episode reward": -200.0, "steps": 6999, "reward": -200.0, "% time spent exploring": 87}
{"episodes": 37, "head": 9, "mean 100 episode reward": -200.0, "steps": 7199, "reward": -200.0, "% time spent exploring": 87}
{"episodes": 38, "head": 6, "mean 100 episode reward": -200.0, "steps": 7399, "reward": -200.0, "% time spent exploring": 86}
{"episodes": 39, "head": 1, "mean 100 episode reward": -200.0, "steps": 7599, "reward": -200.0, "% time spent exploring": 86}
{"episodes": 40, "head": 8, "mean 100 episode reward": -200.0, "steps": 7799, "reward": -200.0, "% time spent exploring": 85}
{"episodes": 41, "head": 4, "mean 100 episode reward": -200.0, "steps": 7999, "reward": -200.0, "% time spent exploring": 85}
{"episodes": 42, "head": 0, "mean 100 episode reward": -200.0, "steps": 8199, "reward": -200.0, "% time spent exploring": 85}
{"episodes": 43, "head": 4, "mean 100 episode reward": -200.0, "steps": 8399, "reward": -200.0, "% time spent exploring": 84}
{"episodes": 44, "head": 1, "mean 100 episode reward": -200.0, "steps": 8599, "reward": -200.0, "% time spent exploring": 84}
{"episodes": 45, "head": 5, "mean 100 episode reward": -200.0, "steps": 8799, "reward": -200.0, "% time spent exploring": 84}
{"episodes": 46, "head": 5, "mean 100 episode reward": -200.0, "steps": 8999, "reward": -200.0, "% time spent exploring": 83}
{"episodes": 47, "head": 3, "mean 100 episode reward": -200.0, "steps": 9199, "reward": -200.0, "% time spent exploring": 83}
{"episodes": 48, "head": 4, "mean 100 episode reward": -200.0, "steps": 9399, "reward": -200.0, "% time spent exploring": 83}
{"episodes": 49, "head": 5, "mean 100 episode reward": -200.0, "steps": 9599, "reward": -200.0, "% time spent exploring": 82}
{"episodes": 50, "head": 5, "mean 100 episode reward": -200.0, "steps": 9799, "reward": -200.0, "% time spent exploring": 82}
{"episodes": 51, "head": 0, "mean 100 episode reward": -200.0, "steps": 9999, "reward": -200.0, "% time spent exploring": 82}
{"episodes": 52, "head": 6, "mean 100 episode reward": -200.0, "steps": 10199, "reward": -200.0, "% time spent exploring": 81}
{"episodes": 53, "head": 6, "mean 100 episode reward": -200.0, "steps": 10399, "reward": -200.0, "% time spent exploring": 81}
{"episodes": 54, "head": 3, "mean 100 episode reward": -200.0, "steps": 10599, "reward": -200.0, "% time spent exploring": 80}
{"episodes": 55, "head": 2, "mean 100 episode reward": -200.0, "steps": 10799, "reward": -200.0, "% time spent exploring": 80}
{"episodes": 56, "head": 4, "mean 100 episode reward": -200.0, "steps": 10999, "reward": -200.0, "% time spent exploring": 80}
{"episodes": 57, "head": 3, "mean 100 episode reward": -200.0, "steps": 11199, "reward": -200.0, "% time spent exploring": 79}
{"episodes": 58, "head": 9, "mean 100 episode reward": -200.0, "steps": 11399, "reward": -200.0, "% time spent exploring": 79}
{"episodes": 59, "head": 6, "mean 100 episode reward": -200.0, "steps": 11599, "reward": -200.0, "% time spent exploring": 79}
{"episodes": 60, "head": 7, "mean 100 episode reward": -200.0, "steps": 11799, "reward": -200.0, "% time spent exploring": 78}
{"episodes": 61, "head": 4, "mean 100 episode reward": -200.0, "steps": 11999, "reward": -200.0, "% time spent exploring": 78}
{"episodes": 62, "head": 1, "mean 100 episode reward": -200.0, "steps": 12199, "reward": -200.0, "% time spent exploring": 78}
{"episodes": 63, "head": 4, "mean 100 episode reward": -200.0, "steps": 12399, "reward": -200.0, "% time spent exploring": 77}
{"episodes": 64, "head": 9, "mean 100 episode reward": -200.0, "steps": 12599, "reward": -200.0, "% time spent exploring": 77}
{"episodes": 65, "head": 5, "mean 100 episode reward": -200.0, "steps": 12799, "reward": -200.0, "% time spent exploring": 76}
{"episodes": 66, "head": 3, "mean 100 episode reward": -200.0, "steps": 12999, "reward": -200.0, "% time spent exploring": 76}
{"episodes": 67, "head": 5, "mean 100 episode reward": -200.0, "steps": 13199, "reward": -200.0, "% time spent exploring": 76}
{"episodes": 68, "head": 0, "mean 100 episode reward": -200.0, "steps": 13399, "reward": -200.0, "% time spent exploring": 75}
{"episodes": 69, "head": 2, "mean 100 episode reward": -200.0, "steps": 13599, "reward": -200.0, "% time spent exploring": 75}
{"episodes": 70, "head": 9, "mean 100 episode reward": -200.0, "steps": 13799, "reward": -200.0, "% time spent exploring": 75}
{"episodes": 71, "head": 6, "mean 100 episode reward": -200.0, "steps": 13999, "reward": -200.0, "% time spent exploring": 74}
{"episodes": 72, "head": 4, "mean 100 episode reward": -200.0, "steps": 14199, "reward": -200.0, "% time spent exploring": 74}
{"episodes": 73, "head": 7, "mean 100 episode reward": -200.0, "steps": 14399, "reward": -200.0, "% time spent exploring": 74}
{"episodes": 74, "head": 6, "mean 100 episode reward": -200.0, "steps": 14599, "reward": -200.0, "% time spent exploring": 73}
{"episodes": 75, "head": 2, "mean 100 episode reward": -200.0, "steps": 14799, "reward": -200.0, "% time spent exploring": 73}
{"episodes": 76, "head": 3, "mean 100 episode reward": -200.0, "steps": 14999, "reward": -200.0, "% time spent exploring": 73}
{"episodes": 77, "head": 2, "mean 100 episode reward": -200.0, "steps": 15199, "reward": -200.0, "% time spent exploring": 72}
{"episodes": 78, "head": 6, "mean 100 episode reward": -200.0, "steps": 15399, "reward": -200.0, "% time spent exploring": 72}
{"episodes": 79, "head": 4, "mean 100 episode reward": -200.0, "steps": 15599, "reward": -200.0, "% time spent exploring": 71}
{"episodes": 80, "head": 5, "mean 100 episode reward": -200.0, "steps": 15799, "reward": -200.0, "% time spent exploring": 71}
{"episodes": 81, "head": 0, "mean 100 episode reward": -200.0, "steps": 15999, "reward": -200.0, "% time spent exploring": 71}
{"episodes": 82, "head": 0, "mean 100 episode reward": -200.0, "steps": 16199, "reward": -200.0, "% time spent exploring": 70}
{"episodes": 83, "head": 5, "mean 100 episode reward": -200.0, "steps": 16399, "reward": -200.0, "% time spent exploring": 70}
{"episodes": 84, "head": 4, "mean 100 episode reward": -200.0, "steps": 16599, "reward": -200.0, "% time spent exploring": 70}
{"episodes": 85, "head": 0, "mean 100 episode reward": -200.0, "steps": 16799, "reward": -200.0, "% time spent exploring": 69}
{"episodes": 86, "head": 3, "mean 100 episode reward": -200.0, "steps": 16999, "reward": -200.0, "% time spent exploring": 69}
{"episodes": 87, "head": 7, "mean 100 episode reward": -200.0, "steps": 17199, "reward": -200.0, "% time spent exploring": 69}
{"episodes": 88, "head": 9, "mean 100 episode reward": -200.0, "steps": 17399, "reward": -200.0, "% time spent exploring": 68}
{"episodes": 89, "head": 6, "mean 100 episode reward": -200.0, "steps": 17599, "reward": -200.0, "% time spent exploring": 68}
{"episodes": 90, "head": 8, "mean 100 episode reward": -200.0, "steps": 17799, "reward": -200.0, "% time spent exploring": 67}
{"episodes": 91, "head": 6, "mean 100 episode reward": -200.0, "steps": 17999, "reward": -200.0, "% time spent exploring": 67}
{"episodes": 92, "head": 1, "mean 100 episode reward": -200.0, "steps": 18199, "reward": -200.0, "% time spent exploring": 67}
{"episodes": 93, "head": 4, "mean 100 episode reward": -200.0, "steps": 18399, "reward": -200.0, "% time spent exploring": 66}
{"episodes": 94, "head": 9, "mean 100 episode reward": -200.0, "steps": 18599, "reward": -200.0, "% time spent exploring": 66}
{"episodes": 95, "head": 6, "mean 100 episode reward": -200.0, "steps": 18799, "reward": -200.0, "% time spent exploring": 66}
{"episodes": 96, "head": 5, "mean 100 episode reward": -200.0, "steps": 18999, "reward": -200.0, "% time spent exploring": 65}
{"episodes": 97, "head": 9, "mean 100 episode reward": -200.0, "steps": 19199, "reward": -200.0, "% time spent exploring": 65}
{"episodes": 98, "head": 2, "mean 100 episode reward": -200.0, "steps": 19399, "reward": -200.0, "% time spent exploring": 65}
{"episodes": 99, "head": 4, "mean 100 episode reward": -200.0, "steps": 19599, "reward": -200.0, "% time spent exploring": 64}
{"episodes": 100, "head": 0, "mean 100 episode reward": -200.0, "steps": 19799, "reward": -200.0, "% time spent exploring": 64}
{"episodes": 101, "head": 8, "mean 100 episode reward": -200.0, "steps": 19999, "reward": -200.0, "% time spent exploring": 64}
{"episodes": 102, "head": 4, "mean 100 episode reward": -200.0, "steps": 20199, "reward": -200.0, "% time spent exploring": 63}
{"episodes": 103, "head": 4, "mean 100 episode reward": -200.0, "steps": 20399, "reward": -200.0, "% time spent exploring": 63}
{"episodes": 104, "head": 3, "mean 100 episode reward": -200.0, "steps": 20599, "reward": -200.0, "% time spent exploring": 62}
{"episodes": 105, "head": 7, "mean 100 episode reward": -200.0, "steps": 20799, "reward": -200.0, "% time spent exploring": 62}
{"episodes": 106, "head": 3, "mean 100 episode reward": -200.0, "steps": 20999, "reward": -200.0, "% time spent exploring": 62}
{"episodes": 107, "head": 6, "mean 100 episode reward": -200.0, "steps": 21199, "reward": -200.0, "% time spent exploring": 61}
{"episodes": 108, "head": 1, "mean 100 episode reward": -200.0, "steps": 21399, "reward": -200.0, "% time spent exploring": 61}
{"episodes": 109, "head": 4, "mean 100 episode reward": -200.0, "steps": 21599, "reward": -200.0, "% time spent exploring": 61}
{"episodes": 110, "head": 3, "mean 100 episode reward": -200.0, "steps": 21799, "reward": -200.0, "% time spent exploring": 60}
{"episodes": 111, "head": 8, "mean 100 episode reward": -200.0, "steps": 21999, "reward": -200.0, "% time spent exploring": 60}
{"episodes": 112, "head": 8, "mean 100 episode reward": -200.0, "steps": 22199, "reward": -200.0, "% time spent exploring": 60}
{"episodes": 113, "head": 6, "mean 100 episode reward": -200.0, "steps": 22399, "reward": -200.0, "% time spent exploring": 59}
{"episodes": 114, "head": 9, "mean 100 episode reward": -200.0, "steps": 22599, "reward": -200.0, "% time spent exploring": 59}
{"episodes": 115, "head": 8, "mean 100 episode reward": -200.0, "steps": 22799, "reward": -200.0, "% time spent exploring": 58}
{"episodes": 116, "head": 4, "mean 100 episode reward": -200.0, "steps": 22999, "reward": -200.0, "% time spent exploring": 58}
{"episodes": 117, "head": 9, "mean 100 episode reward": -200.0, "steps": 23199, "reward": -200.0, "% time spent exploring": 58}
{"episodes": 118, "head": 4, "mean 100 episode reward": -200.0, "steps": 23399, "reward": -200.0, "% time spent exploring": 57}
{"episodes": 119, "head": 1, "mean 100 episode reward": -200.0, "steps": 23599, "reward": -200.0, "% time spent exploring": 57}
{"episodes": 120, "head": 4, "mean 100 episode reward": -200.0, "steps": 23799, "reward": -200.0, "% time spent exploring": 57}
{"episodes": 121, "head": 1, "mean 100 episode reward": -200.0, "steps": 23999, "reward": -200.0, "% time spent exploring": 56}
{"episodes": 122, "head": 4, "mean 100 episode reward": -200.0, "steps": 24199, "reward": -200.0, "% time spent exploring": 56}
{"episodes": 123, "head": 3, "mean 100 episode reward": -200.0, "steps": 24399, "reward": -200.0, "% time spent exploring": 56}
{"episodes": 124, "head": 2, "mean 100 episode reward": -200.0, "steps": 24599, "reward": -200.0, "% time spent exploring": 55}
{"episodes": 125, "head": 8, "mean 100 episode reward": -200.0, "steps": 24799, "reward": -200.0, "% time spent exploring": 55}
{"episodes": 126, "head": 0, "mean 100 episode reward": -200.0, "steps": 24999, "reward": -200.0, "% time spent exploring": 55}
{"episodes": 127, "head": 1, "mean 100 episode reward": -200.0, "steps": 25199, "reward": -200.0, "% time spent exploring": 54}
{"episodes": 128, "head": 4, "mean 100 episode reward": -200.0, "steps": 25399, "reward": -200.0, "% time spent exploring": 54}
{"episodes": 129, "head": 6, "mean 100 episode reward": -200.0, "steps": 25599, "reward": -200.0, "% time spent exploring": 53}
{"episodes": 130, "head": 5, "mean 100 episode reward": -200.0, "steps": 25799, "reward": -200.0, "% time spent exploring": 53}
{"episodes": 131, "head": 4, "mean 100 episode reward": -200.0, "steps": 25999, "reward": -200.0, "% time spent exploring": 53}
{"episodes": 132, "head": 2, "mean 100 episode reward": -200.0, "steps": 26199, "reward": -200.0, "% time spent exploring": 52}
{"episodes": 133, "head": 0, "mean 100 episode reward": -200.0, "steps": 26399, "reward": -200.0, "% time spent exploring": 52}
{"episodes": 134, "head": 1, "mean 100 episode reward": -200.0, "steps": 26599, "reward": -200.0, "% time spent exploring": 52}
{"episodes": 135, "head": 2, "mean 100 episode reward": -200.0, "steps": 26799, "reward": -200.0, "% time spent exploring": 51}
{"episodes": 136, "head": 5, "mean 100 episode reward": -200.0, "steps": 26999, "reward": -200.0, "% time spent exploring": 51}
{"episodes": 137, "head": 2, "mean 100 episode reward": -200.0, "steps": 27199, "reward": -200.0, "% time spent exploring": 51}
{"episodes": 138, "head": 0, "mean 100 episode reward": -200.0, "steps": 27399, "reward": -200.0, "% time spent exploring": 50}
{"episodes": 139, "head": 1, "mean 100 episode reward": -200.0, "steps": 27599, "reward": -200.0, "% time spent exploring": 50}
{"episodes": 140, "head": 4, "mean 100 episode reward": -200.0, "steps": 27799, "reward": -200.0, "% time spent exploring": 49}
{"episodes": 141, "head": 7, "mean 100 episode reward": -200.0, "steps": 27999, "reward": -200.0, "% time spent exploring": 49}
{"episodes": 142, "head": 7, "mean 100 episode reward": -200.0, "steps": 28199, "reward": -200.0, "% time spent exploring": 49}
{"episodes": 143, "head": 9, "mean 100 episode reward": -200.0, "steps": 28399, "reward": -200.0, "% time spent exploring": 48}
{"episodes": 144, "head": 1, "mean 100 episode reward": -200.0, "steps": 28599, "reward": -200.0, "% time spent exploring": 48}
{"episodes": 145, "head": 6, "mean 100 episode reward": -200.0, "steps": 28799, "reward": -200.0, "% time spent exploring": 48}
{"episodes": 146, "head": 1, "mean 100 episode reward": -200.0, "steps": 28999, "reward": -200.0, "% time spent exploring": 47}
{"episodes": 147, "head": 3, "mean 100 episode reward": -200.0, "steps": 29199, "reward": -200.0, "% time spent exploring": 47}
{"episodes": 148, "head": 0, "mean 100 episode reward": -200.0, "steps": 29399, "reward": -200.0, "% time spent exploring": 47}
{"episodes": 149, "head": 1, "mean 100 episode reward": -200.0, "steps": 29599, "reward": -200.0, "% time spent exploring": 46}
{"episodes": 150, "head": 8, "mean 100 episode reward": -200.0, "steps": 29799, "reward": -200.0, "% time spent exploring": 46}
{"episodes": 151, "head": 4, "mean 100 episode reward": -200.0, "steps": 29999, "reward": -200.0, "% time spent exploring": 46}
{"episodes": 152, "head": 8, "mean 100 episode reward": -200.0, "steps": 30199, "reward": -200.0, "% time spent exploring": 45}
{"episodes": 153, "head": 8, "mean 100 episode reward": -200.0, "steps": 30399, "reward": -200.0, "% time spent exploring": 45}
{"episodes": 154, "head": 8, "mean 100 episode reward": -200.0, "steps": 30599, "reward": -200.0, "% time spent exploring": 44}
{"episodes": 155, "head": 8, "mean 100 episode reward": -200.0, "steps": 30799, "reward": -200.0, "% time spent exploring": 44}
{"episodes": 156, "head": 3, "mean 100 episode reward": -200.0, "steps": 30999, "reward": -200.0, "% time spent exploring": 44}
{"episodes": 157, "head": 9, "mean 100 episode reward": -200.0, "steps": 31199, "reward": -200.0, "% time spent exploring": 43}
{"episodes": 158, "head": 5, "mean 100 episode reward": -200.0, "steps": 31399, "reward": -200.0, "% time spent exploring": 43}
{"episodes": 159, "head": 1, "mean 100 episode reward": -200.0, "steps": 31599, "reward": -200.0, "% time spent exploring": 43}
{"episodes": 160, "head": 6, "mean 100 episode reward": -200.0, "steps": 31799, "reward": -200.0, "% time spent exploring": 42}
{"episodes": 161, "head": 3, "mean 100 episode reward": -200.0, "steps": 31999, "reward": -200.0, "% time spent exploring": 42}
{"episodes": 162, "head": 2, "mean 100 episode reward": -200.0, "steps": 32199, "reward": -200.0, "% time spent exploring": 42}
{"episodes": 163, "head": 3, "mean 100 episode reward": -200.0, "steps": 32399, "reward": -200.0, "% time spent exploring": 41}
{"episodes": 164, "head": 1, "mean 100 episode reward": -200.0, "steps": 32599, "reward": -200.0, "% time spent exploring": 41}
{"episodes": 165, "head": 9, "mean 100 episode reward": -200.0, "steps": 32799, "reward": -200.0, "% time spent exploring": 40}
{"episodes": 166, "head": 3, "mean 100 episode reward": -200.0, "steps": 32999, "reward": -200.0, "% time spent exploring": 40}
{"episodes": 167, "head": 2, "mean 100 episode reward": -200.0, "steps": 33199, "reward": -200.0, "% time spent exploring": 40}
{"episodes": 168, "head": 2, "mean 100 episode reward": -200.0, "steps": 33399, "reward": -200.0, "% time spent exploring": 39}
{"episodes": 169, "head": 8, "mean 100 episode reward": -200.0, "steps": 33599, "reward": -200.0, "% time spent exploring": 39}
{"episodes": 170, "head": 9, "mean 100 episode reward": -200.0, "steps": 33799, "reward": -200.0, "% time spent exploring": 39}
{"episodes": 171, "head": 3, "mean 100 episode reward": -200.0, "steps": 33999, "reward": -200.0, "% time spent exploring": 38}
{"episodes": 172, "head": 5, "mean 100 episode reward": -200.0, "steps": 34199, "reward": -200.0, "% time spent exploring": 38}
{"episodes": 173, "head": 9, "mean 100 episode reward": -200.0, "steps": 34399, "reward": -200.0, "% time spent exploring": 38}
{"episodes": 174, "head": 1, "mean 100 episode reward": -200.0, "steps": 34599, "reward": -200.0, "% time spent exploring": 37}
{"episodes": 175, "head": 7, "mean 100 episode reward": -200.0, "steps": 34799, "reward": -200.0, "% time spent exploring": 37}
{"episodes": 176, "head": 4, "mean 100 episode reward": -200.0, "steps": 34999, "reward": -200.0, "% time spent exploring": 37}
{"episodes": 177, "head": 0, "mean 100 episode reward": -200.0, "steps": 35199, "reward": -200.0, "% time spent exploring": 36}
{"episodes": 178, "head": 8, "mean 100 episode reward": -200.0, "steps": 35399, "reward": -200.0, "% time spent exploring": 36}
{"episodes": 179, "head": 3, "mean 100 episode reward": -200.0, "steps": 35599, "reward": -200.0, "% time spent exploring": 35}
{"episodes": 180, "head": 1, "mean 100 episode reward": -200.0, "steps": 35799, "reward": -200.0, "% time spent exploring": 35}
{"episodes": 181, "head": 4, "mean 100 episode reward": -200.0, "steps": 35999, "reward": -200.0, "% time spent exploring": 35}
{"episodes": 182, "head": 7, "mean 100 episode reward": -200.0, "steps": 36199, "reward": -200.0, "% time spent exploring": 34}
{"episodes": 183, "head": 2, "mean 100 episode reward": -200.0, "steps": 36399, "reward": -200.0, "% time spent exploring": 34}
{"episodes": 184, "head": 2, "mean 100 episode reward": -200.0, "steps": 36599, "reward": -200.0, "% time spent exploring": 34}
{"episodes": 185, "head": 4, "mean 100 episode reward": -200.0, "steps": 36799, "reward": -200.0, "% time spent exploring": 33}
{"episodes": 186, "head": 0, "mean 100 episode reward": -200.0, "steps": 36999, "reward": -200.0, "% time spent exploring": 33}
{"episodes": 187, "head": 8, "mean 100 episode reward": -200.0, "steps": 37199, "reward": -200.0, "% time spent exploring": 33}
{"episodes": 188, "head": 3, "mean 100 episode reward": -200.0, "steps": 37399, "reward": -200.0, "% time spent exploring": 32}
{"episodes": 189, "head": 5, "mean 100 episode reward": -200.0, "steps": 37599, "reward": -200.0, "% time spent exploring": 32}
{"episodes": 190, "head": 2, "mean 100 episode reward": -200.0, "steps": 37799, "reward": -200.0, "% time spent exploring": 31}
{"episodes": 191, "head": 4, "mean 100 episode reward": -200.0, "steps": 37999, "reward": -200.0, "% time spent exploring": 31}
{"episodes": 192, "head": 7, "mean 100 episode reward": -200.0, "steps": 38199, "reward": -200.0, "% time spent exploring": 31}
{"episodes": 193, "head": 5, "mean 100 episode reward": -200.0, "steps": 38399, "reward": -200.0, "% time spent exploring": 30}
{"episodes": 194, "head": 0, "mean 100 episode reward": -200.0, "steps": 38599, "reward": -200.0, "% time spent exploring": 30}
{"episodes": 195, "head": 2, "mean 100 episode reward": -200.0, "steps": 38799, "reward": -200.0, "% time spent exploring": 30}
{"episodes": 196, "head": 4, "mean 100 episode reward": -200.0, "steps": 38999, "reward": -200.0, "% time spent exploring": 29}
{"episodes": 197, "head": 6, "mean 100 episode reward": -200.0, "steps": 39199, "reward": -200.0, "% time spent exploring": 29}
{"episodes": 198, "head": 6, "mean 100 episode reward": -200.0, "steps": 39399, "reward": -200.0, "% time spent exploring": 29}
{"episodes": 199, "head": 2, "mean 100 episode reward": -200.0, "steps": 39599, "reward": -200.0, "% time spent exploring": 28}
{"episodes": 200, "head": 7, "mean 100 episode reward": -200.0, "steps": 39799, "reward": -200.0, "% time spent exploring": 28}
{"episodes": 201, "head": 2, "mean 100 episode reward": -200.0, "steps": 39999, "reward": -200.0, "% time spent exploring": 28}
{"episodes": 202, "head": 5, "mean 100 episode reward": -200.0, "steps": 40199, "reward": -200.0, "% time spent exploring": 27}
{"episodes": 203, "head": 1, "mean 100 episode reward": -200.0, "steps": 40399, "reward": -200.0, "% time spent exploring": 27}
{"episodes": 204, "head": 5, "mean 100 episode reward": -200.0, "steps": 40599, "reward": -200.0, "% time spent exploring": 26}
{"episodes": 205, "head": 3, "mean 100 episode reward": -200.0, "steps": 40799, "reward": -200.0, "% time spent exploring": 26}
{"episodes": 206, "head": 7, "mean 100 episode reward": -200.0, "steps": 40999, "reward": -200.0, "% time spent exploring": 26}
{"episodes": 207, "head": 7, "mean 100 episode reward": -200.0, "steps": 41199, "reward": -200.0, "% time spent exploring": 25}
{"episodes": 208, "head": 1, "mean 100 episode reward": -200.0, "steps": 41399, "reward": -200.0, "% time spent exploring": 25}
{"episodes": 209, "head": 2, "mean 100 episode reward": -200.0, "steps": 41599, "reward": -200.0, "% time spent exploring": 25}
{"episodes": 210, "head": 1, "mean 100 episode reward": -200.0, "steps": 41799, "reward": -200.0, "% time spent exploring": 24}
{"episodes": 211, "head": 5, "mean 100 episode reward": -200.0, "steps": 41999, "reward": -200.0, "% time spent exploring": 24}
{"episodes": 212, "head": 8, "mean 100 episode reward": -200.0, "steps": 42199, "reward": -200.0, "% time spent exploring": 24}
{"episodes": 213, "head": 0, "mean 100 episode reward": -200.0, "steps": 42399, "reward": -200.0, "% time spent exploring": 23}
{"episodes": 214, "head": 8, "mean 100 episode reward": -200.0, "steps": 42599, "reward": -200.0, "% time spent exploring": 23}
{"episodes": 215, "head": 6, "mean 100 episode reward": -200.0, "steps": 42799, "reward": -200.0, "% time spent exploring": 22}
{"episodes": 216, "head": 5, "mean 100 episode reward": -200.0, "steps": 42999, "reward": -200.0, "% time spent exploring": 22}
{"episodes": 217, "head": 6, "mean 100 episode reward": -200.0, "steps": 43199, "reward": -200.0, "% time spent exploring": 22}
{"episodes": 218, "head": 4, "mean 100 episode reward": -200.0, "steps": 43399, "reward": -200.0, "% time spent exploring": 21}
{"episodes": 219, "head": 7, "mean 100 episode reward": -200.0, "steps": 43599, "reward": -200.0, "% time spent exploring": 21}
{"episodes": 220, "head": 8, "mean 100 episode reward": -200.0, "steps": 43799, "reward": -200.0, "% time spent exploring": 21}
{"episodes": 221, "head": 7, "mean 100 episode reward": -200.0, "steps": 43999, "reward": -200.0, "% time spent exploring": 20}
{"episodes": 222, "head": 1, "mean 100 episode reward": -200.0, "steps": 44199, "reward": -200.0, "% time spent exploring": 20}
{"episodes": 223, "head": 8, "mean 100 episode reward": -200.0, "steps": 44399, "reward": -200.0, "% time spent exploring": 20}
{"episodes": 224, "head": 2, "mean 100 episode reward": -200.0, "steps": 44599, "reward": -200.0, "% time spent exploring": 19}
{"episodes": 225, "head": 2, "mean 100 episode reward": -200.0, "steps": 44799, "reward": -200.0, "% time spent exploring": 19}
{"episodes": 226, "head": 7, "mean 100 episode reward": -200.0, "steps": 44999, "reward": -200.0, "% time spent exploring": 19}
{"episodes": 227, "head": 0, "mean 100 episode reward": -200.0, "steps": 45199, "reward": -200.0, "% time spent exploring": 18}
{"episodes": 228, "head": 1, "mean 100 episode reward": -200.0, "steps": 45399, "reward": -200.0, "% time spent exploring": 18}
{"episodes": 229, "head": 0, "mean 100 episode reward": -200.0, "steps": 45599, "reward": -200.0, "% time spent exploring": 17}
{"episodes": 230, "head": 5, "mean 100 episode reward": -200.0, "steps": 45799, "reward": -200.0, "% time spent exploring": 17}
{"episodes": 231, "head": 9, "mean 100 episode reward": -200.0, "steps": 45999, "reward": -200.0, "% time spent exploring": 17}
{"episodes": 232, "head": 0, "mean 100 episode reward": -200.0, "steps": 46199, "reward": -200.0, "% time spent exploring": 16}
{"episodes": 233, "head": 0, "mean 100 episode reward": -200.0, "steps": 46399, "reward": -200.0, "% time spent exploring": 16}
{"episodes": 234, "head": 7, "mean 100 episode reward": -200.0, "steps": 46599, "reward": -200.0, "% time spent exploring": 16}
{"episodes": 235, "head": 4, "mean 100 episode reward": -200.0, "steps": 46799, "reward": -200.0, "% time spent exploring": 15}
{"episodes": 236, "head": 3, "mean 100 episode reward": -200.0, "steps": 46999, "reward": -200.0, "% time spent exploring": 15}
{"episodes": 237, "head": 2, "mean 100 episode reward": -200.0, "steps": 47199, "reward": -200.0, "% time spent exploring": 15}
{"episodes": 238, "head": 6, "mean 100 episode reward": -200.0, "steps": 47399, "reward": -200.0, "% time spent exploring": 14}
{"episodes": 239, "head": 3, "mean 100 episode reward": -200.0, "steps": 47599, "reward": -200.0, "% time spent exploring": 14}
{"episodes": 240, "head": 4, "mean 100 episode reward": -200.0, "steps": 47799, "reward": -200.0, "% time spent exploring": 13}
{"episodes": 241, "head": 4, "mean 100 episode reward": -200.0, "steps": 47999, "reward": -200.0, "% time spent exploring": 13}
{"episodes": 242, "head": 9, "mean 100 episode reward": -200.0, "steps": 48199, "reward": -200.0, "% time spent exploring": 13}
{"episodes": 243, "head": 7, "mean 100 episode reward": -200.0, "steps": 48399, "reward": -200.0, "% time spent exploring": 12}
{"episodes": 244, "head": 2, "mean 100 episode reward": -200.0, "steps": 48599, "reward": -200.0, "% time spent exploring": 12}
{"episodes": 245, "head": 2, "mean 100 episode reward": -200.0, "steps": 48799, "reward": -200.0, "% time spent exploring": 12}
{"episodes": 246, "head": 3, "mean 100 episode reward": -200.0, "steps": 48999, "reward": -200.0, "% time spent exploring": 11}
{"episodes": 247, "head": 0, "mean 100 episode reward": -200.0, "steps": 49199, "reward": -200.0, "% time spent exploring": 11}
{"episodes": 248, "head": 3, "mean 100 episode reward": -200.0, "steps": 49399, "reward": -200.0, "% time spent exploring": 11}
{"episodes": 249, "head": 7, "mean 100 episode reward": -200.0, "steps": 49599, "reward": -200.0, "% time spent exploring": 10}
{"episodes": 250, "head": 8, "mean 100 episode reward": -200.0, "steps": 49799, "reward": -200.0, "% time spent exploring": 10}
{"episodes": 251, "head": 1, "mean 100 episode reward": -200.0, "steps": 49999, "reward": -200.0, "% time spent exploring": 10}
{"episodes": 252, "head": 6, "mean 100 episode reward": -200.0, "steps": 50199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 253, "head": 1, "mean 100 episode reward": -200.0, "steps": 50399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 254, "head": 8, "mean 100 episode reward": -200.0, "steps": 50599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 255, "head": 6, "mean 100 episode reward": -200.0, "steps": 50799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 256, "head": 6, "mean 100 episode reward": -200.0, "steps": 50999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 257, "head": 1, "mean 100 episode reward": -200.0, "steps": 51199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 258, "head": 3, "mean 100 episode reward": -200.0, "steps": 51399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 259, "head": 0, "mean 100 episode reward": -200.0, "steps": 51599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 260, "head": 5, "mean 100 episode reward": -200.0, "steps": 51799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 261, "head": 5, "mean 100 episode reward": -200.0, "steps": 51999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 262, "head": 3, "mean 100 episode reward": -200.0, "steps": 52199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 263, "head": 3, "mean 100 episode reward": -200.0, "steps": 52399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 264, "head": 7, "mean 100 episode reward": -200.0, "steps": 52599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 265, "head": 5, "mean 100 episode reward": -200.0, "steps": 52799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 266, "head": 1, "mean 100 episode reward": -200.0, "steps": 52999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 267, "head": 7, "mean 100 episode reward": -200.0, "steps": 53199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 268, "head": 7, "mean 100 episode reward": -200.0, "steps": 53399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 269, "head": 2, "mean 100 episode reward": -200.0, "steps": 53599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 270, "head": 3, "mean 100 episode reward": -200.0, "steps": 53799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 271, "head": 0, "mean 100 episode reward": -200.0, "steps": 53999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 272, "head": 2, "mean 100 episode reward": -200.0, "steps": 54199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 273, "head": 5, "mean 100 episode reward": -200.0, "steps": 54399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 274, "head": 8, "mean 100 episode reward": -200.0, "steps": 54599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 275, "head": 9, "mean 100 episode reward": -200.0, "steps": 54799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 276, "head": 4, "mean 100 episode reward": -200.0, "steps": 54999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 277, "head": 3, "mean 100 episode reward": -200.0, "steps": 55199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 278, "head": 0, "mean 100 episode reward": -200.0, "steps": 55399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 279, "head": 9, "mean 100 episode reward": -200.0, "steps": 55599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 280, "head": 1, "mean 100 episode reward": -200.0, "steps": 55799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 281, "head": 8, "mean 100 episode reward": -200.0, "steps": 55999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 282, "head": 4, "mean 100 episode reward": -200.0, "steps": 56199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 283, "head": 8, "mean 100 episode reward": -200.0, "steps": 56399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 284, "head": 0, "mean 100 episode reward": -200.0, "steps": 56599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 285, "head": 1, "mean 100 episode reward": -200.0, "steps": 56799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 286, "head": 5, "mean 100 episode reward": -200.0, "steps": 56999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 287, "head": 1, "mean 100 episode reward": -200.0, "steps": 57199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 288, "head": 2, "mean 100 episode reward": -200.0, "steps": 57399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 289, "head": 9, "mean 100 episode reward": -200.0, "steps": 57599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 290, "head": 3, "mean 100 episode reward": -200.0, "steps": 57799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 291, "head": 5, "mean 100 episode reward": -200.0, "steps": 57999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 292, "head": 6, "mean 100 episode reward": -200.0, "steps": 58199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 293, "head": 0, "mean 100 episode reward": -200.0, "steps": 58399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 294, "head": 8, "mean 100 episode reward": -200.0, "steps": 58599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 295, "head": 3, "mean 100 episode reward": -200.0, "steps": 58799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 296, "head": 4, "mean 100 episode reward": -200.0, "steps": 58999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 297, "head": 4, "mean 100 episode reward": -200.0, "steps": 59199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 298, "head": 2, "mean 100 episode reward": -200.0, "steps": 59399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 299, "head": 7, "mean 100 episode reward": -200.0, "steps": 59599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 300, "head": 7, "mean 100 episode reward": -200.0, "steps": 59799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 301, "head": 9, "mean 100 episode reward": -200.0, "steps": 59999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 302, "head": 0, "mean 100 episode reward": -200.0, "steps": 60199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 303, "head": 3, "mean 100 episode reward": -200.0, "steps": 60399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 304, "head": 4, "mean 100 episode reward": -200.0, "steps": 60599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 305, "head": 0, "mean 100 episode reward": -200.0, "steps": 60799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 306, "head": 2, "mean 100 episode reward": -200.0, "steps": 60999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 307, "head": 4, "mean 100 episode reward": -200.0, "steps": 61199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 308, "head": 2, "mean 100 episode reward": -200.0, "steps": 61399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 309, "head": 2, "mean 100 episode reward": -200.0, "steps": 61599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 310, "head": 5, "mean 100 episode reward": -200.0, "steps": 61799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 311, "head": 1, "mean 100 episode reward": -200.0, "steps": 61999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 312, "head": 9, "mean 100 episode reward": -200.0, "steps": 62199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 313, "head": 2, "mean 100 episode reward": -200.0, "steps": 62399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 314, "head": 2, "mean 100 episode reward": -200.0, "steps": 62599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 315, "head": 2, "mean 100 episode reward": -200.0, "steps": 62799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 316, "head": 6, "mean 100 episode reward": -200.0, "steps": 62999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 317, "head": 0, "mean 100 episode reward": -200.0, "steps": 63199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 318, "head": 7, "mean 100 episode reward": -200.0, "steps": 63399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 319, "head": 4, "mean 100 episode reward": -200.0, "steps": 63599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 320, "head": 8, "mean 100 episode reward": -200.0, "steps": 63799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 321, "head": 9, "mean 100 episode reward": -200.0, "steps": 63999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 322, "head": 3, "mean 100 episode reward": -200.0, "steps": 64199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 323, "head": 9, "mean 100 episode reward": -200.0, "steps": 64399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 324, "head": 9, "mean 100 episode reward": -200.0, "steps": 64599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 325, "head": 8, "mean 100 episode reward": -200.0, "steps": 64799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 326, "head": 8, "mean 100 episode reward": -200.0, "steps": 64999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 327, "head": 7, "mean 100 episode reward": -200.0, "steps": 65199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 328, "head": 4, "mean 100 episode reward": -200.0, "steps": 65399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 329, "head": 2, "mean 100 episode reward": -200.0, "steps": 65599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 330, "head": 2, "mean 100 episode reward": -200.0, "steps": 65799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 331, "head": 3, "mean 100 episode reward": -200.0, "steps": 65999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 332, "head": 3, "mean 100 episode reward": -200.0, "steps": 66199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 333, "head": 7, "mean 100 episode reward": -200.0, "steps": 66399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 334, "head": 4, "mean 100 episode reward": -200.0, "steps": 66599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 335, "head": 8, "mean 100 episode reward": -200.0, "steps": 66799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 336, "head": 7, "mean 100 episode reward": -200.0, "steps": 66999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 337, "head": 6, "mean 100 episode reward": -200.0, "steps": 67199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 338, "head": 0, "mean 100 episode reward": -200.0, "steps": 67399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 339, "head": 9, "mean 100 episode reward": -200.0, "steps": 67599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 340, "head": 9, "mean 100 episode reward": -200.0, "steps": 67799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 341, "head": 8, "mean 100 episode reward": -200.0, "steps": 67999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 342, "head": 9, "mean 100 episode reward": -200.0, "steps": 68199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 343, "head": 0, "mean 100 episode reward": -200.0, "steps": 68399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 344, "head": 6, "mean 100 episode reward": -200.0, "steps": 68599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 345, "head": 7, "mean 100 episode reward": -200.0, "steps": 68799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 346, "head": 4, "mean 100 episode reward": -200.0, "steps": 68999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 347, "head": 0, "mean 100 episode reward": -200.0, "steps": 69199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 348, "head": 9, "mean 100 episode reward": -200.0, "steps": 69399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 349, "head": 1, "mean 100 episode reward": -200.0, "steps": 69599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 350, "head": 9, "mean 100 episode reward": -200.0, "steps": 69799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 351, "head": 6, "mean 100 episode reward": -200.0, "steps": 69999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 352, "head": 4, "mean 100 episode reward": -200.0, "steps": 70199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 353, "head": 9, "mean 100 episode reward": -200.0, "steps": 70399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 354, "head": 2, "mean 100 episode reward": -200.0, "steps": 70599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 355, "head": 9, "mean 100 episode reward": -200.0, "steps": 70799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 356, "head": 0, "mean 100 episode reward": -200.0, "steps": 70999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 357, "head": 2, "mean 100 episode reward": -200.0, "steps": 71199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 358, "head": 8, "mean 100 episode reward": -200.0, "steps": 71399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 359, "head": 2, "mean 100 episode reward": -200.0, "steps": 71599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 360, "head": 0, "mean 100 episode reward": -200.0, "steps": 71799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 361, "head": 5, "mean 100 episode reward": -200.0, "steps": 71999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 362, "head": 9, "mean 100 episode reward": -200.0, "steps": 72199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 363, "head": 9, "mean 100 episode reward": -200.0, "steps": 72399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 364, "head": 0, "mean 100 episode reward": -200.0, "steps": 72599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 365, "head": 3, "mean 100 episode reward": -200.0, "steps": 72799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 366, "head": 0, "mean 100 episode reward": -200.0, "steps": 72999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 367, "head": 8, "mean 100 episode reward": -200.0, "steps": 73199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 368, "head": 1, "mean 100 episode reward": -200.0, "steps": 73399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 369, "head": 0, "mean 100 episode reward": -200.0, "steps": 73599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 370, "head": 6, "mean 100 episode reward": -200.0, "steps": 73799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 371, "head": 0, "mean 100 episode reward": -200.0, "steps": 73999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 372, "head": 0, "mean 100 episode reward": -200.0, "steps": 74199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 373, "head": 2, "mean 100 episode reward": -200.0, "steps": 74399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 374, "head": 4, "mean 100 episode reward": -200.0, "steps": 74599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 375, "head": 3, "mean 100 episode reward": -200.0, "steps": 74799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 376, "head": 9, "mean 100 episode reward": -200.0, "steps": 74999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 377, "head": 9, "mean 100 episode reward": -200.0, "steps": 75199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 378, "head": 1, "mean 100 episode reward": -200.0, "steps": 75399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 379, "head": 5, "mean 100 episode reward": -200.0, "steps": 75599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 380, "head": 0, "mean 100 episode reward": -200.0, "steps": 75799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 381, "head": 2, "mean 100 episode reward": -200.0, "steps": 75999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 382, "head": 2, "mean 100 episode reward": -200.0, "steps": 76199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 383, "head": 8, "mean 100 episode reward": -200.0, "steps": 76399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 384, "head": 5, "mean 100 episode reward": -200.0, "steps": 76599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 385, "head": 0, "mean 100 episode reward": -200.0, "steps": 76799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 386, "head": 7, "mean 100 episode reward": -200.0, "steps": 76999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 387, "head": 9, "mean 100 episode reward": -200.0, "steps": 77199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 388, "head": 0, "mean 100 episode reward": -200.0, "steps": 77399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 389, "head": 2, "mean 100 episode reward": -200.0, "steps": 77599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 390, "head": 8, "mean 100 episode reward": -200.0, "steps": 77799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 391, "head": 0, "mean 100 episode reward": -200.0, "steps": 77999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 392, "head": 5, "mean 100 episode reward": -200.0, "steps": 78199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 393, "head": 3, "mean 100 episode reward": -200.0, "steps": 78399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 394, "head": 5, "mean 100 episode reward": -200.0, "steps": 78599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 395, "head": 7, "mean 100 episode reward": -200.0, "steps": 78799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 396, "head": 9, "mean 100 episode reward": -200.0, "steps": 78999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 397, "head": 6, "mean 100 episode reward": -200.0, "steps": 79199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 398, "head": 6, "mean 100 episode reward": -200.0, "steps": 79399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 399, "head": 4, "mean 100 episode reward": -200.0, "steps": 79599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 400, "head": 6, "mean 100 episode reward": -200.0, "steps": 79799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 401, "head": 1, "mean 100 episode reward": -200.0, "steps": 79999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 402, "head": 9, "mean 100 episode reward": -200.0, "steps": 80199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 403, "head": 1, "mean 100 episode reward": -200.0, "steps": 80399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 404, "head": 2, "mean 100 episode reward": -200.0, "steps": 80599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 405, "head": 4, "mean 100 episode reward": -200.0, "steps": 80799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 406, "head": 4, "mean 100 episode reward": -200.0, "steps": 80999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 407, "head": 6, "mean 100 episode reward": -200.0, "steps": 81199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 408, "head": 2, "mean 100 episode reward": -200.0, "steps": 81399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 409, "head": 3, "mean 100 episode reward": -200.0, "steps": 81599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 410, "head": 4, "mean 100 episode reward": -200.0, "steps": 81799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 411, "head": 4, "mean 100 episode reward": -200.0, "steps": 81999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 412, "head": 3, "mean 100 episode reward": -200.0, "steps": 82199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 413, "head": 0, "mean 100 episode reward": -200.0, "steps": 82399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 414, "head": 8, "mean 100 episode reward": -200.0, "steps": 82599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 415, "head": 7, "mean 100 episode reward": -200.0, "steps": 82799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 416, "head": 5, "mean 100 episode reward": -200.0, "steps": 82999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 417, "head": 1, "mean 100 episode reward": -200.0, "steps": 83199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 418, "head": 1, "mean 100 episode reward": -200.0, "steps": 83399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 419, "head": 3, "mean 100 episode reward": -200.0, "steps": 83599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 420, "head": 7, "mean 100 episode reward": -200.0, "steps": 83799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 421, "head": 9, "mean 100 episode reward": -200.0, "steps": 83999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 422, "head": 5, "mean 100 episode reward": -200.0, "steps": 84199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 423, "head": 9, "mean 100 episode reward": -200.0, "steps": 84399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 424, "head": 0, "mean 100 episode reward": -200.0, "steps": 84599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 425, "head": 7, "mean 100 episode reward": -200.0, "steps": 84799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 426, "head": 8, "mean 100 episode reward": -200.0, "steps": 84999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 427, "head": 9, "mean 100 episode reward": -200.0, "steps": 85199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 428, "head": 9, "mean 100 episode reward": -200.0, "steps": 85399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 429, "head": 5, "mean 100 episode reward": -200.0, "steps": 85599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 430, "head": 0, "mean 100 episode reward": -200.0, "steps": 85799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 431, "head": 5, "mean 100 episode reward": -200.0, "steps": 85999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 432, "head": 9, "mean 100 episode reward": -200.0, "steps": 86199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 433, "head": 9, "mean 100 episode reward": -200.0, "steps": 86399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 434, "head": 1, "mean 100 episode reward": -200.0, "steps": 86599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 435, "head": 4, "mean 100 episode reward": -200.0, "steps": 86799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 436, "head": 9, "mean 100 episode reward": -200.0, "steps": 86999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 437, "head": 6, "mean 100 episode reward": -200.0, "steps": 87199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 438, "head": 2, "mean 100 episode reward": -200.0, "steps": 87399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 439, "head": 3, "mean 100 episode reward": -200.0, "steps": 87599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 440, "head": 4, "mean 100 episode reward": -200.0, "steps": 87799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 441, "head": 7, "mean 100 episode reward": -200.0, "steps": 87999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 442, "head": 4, "mean 100 episode reward": -200.0, "steps": 88199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 443, "head": 7, "mean 100 episode reward": -200.0, "steps": 88399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 444, "head": 6, "mean 100 episode reward": -200.0, "steps": 88599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 445, "head": 4, "mean 100 episode reward": -200.0, "steps": 88799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 446, "head": 6, "mean 100 episode reward": -200.0, "steps": 88999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 447, "head": 3, "mean 100 episode reward": -200.0, "steps": 89199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 448, "head": 1, "mean 100 episode reward": -200.0, "steps": 89399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 449, "head": 9, "mean 100 episode reward": -200.0, "steps": 89599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 450, "head": 2, "mean 100 episode reward": -200.0, "steps": 89799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 451, "head": 4, "mean 100 episode reward": -200.0, "steps": 89999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 452, "head": 2, "mean 100 episode reward": -200.0, "steps": 90199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 453, "head": 8, "mean 100 episode reward": -200.0, "steps": 90399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 454, "head": 8, "mean 100 episode reward": -200.0, "steps": 90599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 455, "head": 9, "mean 100 episode reward": -200.0, "steps": 90799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 456, "head": 3, "mean 100 episode reward": -200.0, "steps": 90999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 457, "head": 5, "mean 100 episode reward": -200.0, "steps": 91199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 458, "head": 3, "mean 100 episode reward": -200.0, "steps": 91399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 459, "head": 1, "mean 100 episode reward": -200.0, "steps": 91599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 460, "head": 4, "mean 100 episode reward": -200.0, "steps": 91799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 461, "head": 5, "mean 100 episode reward": -200.0, "steps": 91999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 462, "head": 4, "mean 100 episode reward": -200.0, "steps": 92199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 463, "head": 9, "mean 100 episode reward": -200.0, "steps": 92399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 464, "head": 7, "mean 100 episode reward": -200.0, "steps": 92599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 465, "head": 9, "mean 100 episode reward": -200.0, "steps": 92799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 466, "head": 9, "mean 100 episode reward": -200.0, "steps": 92999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 467, "head": 9, "mean 100 episode reward": -200.0, "steps": 93199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 468, "head": 4, "mean 100 episode reward": -200.0, "steps": 93399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 469, "head": 0, "mean 100 episode reward": -200.0, "steps": 93599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 470, "head": 9, "mean 100 episode reward": -200.0, "steps": 93799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 471, "head": 4, "mean 100 episode reward": -200.0, "steps": 93999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 472, "head": 2, "mean 100 episode reward": -200.0, "steps": 94199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 473, "head": 2, "mean 100 episode reward": -200.0, "steps": 94399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 474, "head": 7, "mean 100 episode reward": -200.0, "steps": 94599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 475, "head": 7, "mean 100 episode reward": -200.0, "steps": 94799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 476, "head": 6, "mean 100 episode reward": -200.0, "steps": 94999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 477, "head": 0, "mean 100 episode reward": -200.0, "steps": 95199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 478, "head": 9, "mean 100 episode reward": -200.0, "steps": 95399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 479, "head": 6, "mean 100 episode reward": -200.0, "steps": 95599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 480, "head": 1, "mean 100 episode reward": -200.0, "steps": 95799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 481, "head": 2, "mean 100 episode reward": -200.0, "steps": 95999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 482, "head": 2, "mean 100 episode reward": -200.0, "steps": 96199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 483, "head": 6, "mean 100 episode reward": -200.0, "steps": 96399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 484, "head": 3, "mean 100 episode reward": -200.0, "steps": 96599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 485, "head": 1, "mean 100 episode reward": -200.0, "steps": 96799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 486, "head": 6, "mean 100 episode reward": -200.0, "steps": 96999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 487, "head": 3, "mean 100 episode reward": -200.0, "steps": 97199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 488, "head": 7, "mean 100 episode reward": -200.0, "steps": 97399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 489, "head": 2, "mean 100 episode reward": -200.0, "steps": 97599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 490, "head": 0, "mean 100 episode reward": -200.0, "steps": 97799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 491, "head": 6, "mean 100 episode reward": -200.0, "steps": 97999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 492, "head": 4, "mean 100 episode reward": -200.0, "steps": 98199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 493, "head": 9, "mean 100 episode reward": -200.0, "steps": 98399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 494, "head": 2, "mean 100 episode reward": -200.0, "steps": 98599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 495, "head": 8, "mean 100 episode reward": -200.0, "steps": 98799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 496, "head": 0, "mean 100 episode reward": -200.0, "steps": 98999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 497, "head": 1, "mean 100 episode reward": -200.0, "steps": 99199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 498, "head": 9, "mean 100 episode reward": -200.0, "steps": 99399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 499, "head": 9, "mean 100 episode reward": -200.0, "steps": 99599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 500, "head": 7, "mean 100 episode reward": -200.0, "steps": 99799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 501, "head": 3, "mean 100 episode reward": -200.0, "steps": 99999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 502, "head": 9, "mean 100 episode reward": -200.0, "steps": 100199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 503, "head": 9, "mean 100 episode reward": -200.0, "steps": 100399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 504, "head": 4, "mean 100 episode reward": -200.0, "steps": 100599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 505, "head": 3, "mean 100 episode reward": -200.0, "steps": 100799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 506, "head": 5, "mean 100 episode reward": -200.0, "steps": 100999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 507, "head": 4, "mean 100 episode reward": -200.0, "steps": 101199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 508, "head": 4, "mean 100 episode reward": -200.0, "steps": 101399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 509, "head": 5, "mean 100 episode reward": -200.0, "steps": 101599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 510, "head": 5, "mean 100 episode reward": -200.0, "steps": 101799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 511, "head": 6, "mean 100 episode reward": -200.0, "steps": 101999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 512, "head": 1, "mean 100 episode reward": -200.0, "steps": 102199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 513, "head": 7, "mean 100 episode reward": -200.0, "steps": 102399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 514, "head": 7, "mean 100 episode reward": -200.0, "steps": 102599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 515, "head": 9, "mean 100 episode reward": -200.0, "steps": 102799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 516, "head": 1, "mean 100 episode reward": -200.0, "steps": 102999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 517, "head": 7, "mean 100 episode reward": -200.0, "steps": 103199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 518, "head": 0, "mean 100 episode reward": -200.0, "steps": 103399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 519, "head": 1, "mean 100 episode reward": -200.0, "steps": 103599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 520, "head": 8, "mean 100 episode reward": -200.0, "steps": 103799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 521, "head": 7, "mean 100 episode reward": -200.0, "steps": 103999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 522, "head": 7, "mean 100 episode reward": -200.0, "steps": 104199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 523, "head": 3, "mean 100 episode reward": -200.0, "steps": 104399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 524, "head": 1, "mean 100 episode reward": -200.0, "steps": 104599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 525, "head": 9, "mean 100 episode reward": -200.0, "steps": 104799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 526, "head": 6, "mean 100 episode reward": -200.0, "steps": 104999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 527, "head": 9, "mean 100 episode reward": -200.0, "steps": 105199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 528, "head": 0, "mean 100 episode reward": -200.0, "steps": 105399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 529, "head": 0, "mean 100 episode reward": -200.0, "steps": 105599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 530, "head": 7, "mean 100 episode reward": -200.0, "steps": 105799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 531, "head": 2, "mean 100 episode reward": -200.0, "steps": 105999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 532, "head": 2, "mean 100 episode reward": -200.0, "steps": 106199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 533, "head": 6, "mean 100 episode reward": -200.0, "steps": 106399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 534, "head": 3, "mean 100 episode reward": -200.0, "steps": 106599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 535, "head": 7, "mean 100 episode reward": -200.0, "steps": 106799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 536, "head": 3, "mean 100 episode reward": -200.0, "steps": 106999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 537, "head": 6, "mean 100 episode reward": -200.0, "steps": 107199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 538, "head": 9, "mean 100 episode reward": -200.0, "steps": 107399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 539, "head": 1, "mean 100 episode reward": -200.0, "steps": 107599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 540, "head": 5, "mean 100 episode reward": -200.0, "steps": 107799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 541, "head": 9, "mean 100 episode reward": -200.0, "steps": 107999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 542, "head": 1, "mean 100 episode reward": -200.0, "steps": 108199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 543, "head": 1, "mean 100 episode reward": -200.0, "steps": 108399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 544, "head": 1, "mean 100 episode reward": -200.0, "steps": 108599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 545, "head": 8, "mean 100 episode reward": -200.0, "steps": 108799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 546, "head": 0, "mean 100 episode reward": -200.0, "steps": 108999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 547, "head": 5, "mean 100 episode reward": -200.0, "steps": 109199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 548, "head": 5, "mean 100 episode reward": -200.0, "steps": 109399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 549, "head": 0, "mean 100 episode reward": -200.0, "steps": 109599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 550, "head": 2, "mean 100 episode reward": -200.0, "steps": 109799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 551, "head": 3, "mean 100 episode reward": -200.0, "steps": 109999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 552, "head": 3, "mean 100 episode reward": -200.0, "steps": 110199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 553, "head": 5, "mean 100 episode reward": -200.0, "steps": 110399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 554, "head": 9, "mean 100 episode reward": -200.0, "steps": 110599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 555, "head": 0, "mean 100 episode reward": -200.0, "steps": 110799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 556, "head": 3, "mean 100 episode reward": -200.0, "steps": 110999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 557, "head": 5, "mean 100 episode reward": -200.0, "steps": 111199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 558, "head": 1, "mean 100 episode reward": -200.0, "steps": 111399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 559, "head": 4, "mean 100 episode reward": -200.0, "steps": 111599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 560, "head": 6, "mean 100 episode reward": -200.0, "steps": 111799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 561, "head": 0, "mean 100 episode reward": -200.0, "steps": 111999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 562, "head": 0, "mean 100 episode reward": -200.0, "steps": 112199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 563, "head": 5, "mean 100 episode reward": -200.0, "steps": 112399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 564, "head": 3, "mean 100 episode reward": -200.0, "steps": 112599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 565, "head": 0, "mean 100 episode reward": -200.0, "steps": 112799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 566, "head": 5, "mean 100 episode reward": -200.0, "steps": 112999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 567, "head": 5, "mean 100 episode reward": -200.0, "steps": 113199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 568, "head": 8, "mean 100 episode reward": -200.0, "steps": 113399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 569, "head": 5, "mean 100 episode reward": -200.0, "steps": 113599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 570, "head": 5, "mean 100 episode reward": -200.0, "steps": 113799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 571, "head": 4, "mean 100 episode reward": -200.0, "steps": 113999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 572, "head": 4, "mean 100 episode reward": -200.0, "steps": 114199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 573, "head": 6, "mean 100 episode reward": -200.0, "steps": 114399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 574, "head": 5, "mean 100 episode reward": -200.0, "steps": 114599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 575, "head": 3, "mean 100 episode reward": -200.0, "steps": 114799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 576, "head": 1, "mean 100 episode reward": -200.0, "steps": 114999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 577, "head": 5, "mean 100 episode reward": -200.0, "steps": 115199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 578, "head": 9, "mean 100 episode reward": -200.0, "steps": 115399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 579, "head": 7, "mean 100 episode reward": -200.0, "steps": 115599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 580, "head": 9, "mean 100 episode reward": -200.0, "steps": 115799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 581, "head": 1, "mean 100 episode reward": -200.0, "steps": 115999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 582, "head": 2, "mean 100 episode reward": -200.0, "steps": 116199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 583, "head": 2, "mean 100 episode reward": -200.0, "steps": 116399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 584, "head": 0, "mean 100 episode reward": -200.0, "steps": 116599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 585, "head": 4, "mean 100 episode reward": -200.0, "steps": 116799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 586, "head": 8, "mean 100 episode reward": -200.0, "steps": 116999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 587, "head": 1, "mean 100 episode reward": -200.0, "steps": 117199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 588, "head": 6, "mean 100 episode reward": -200.0, "steps": 117399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 589, "head": 5, "mean 100 episode reward": -200.0, "steps": 117599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 590, "head": 6, "mean 100 episode reward": -200.0, "steps": 117799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 591, "head": 6, "mean 100 episode reward": -200.0, "steps": 117999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 592, "head": 6, "mean 100 episode reward": -200.0, "steps": 118199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 593, "head": 9, "mean 100 episode reward": -200.0, "steps": 118399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 594, "head": 1, "mean 100 episode reward": -200.0, "steps": 118599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 595, "head": 7, "mean 100 episode reward": -200.0, "steps": 118799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 596, "head": 6, "mean 100 episode reward": -200.0, "steps": 118999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 597, "head": 5, "mean 100 episode reward": -200.0, "steps": 119199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 598, "head": 5, "mean 100 episode reward": -200.0, "steps": 119399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 599, "head": 3, "mean 100 episode reward": -200.0, "steps": 119599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 600, "head": 0, "mean 100 episode reward": -200.0, "steps": 119799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 601, "head": 3, "mean 100 episode reward": -200.0, "steps": 119999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 602, "head": 5, "mean 100 episode reward": -200.0, "steps": 120199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 603, "head": 3, "mean 100 episode reward": -200.0, "steps": 120399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 604, "head": 5, "mean 100 episode reward": -200.0, "steps": 120599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 605, "head": 5, "mean 100 episode reward": -200.0, "steps": 120799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 606, "head": 6, "mean 100 episode reward": -200.0, "steps": 120999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 607, "head": 2, "mean 100 episode reward": -200.0, "steps": 121199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 608, "head": 8, "mean 100 episode reward": -200.0, "steps": 121399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 609, "head": 0, "mean 100 episode reward": -200.0, "steps": 121599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 610, "head": 5, "mean 100 episode reward": -200.0, "steps": 121799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 611, "head": 7, "mean 100 episode reward": -200.0, "steps": 121999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 612, "head": 0, "mean 100 episode reward": -200.0, "steps": 122199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 613, "head": 7, "mean 100 episode reward": -200.0, "steps": 122399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 614, "head": 7, "mean 100 episode reward": -200.0, "steps": 122599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 615, "head": 2, "mean 100 episode reward": -200.0, "steps": 122799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 616, "head": 4, "mean 100 episode reward": -200.0, "steps": 122999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 617, "head": 4, "mean 100 episode reward": -200.0, "steps": 123199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 618, "head": 0, "mean 100 episode reward": -200.0, "steps": 123399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 619, "head": 9, "mean 100 episode reward": -200.0, "steps": 123599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 620, "head": 5, "mean 100 episode reward": -200.0, "steps": 123799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 621, "head": 6, "mean 100 episode reward": -200.0, "steps": 123999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 622, "head": 6, "mean 100 episode reward": -200.0, "steps": 124199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 623, "head": 5, "mean 100 episode reward": -200.0, "steps": 124399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 624, "head": 3, "mean 100 episode reward": -200.0, "steps": 124599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 625, "head": 3, "mean 100 episode reward": -200.0, "steps": 124799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 626, "head": 4, "mean 100 episode reward": -200.0, "steps": 124999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 627, "head": 9, "mean 100 episode reward": -200.0, "steps": 125199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 628, "head": 4, "mean 100 episode reward": -200.0, "steps": 125399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 629, "head": 8, "mean 100 episode reward": -200.0, "steps": 125599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 630, "head": 0, "mean 100 episode reward": -200.0, "steps": 125799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 631, "head": 6, "mean 100 episode reward": -200.0, "steps": 125999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 632, "head": 8, "mean 100 episode reward": -200.0, "steps": 126199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 633, "head": 0, "mean 100 episode reward": -200.0, "steps": 126399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 634, "head": 3, "mean 100 episode reward": -200.0, "steps": 126599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 635, "head": 8, "mean 100 episode reward": -200.0, "steps": 126799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 636, "head": 2, "mean 100 episode reward": -200.0, "steps": 126999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 637, "head": 3, "mean 100 episode reward": -200.0, "steps": 127199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 638, "head": 6, "mean 100 episode reward": -200.0, "steps": 127399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 639, "head": 4, "mean 100 episode reward": -200.0, "steps": 127599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 640, "head": 1, "mean 100 episode reward": -200.0, "steps": 127799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 641, "head": 0, "mean 100 episode reward": -200.0, "steps": 127999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 642, "head": 5, "mean 100 episode reward": -200.0, "steps": 128199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 643, "head": 5, "mean 100 episode reward": -200.0, "steps": 128399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 644, "head": 8, "mean 100 episode reward": -200.0, "steps": 128599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 645, "head": 5, "mean 100 episode reward": -200.0, "steps": 128799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 646, "head": 7, "mean 100 episode reward": -200.0, "steps": 128999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 647, "head": 4, "mean 100 episode reward": -200.0, "steps": 129199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 648, "head": 4, "mean 100 episode reward": -200.0, "steps": 129399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 649, "head": 4, "mean 100 episode reward": -200.0, "steps": 129599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 650, "head": 4, "mean 100 episode reward": -200.0, "steps": 129799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 651, "head": 6, "mean 100 episode reward": -200.0, "steps": 129999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 652, "head": 9, "mean 100 episode reward": -200.0, "steps": 130199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 653, "head": 3, "mean 100 episode reward": -200.0, "steps": 130399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 654, "head": 1, "mean 100 episode reward": -200.0, "steps": 130599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 655, "head": 6, "mean 100 episode reward": -200.0, "steps": 130799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 656, "head": 3, "mean 100 episode reward": -200.0, "steps": 130999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 657, "head": 9, "mean 100 episode reward": -200.0, "steps": 131199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 658, "head": 0, "mean 100 episode reward": -200.0, "steps": 131399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 659, "head": 3, "mean 100 episode reward": -200.0, "steps": 131599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 660, "head": 5, "mean 100 episode reward": -200.0, "steps": 131799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 661, "head": 2, "mean 100 episode reward": -200.0, "steps": 131999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 662, "head": 2, "mean 100 episode reward": -200.0, "steps": 132199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 663, "head": 7, "mean 100 episode reward": -200.0, "steps": 132399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 664, "head": 6, "mean 100 episode reward": -200.0, "steps": 132599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 665, "head": 8, "mean 100 episode reward": -200.0, "steps": 132799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 666, "head": 5, "mean 100 episode reward": -200.0, "steps": 132999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 667, "head": 8, "mean 100 episode reward": -200.0, "steps": 133199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 668, "head": 6, "mean 100 episode reward": -200.0, "steps": 133399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 669, "head": 9, "mean 100 episode reward": -200.0, "steps": 133599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 670, "head": 6, "mean 100 episode reward": -200.0, "steps": 133799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 671, "head": 2, "mean 100 episode reward": -200.0, "steps": 133999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 672, "head": 6, "mean 100 episode reward": -200.0, "steps": 134199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 673, "head": 1, "mean 100 episode reward": -200.0, "steps": 134399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 674, "head": 7, "mean 100 episode reward": -200.0, "steps": 134599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 675, "head": 3, "mean 100 episode reward": -200.0, "steps": 134799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 676, "head": 5, "mean 100 episode reward": -200.0, "steps": 134999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 677, "head": 3, "mean 100 episode reward": -200.0, "steps": 135199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 678, "head": 1, "mean 100 episode reward": -200.0, "steps": 135399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 679, "head": 6, "mean 100 episode reward": -200.0, "steps": 135599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 680, "head": 2, "mean 100 episode reward": -200.0, "steps": 135799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 681, "head": 0, "mean 100 episode reward": -200.0, "steps": 135999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 682, "head": 0, "mean 100 episode reward": -200.0, "steps": 136199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 683, "head": 4, "mean 100 episode reward": -200.0, "steps": 136399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 684, "head": 2, "mean 100 episode reward": -200.0, "steps": 136599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 685, "head": 3, "mean 100 episode reward": -200.0, "steps": 136799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 686, "head": 0, "mean 100 episode reward": -200.0, "steps": 136999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 687, "head": 8, "mean 100 episode reward": -200.0, "steps": 137199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 688, "head": 0, "mean 100 episode reward": -200.0, "steps": 137399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 689, "head": 8, "mean 100 episode reward": -200.0, "steps": 137599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 690, "head": 7, "mean 100 episode reward": -200.0, "steps": 137799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 691, "head": 4, "mean 100 episode reward": -200.0, "steps": 137999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 692, "head": 2, "mean 100 episode reward": -200.0, "steps": 138199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 693, "head": 1, "mean 100 episode reward": -200.0, "steps": 138399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 694, "head": 1, "mean 100 episode reward": -200.0, "steps": 138599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 695, "head": 3, "mean 100 episode reward": -200.0, "steps": 138799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 696, "head": 9, "mean 100 episode reward": -200.0, "steps": 138999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 697, "head": 9, "mean 100 episode reward": -200.0, "steps": 139199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 698, "head": 8, "mean 100 episode reward": -200.0, "steps": 139399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 699, "head": 5, "mean 100 episode reward": -200.0, "steps": 139599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 700, "head": 9, "mean 100 episode reward": -200.0, "steps": 139799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 701, "head": 8, "mean 100 episode reward": -200.0, "steps": 139999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 702, "head": 2, "mean 100 episode reward": -200.0, "steps": 140199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 703, "head": 0, "mean 100 episode reward": -200.0, "steps": 140399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 704, "head": 5, "mean 100 episode reward": -200.0, "steps": 140599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 705, "head": 6, "mean 100 episode reward": -200.0, "steps": 140799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 706, "head": 9, "mean 100 episode reward": -200.0, "steps": 140999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 707, "head": 3, "mean 100 episode reward": -200.0, "steps": 141199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 708, "head": 0, "mean 100 episode reward": -200.0, "steps": 141399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 709, "head": 9, "mean 100 episode reward": -200.0, "steps": 141599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 710, "head": 7, "mean 100 episode reward": -200.0, "steps": 141799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 711, "head": 5, "mean 100 episode reward": -200.0, "steps": 141999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 712, "head": 9, "mean 100 episode reward": -200.0, "steps": 142199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 713, "head": 8, "mean 100 episode reward": -200.0, "steps": 142399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 714, "head": 8, "mean 100 episode reward": -200.0, "steps": 142599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 715, "head": 4, "mean 100 episode reward": -200.0, "steps": 142799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 716, "head": 8, "mean 100 episode reward": -200.0, "steps": 142999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 717, "head": 9, "mean 100 episode reward": -200.0, "steps": 143199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 718, "head": 3, "mean 100 episode reward": -200.0, "steps": 143399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 719, "head": 5, "mean 100 episode reward": -200.0, "steps": 143599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 720, "head": 9, "mean 100 episode reward": -200.0, "steps": 143799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 721, "head": 7, "mean 100 episode reward": -200.0, "steps": 143999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 722, "head": 1, "mean 100 episode reward": -200.0, "steps": 144199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 723, "head": 1, "mean 100 episode reward": -200.0, "steps": 144399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 724, "head": 9, "mean 100 episode reward": -200.0, "steps": 144599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 725, "head": 2, "mean 100 episode reward": -200.0, "steps": 144799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 726, "head": 2, "mean 100 episode reward": -200.0, "steps": 144999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 727, "head": 4, "mean 100 episode reward": -200.0, "steps": 145199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 728, "head": 5, "mean 100 episode reward": -200.0, "steps": 145399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 729, "head": 2, "mean 100 episode reward": -200.0, "steps": 145599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 730, "head": 3, "mean 100 episode reward": -200.0, "steps": 145799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 731, "head": 3, "mean 100 episode reward": -200.0, "steps": 145999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 732, "head": 6, "mean 100 episode reward": -200.0, "steps": 146199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 733, "head": 2, "mean 100 episode reward": -200.0, "steps": 146399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 734, "head": 5, "mean 100 episode reward": -200.0, "steps": 146599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 735, "head": 2, "mean 100 episode reward": -200.0, "steps": 146799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 736, "head": 8, "mean 100 episode reward": -200.0, "steps": 146999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 737, "head": 3, "mean 100 episode reward": -200.0, "steps": 147199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 738, "head": 6, "mean 100 episode reward": -200.0, "steps": 147399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 739, "head": 8, "mean 100 episode reward": -200.0, "steps": 147599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 740, "head": 2, "mean 100 episode reward": -200.0, "steps": 147799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 741, "head": 8, "mean 100 episode reward": -200.0, "steps": 147999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 742, "head": 0, "mean 100 episode reward": -200.0, "steps": 148199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 743, "head": 3, "mean 100 episode reward": -200.0, "steps": 148399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 744, "head": 4, "mean 100 episode reward": -200.0, "steps": 148599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 745, "head": 5, "mean 100 episode reward": -200.0, "steps": 148799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 746, "head": 3, "mean 100 episode reward": -200.0, "steps": 148999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 747, "head": 3, "mean 100 episode reward": -200.0, "steps": 149199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 748, "head": 8, "mean 100 episode reward": -200.0, "steps": 149399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 749, "head": 2, "mean 100 episode reward": -200.0, "steps": 149599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 750, "head": 0, "mean 100 episode reward": -200.0, "steps": 149799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 751, "head": 4, "mean 100 episode reward": -200.0, "steps": 149999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 752, "head": 5, "mean 100 episode reward": -200.0, "steps": 150199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 753, "head": 2, "mean 100 episode reward": -200.0, "steps": 150399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 754, "head": 1, "mean 100 episode reward": -200.0, "steps": 150599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 755, "head": 8, "mean 100 episode reward": -200.0, "steps": 150799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 756, "head": 6, "mean 100 episode reward": -200.0, "steps": 150999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 757, "head": 9, "mean 100 episode reward": -200.0, "steps": 151199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 758, "head": 5, "mean 100 episode reward": -200.0, "steps": 151399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 759, "head": 5, "mean 100 episode reward": -200.0, "steps": 151599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 760, "head": 5, "mean 100 episode reward": -200.0, "steps": 151799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 761, "head": 2, "mean 100 episode reward": -200.0, "steps": 151999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 762, "head": 0, "mean 100 episode reward": -200.0, "steps": 152199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 763, "head": 6, "mean 100 episode reward": -200.0, "steps": 152399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 764, "head": 9, "mean 100 episode reward": -200.0, "steps": 152599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 765, "head": 1, "mean 100 episode reward": -200.0, "steps": 152799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 766, "head": 8, "mean 100 episode reward": -200.0, "steps": 152999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 767, "head": 0, "mean 100 episode reward": -200.0, "steps": 153199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 768, "head": 7, "mean 100 episode reward": -200.0, "steps": 153399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 769, "head": 5, "mean 100 episode reward": -200.0, "steps": 153599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 770, "head": 8, "mean 100 episode reward": -200.0, "steps": 153799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 771, "head": 6, "mean 100 episode reward": -200.0, "steps": 153999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 772, "head": 9, "mean 100 episode reward": -200.0, "steps": 154199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 773, "head": 5, "mean 100 episode reward": -200.0, "steps": 154399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 774, "head": 8, "mean 100 episode reward": -200.0, "steps": 154599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 775, "head": 4, "mean 100 episode reward": -200.0, "steps": 154799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 776, "head": 0, "mean 100 episode reward": -200.0, "steps": 154999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 777, "head": 5, "mean 100 episode reward": -200.0, "steps": 155199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 778, "head": 8, "mean 100 episode reward": -200.0, "steps": 155399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 779, "head": 1, "mean 100 episode reward": -200.0, "steps": 155599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 780, "head": 6, "mean 100 episode reward": -200.0, "steps": 155799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 781, "head": 4, "mean 100 episode reward": -200.0, "steps": 155999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 782, "head": 2, "mean 100 episode reward": -200.0, "steps": 156199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 783, "head": 6, "mean 100 episode reward": -200.0, "steps": 156399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 784, "head": 0, "mean 100 episode reward": -200.0, "steps": 156599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 785, "head": 3, "mean 100 episode reward": -200.0, "steps": 156799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 786, "head": 0, "mean 100 episode reward": -200.0, "steps": 156999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 787, "head": 0, "mean 100 episode reward": -200.0, "steps": 157199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 788, "head": 3, "mean 100 episode reward": -200.0, "steps": 157399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 789, "head": 3, "mean 100 episode reward": -200.0, "steps": 157599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 790, "head": 5, "mean 100 episode reward": -200.0, "steps": 157799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 791, "head": 1, "mean 100 episode reward": -200.0, "steps": 157999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 792, "head": 8, "mean 100 episode reward": -200.0, "steps": 158199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 793, "head": 4, "mean 100 episode reward": -200.0, "steps": 158399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 794, "head": 9, "mean 100 episode reward": -200.0, "steps": 158599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 795, "head": 9, "mean 100 episode reward": -200.0, "steps": 158799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 796, "head": 5, "mean 100 episode reward": -200.0, "steps": 158999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 797, "head": 4, "mean 100 episode reward": -200.0, "steps": 159199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 798, "head": 3, "mean 100 episode reward": -200.0, "steps": 159399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 799, "head": 2, "mean 100 episode reward": -200.0, "steps": 159599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 800, "head": 5, "mean 100 episode reward": -200.0, "steps": 159799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 801, "head": 1, "mean 100 episode reward": -200.0, "steps": 159999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 802, "head": 8, "mean 100 episode reward": -200.0, "steps": 160199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 803, "head": 7, "mean 100 episode reward": -200.0, "steps": 160399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 804, "head": 6, "mean 100 episode reward": -200.0, "steps": 160599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 805, "head": 7, "mean 100 episode reward": -200.0, "steps": 160799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 806, "head": 9, "mean 100 episode reward": -200.0, "steps": 160999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 807, "head": 3, "mean 100 episode reward": -200.0, "steps": 161199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 808, "head": 8, "mean 100 episode reward": -200.0, "steps": 161399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 809, "head": 7, "mean 100 episode reward": -200.0, "steps": 161599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 810, "head": 9, "mean 100 episode reward": -200.0, "steps": 161799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 811, "head": 4, "mean 100 episode reward": -200.0, "steps": 161999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 812, "head": 4, "mean 100 episode reward": -200.0, "steps": 162199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 813, "head": 8, "mean 100 episode reward": -200.0, "steps": 162399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 814, "head": 1, "mean 100 episode reward": -200.0, "steps": 162599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 815, "head": 7, "mean 100 episode reward": -200.0, "steps": 162799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 816, "head": 9, "mean 100 episode reward": -200.0, "steps": 162999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 817, "head": 5, "mean 100 episode reward": -200.0, "steps": 163199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 818, "head": 4, "mean 100 episode reward": -200.0, "steps": 163399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 819, "head": 7, "mean 100 episode reward": -200.0, "steps": 163599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 820, "head": 3, "mean 100 episode reward": -200.0, "steps": 163799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 821, "head": 9, "mean 100 episode reward": -200.0, "steps": 163999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 822, "head": 7, "mean 100 episode reward": -200.0, "steps": 164199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 823, "head": 4, "mean 100 episode reward": -200.0, "steps": 164399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 824, "head": 2, "mean 100 episode reward": -200.0, "steps": 164599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 825, "head": 2, "mean 100 episode reward": -200.0, "steps": 164799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 826, "head": 5, "mean 100 episode reward": -200.0, "steps": 164999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 827, "head": 9, "mean 100 episode reward": -200.0, "steps": 165199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 828, "head": 6, "mean 100 episode reward": -200.0, "steps": 165399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 829, "head": 0, "mean 100 episode reward": -200.0, "steps": 165599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 830, "head": 3, "mean 100 episode reward": -200.0, "steps": 165799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 831, "head": 4, "mean 100 episode reward": -200.0, "steps": 165999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 832, "head": 1, "mean 100 episode reward": -200.0, "steps": 166199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 833, "head": 8, "mean 100 episode reward": -200.0, "steps": 166399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 834, "head": 9, "mean 100 episode reward": -200.0, "steps": 166599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 835, "head": 7, "mean 100 episode reward": -200.0, "steps": 166799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 836, "head": 2, "mean 100 episode reward": -200.0, "steps": 166999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 837, "head": 8, "mean 100 episode reward": -200.0, "steps": 167199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 838, "head": 6, "mean 100 episode reward": -200.0, "steps": 167399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 839, "head": 8, "mean 100 episode reward": -200.0, "steps": 167599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 840, "head": 3, "mean 100 episode reward": -200.0, "steps": 167799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 841, "head": 1, "mean 100 episode reward": -200.0, "steps": 167999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 842, "head": 6, "mean 100 episode reward": -200.0, "steps": 168199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 843, "head": 3, "mean 100 episode reward": -200.0, "steps": 168399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 844, "head": 0, "mean 100 episode reward": -200.0, "steps": 168599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 845, "head": 0, "mean 100 episode reward": -200.0, "steps": 168799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 846, "head": 6, "mean 100 episode reward": -200.0, "steps": 168999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 847, "head": 8, "mean 100 episode reward": -200.0, "steps": 169199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 848, "head": 4, "mean 100 episode reward": -200.0, "steps": 169399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 849, "head": 9, "mean 100 episode reward": -200.0, "steps": 169599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 850, "head": 5, "mean 100 episode reward": -200.0, "steps": 169799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 851, "head": 9, "mean 100 episode reward": -200.0, "steps": 169999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 852, "head": 4, "mean 100 episode reward": -200.0, "steps": 170199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 853, "head": 1, "mean 100 episode reward": -200.0, "steps": 170399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 854, "head": 2, "mean 100 episode reward": -200.0, "steps": 170599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 855, "head": 2, "mean 100 episode reward": -200.0, "steps": 170799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 856, "head": 2, "mean 100 episode reward": -200.0, "steps": 170999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 857, "head": 0, "mean 100 episode reward": -200.0, "steps": 171199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 858, "head": 8, "mean 100 episode reward": -200.0, "steps": 171399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 859, "head": 3, "mean 100 episode reward": -200.0, "steps": 171599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 860, "head": 8, "mean 100 episode reward": -200.0, "steps": 171799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 861, "head": 8, "mean 100 episode reward": -200.0, "steps": 171999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 862, "head": 1, "mean 100 episode reward": -200.0, "steps": 172199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 863, "head": 1, "mean 100 episode reward": -200.0, "steps": 172399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 864, "head": 0, "mean 100 episode reward": -200.0, "steps": 172599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 865, "head": 2, "mean 100 episode reward": -200.0, "steps": 172799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 866, "head": 4, "mean 100 episode reward": -200.0, "steps": 172999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 867, "head": 8, "mean 100 episode reward": -200.0, "steps": 173199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 868, "head": 6, "mean 100 episode reward": -200.0, "steps": 173399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 869, "head": 6, "mean 100 episode reward": -200.0, "steps": 173599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 870, "head": 6, "mean 100 episode reward": -200.0, "steps": 173799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 871, "head": 4, "mean 100 episode reward": -200.0, "steps": 173999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 872, "head": 7, "mean 100 episode reward": -200.0, "steps": 174199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 873, "head": 0, "mean 100 episode reward": -200.0, "steps": 174399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 874, "head": 0, "mean 100 episode reward": -200.0, "steps": 174599, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 875, "head": 2, "mean 100 episode reward": -200.0, "steps": 174799, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 876, "head": 6, "mean 100 episode reward": -200.0, "steps": 174999, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 877, "head": 6, "mean 100 episode reward": -200.0, "steps": 175199, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 878, "head": 8, "mean 100 episode reward": -200.0, "steps": 175399, "reward": -200.0, "% time spent exploring": 9}
{"episodes": 879, "head": 5, "mean 100 episode reward": -200.0, "steps": 175599, "reward": -200.0, "% time spent exploring": 9}
