{"steps": 199, "episodes": 2, "reward": -200.0, "% time spent exploring": 99, "mean 100 episode reward": -200.0}
{"steps": 399, "episodes": 3, "reward": -200.0, "% time spent exploring": 99, "mean 100 episode reward": -200.0}
{"steps": 599, "episodes": 4, "reward": -200.0, "% time spent exploring": 98, "mean 100 episode reward": -200.0}
{"steps": 799, "episodes": 5, "reward": -200.0, "% time spent exploring": 98, "mean 100 episode reward": -200.0}
{"steps": 999, "episodes": 6, "reward": -200.0, "% time spent exploring": 97, "mean 100 episode reward": -200.0}
{"steps": 1199, "episodes": 7, "reward": -200.0, "% time spent exploring": 97, "mean 100 episode reward": -200.0}
{"steps": 1399, "episodes": 8, "reward": -200.0, "% time spent exploring": 96, "mean 100 episode reward": -200.0}
{"steps": 1599, "episodes": 9, "reward": -200.0, "% time spent exploring": 96, "mean 100 episode reward": -200.0}
{"steps": 1799, "episodes": 10, "reward": -200.0, "% time spent exploring": 95, "mean 100 episode reward": -200.0}
{"steps": 1999, "episodes": 11, "reward": -200.0, "% time spent exploring": 95, "mean 100 episode reward": -200.0}
{"steps": 2199, "episodes": 12, "reward": -200.0, "% time spent exploring": 95, "mean 100 episode reward": -200.0}
{"steps": 2399, "episodes": 13, "reward": -200.0, "% time spent exploring": 94, "mean 100 episode reward": -200.0}
{"steps": 2599, "episodes": 14, "reward": -200.0, "% time spent exploring": 94, "mean 100 episode reward": -200.0}
{"steps": 2799, "episodes": 15, "reward": -200.0, "% time spent exploring": 93, "mean 100 episode reward": -200.0}
{"steps": 2999, "episodes": 16, "reward": -200.0, "% time spent exploring": 93, "mean 100 episode reward": -200.0}
{"steps": 3199, "episodes": 17, "reward": -200.0, "% time spent exploring": 92, "mean 100 episode reward": -200.0}
{"steps": 3399, "episodes": 18, "reward": -200.0, "% time spent exploring": 92, "mean 100 episode reward": -200.0}
{"steps": 3599, "episodes": 19, "reward": -200.0, "% time spent exploring": 91, "mean 100 episode reward": -200.0}
{"steps": 3799, "episodes": 20, "reward": -200.0, "% time spent exploring": 91, "mean 100 episode reward": -200.0}
{"steps": 3999, "episodes": 21, "reward": -200.0, "% time spent exploring": 91, "mean 100 episode reward": -200.0}
{"steps": 4199, "episodes": 22, "reward": -200.0, "% time spent exploring": 90, "mean 100 episode reward": -200.0}
{"steps": 4399, "episodes": 23, "reward": -200.0, "% time spent exploring": 90, "mean 100 episode reward": -200.0}
{"steps": 4599, "episodes": 24, "reward": -200.0, "% time spent exploring": 89, "mean 100 episode reward": -200.0}
{"steps": 4799, "episodes": 25, "reward": -200.0, "% time spent exploring": 89, "mean 100 episode reward": -200.0}
{"steps": 4999, "episodes": 26, "reward": -200.0, "% time spent exploring": 88, "mean 100 episode reward": -200.0}
{"steps": 5199, "episodes": 27, "reward": -200.0, "% time spent exploring": 88, "mean 100 episode reward": -200.0}
{"steps": 5399, "episodes": 28, "reward": -200.0, "% time spent exploring": 87, "mean 100 episode reward": -200.0}
{"steps": 5599, "episodes": 29, "reward": -200.0, "% time spent exploring": 87, "mean 100 episode reward": -200.0}
{"steps": 5799, "episodes": 30, "reward": -200.0, "% time spent exploring": 86, "mean 100 episode reward": -200.0}
{"steps": 5999, "episodes": 31, "reward": -200.0, "% time spent exploring": 86, "mean 100 episode reward": -200.0}
{"steps": 6199, "episodes": 32, "reward": -200.0, "% time spent exploring": 86, "mean 100 episode reward": -200.0}
{"steps": 6399, "episodes": 33, "reward": -200.0, "% time spent exploring": 85, "mean 100 episode reward": -200.0}
{"steps": 6599, "episodes": 34, "reward": -200.0, "% time spent exploring": 85, "mean 100 episode reward": -200.0}
{"steps": 6799, "episodes": 35, "reward": -200.0, "% time spent exploring": 84, "mean 100 episode reward": -200.0}
{"steps": 6999, "episodes": 36, "reward": -200.0, "% time spent exploring": 84, "mean 100 episode reward": -200.0}
{"steps": 7199, "episodes": 37, "reward": -200.0, "% time spent exploring": 83, "mean 100 episode reward": -200.0}
{"steps": 7399, "episodes": 38, "reward": -200.0, "% time spent exploring": 83, "mean 100 episode reward": -200.0}
{"steps": 7599, "episodes": 39, "reward": -200.0, "% time spent exploring": 82, "mean 100 episode reward": -200.0}
{"steps": 7799, "episodes": 40, "reward": -200.0, "% time spent exploring": 82, "mean 100 episode reward": -200.0}
{"steps": 7999, "episodes": 41, "reward": -200.0, "% time spent exploring": 82, "mean 100 episode reward": -200.0}
{"steps": 8199, "episodes": 42, "reward": -200.0, "% time spent exploring": 81, "mean 100 episode reward": -200.0}
{"steps": 8399, "episodes": 43, "reward": -200.0, "% time spent exploring": 81, "mean 100 episode reward": -200.0}
{"steps": 8599, "episodes": 44, "reward": -200.0, "% time spent exploring": 80, "mean 100 episode reward": -200.0}
{"steps": 8799, "episodes": 45, "reward": -200.0, "% time spent exploring": 80, "mean 100 episode reward": -200.0}
{"steps": 8999, "episodes": 46, "reward": -200.0, "% time spent exploring": 79, "mean 100 episode reward": -200.0}
{"steps": 9199, "episodes": 47, "reward": -200.0, "% time spent exploring": 79, "mean 100 episode reward": -200.0}
{"steps": 9399, "episodes": 48, "reward": -200.0, "% time spent exploring": 78, "mean 100 episode reward": -200.0}
{"steps": 9599, "episodes": 49, "reward": -200.0, "% time spent exploring": 78, "mean 100 episode reward": -200.0}
{"steps": 9799, "episodes": 50, "reward": -200.0, "% time spent exploring": 77, "mean 100 episode reward": -200.0}
{"steps": 9999, "episodes": 51, "reward": -200.0, "% time spent exploring": 77, "mean 100 episode reward": -200.0}
{"steps": 10199, "episodes": 52, "reward": -200.0, "% time spent exploring": 77, "mean 100 episode reward": -200.0}
{"steps": 10399, "episodes": 53, "reward": -200.0, "% time spent exploring": 76, "mean 100 episode reward": -200.0}
{"steps": 10599, "episodes": 54, "reward": -200.0, "% time spent exploring": 76, "mean 100 episode reward": -200.0}
{"steps": 10799, "episodes": 55, "reward": -200.0, "% time spent exploring": 75, "mean 100 episode reward": -200.0}
{"steps": 10999, "episodes": 56, "reward": -200.0, "% time spent exploring": 75, "mean 100 episode reward": -200.0}
{"steps": 11199, "episodes": 57, "reward": -200.0, "% time spent exploring": 74, "mean 100 episode reward": -200.0}
{"steps": 11399, "episodes": 58, "reward": -200.0, "% time spent exploring": 74, "mean 100 episode reward": -200.0}
{"steps": 11599, "episodes": 59, "reward": -200.0, "% time spent exploring": 73, "mean 100 episode reward": -200.0}
{"steps": 11799, "episodes": 60, "reward": -200.0, "% time spent exploring": 73, "mean 100 episode reward": -200.0}
{"steps": 11999, "episodes": 61, "reward": -200.0, "% time spent exploring": 73, "mean 100 episode reward": -200.0}
{"steps": 12199, "episodes": 62, "reward": -200.0, "% time spent exploring": 72, "mean 100 episode reward": -200.0}
{"steps": 12399, "episodes": 63, "reward": -200.0, "% time spent exploring": 72, "mean 100 episode reward": -200.0}
{"steps": 12599, "episodes": 64, "reward": -200.0, "% time spent exploring": 71, "mean 100 episode reward": -200.0}
{"steps": 12799, "episodes": 65, "reward": -200.0, "% time spent exploring": 71, "mean 100 episode reward": -200.0}
{"steps": 12999, "episodes": 66, "reward": -200.0, "% time spent exploring": 70, "mean 100 episode reward": -200.0}
{"steps": 13199, "episodes": 67, "reward": -200.0, "% time spent exploring": 70, "mean 100 episode reward": -200.0}
{"steps": 13399, "episodes": 68, "reward": -200.0, "% time spent exploring": 69, "mean 100 episode reward": -200.0}
{"steps": 13599, "episodes": 69, "reward": -200.0, "% time spent exploring": 69, "mean 100 episode reward": -200.0}
{"steps": 13799, "episodes": 70, "reward": -200.0, "% time spent exploring": 68, "mean 100 episode reward": -200.0}
{"steps": 13999, "episodes": 71, "reward": -200.0, "% time spent exploring": 68, "mean 100 episode reward": -200.0}
{"steps": 14199, "episodes": 72, "reward": -200.0, "% time spent exploring": 68, "mean 100 episode reward": -200.0}
{"steps": 14399, "episodes": 73, "reward": -200.0, "% time spent exploring": 67, "mean 100 episode reward": -200.0}
{"steps": 14599, "episodes": 74, "reward": -200.0, "% time spent exploring": 67, "mean 100 episode reward": -200.0}
{"steps": 14799, "episodes": 75, "reward": -200.0, "% time spent exploring": 66, "mean 100 episode reward": -200.0}
{"steps": 14999, "episodes": 76, "reward": -200.0, "% time spent exploring": 66, "mean 100 episode reward": -200.0}
{"steps": 15199, "episodes": 77, "reward": -200.0, "% time spent exploring": 65, "mean 100 episode reward": -200.0}
{"steps": 15399, "episodes": 78, "reward": -200.0, "% time spent exploring": 65, "mean 100 episode reward": -200.0}
{"steps": 15599, "episodes": 79, "reward": -200.0, "% time spent exploring": 64, "mean 100 episode reward": -200.0}
{"steps": 15799, "episodes": 80, "reward": -200.0, "% time spent exploring": 64, "mean 100 episode reward": -200.0}
{"steps": 15999, "episodes": 81, "reward": -200.0, "% time spent exploring": 64, "mean 100 episode reward": -200.0}
{"steps": 16199, "episodes": 82, "reward": -200.0, "% time spent exploring": 63, "mean 100 episode reward": -200.0}
{"steps": 16399, "episodes": 83, "reward": -200.0, "% time spent exploring": 63, "mean 100 episode reward": -200.0}
{"steps": 16599, "episodes": 84, "reward": -200.0, "% time spent exploring": 62, "mean 100 episode reward": -200.0}
{"steps": 16799, "episodes": 85, "reward": -200.0, "% time spent exploring": 62, "mean 100 episode reward": -200.0}
{"steps": 16999, "episodes": 86, "reward": -200.0, "% time spent exploring": 61, "mean 100 episode reward": -200.0}
{"steps": 17199, "episodes": 87, "reward": -200.0, "% time spent exploring": 61, "mean 100 episode reward": -200.0}
{"steps": 17399, "episodes": 88, "reward": -200.0, "% time spent exploring": 60, "mean 100 episode reward": -200.0}
{"steps": 17599, "episodes": 89, "reward": -200.0, "% time spent exploring": 60, "mean 100 episode reward": -200.0}
{"steps": 17799, "episodes": 90, "reward": -200.0, "% time spent exploring": 59, "mean 100 episode reward": -200.0}
{"steps": 17999, "episodes": 91, "reward": -200.0, "% time spent exploring": 59, "mean 100 episode reward": -200.0}
{"steps": 18199, "episodes": 92, "reward": -200.0, "% time spent exploring": 59, "mean 100 episode reward": -200.0}
{"steps": 18399, "episodes": 93, "reward": -200.0, "% time spent exploring": 58, "mean 100 episode reward": -200.0}
{"steps": 18599, "episodes": 94, "reward": -200.0, "% time spent exploring": 58, "mean 100 episode reward": -200.0}
{"steps": 18799, "episodes": 95, "reward": -200.0, "% time spent exploring": 57, "mean 100 episode reward": -200.0}
{"steps": 18999, "episodes": 96, "reward": -200.0, "% time spent exploring": 57, "mean 100 episode reward": -200.0}
{"steps": 19199, "episodes": 97, "reward": -200.0, "% time spent exploring": 56, "mean 100 episode reward": -200.0}
{"steps": 19399, "episodes": 98, "reward": -200.0, "% time spent exploring": 56, "mean 100 episode reward": -200.0}
{"steps": 19599, "episodes": 99, "reward": -200.0, "% time spent exploring": 55, "mean 100 episode reward": -200.0}
{"steps": 19799, "episodes": 100, "reward": -200.0, "% time spent exploring": 55, "mean 100 episode reward": -200.0}
{"steps": 19999, "episodes": 101, "reward": -200.0, "% time spent exploring": 55, "mean 100 episode reward": -200.0}
{"steps": 20199, "episodes": 102, "reward": -200.0, "% time spent exploring": 54, "mean 100 episode reward": -200.0}
{"steps": 20399, "episodes": 103, "reward": -200.0, "% time spent exploring": 54, "mean 100 episode reward": -200.0}
{"steps": 20599, "episodes": 104, "reward": -200.0, "% time spent exploring": 53, "mean 100 episode reward": -200.0}
{"steps": 20799, "episodes": 105, "reward": -200.0, "% time spent exploring": 53, "mean 100 episode reward": -200.0}
{"steps": 20999, "episodes": 106, "reward": -200.0, "% time spent exploring": 52, "mean 100 episode reward": -200.0}
{"steps": 21199, "episodes": 107, "reward": -200.0, "% time spent exploring": 52, "mean 100 episode reward": -200.0}
{"steps": 21399, "episodes": 108, "reward": -200.0, "% time spent exploring": 51, "mean 100 episode reward": -200.0}
{"steps": 21599, "episodes": 109, "reward": -200.0, "% time spent exploring": 51, "mean 100 episode reward": -200.0}
{"steps": 21799, "episodes": 110, "reward": -200.0, "% time spent exploring": 50, "mean 100 episode reward": -200.0}
{"steps": 21999, "episodes": 111, "reward": -200.0, "% time spent exploring": 50, "mean 100 episode reward": -200.0}
{"steps": 22199, "episodes": 112, "reward": -200.0, "% time spent exploring": 50, "mean 100 episode reward": -200.0}
{"steps": 22399, "episodes": 113, "reward": -200.0, "% time spent exploring": 49, "mean 100 episode reward": -200.0}
{"steps": 22599, "episodes": 114, "reward": -200.0, "% time spent exploring": 49, "mean 100 episode reward": -200.0}
{"steps": 22799, "episodes": 115, "reward": -200.0, "% time spent exploring": 48, "mean 100 episode reward": -200.0}
{"steps": 22999, "episodes": 116, "reward": -200.0, "% time spent exploring": 48, "mean 100 episode reward": -200.0}
{"steps": 23199, "episodes": 117, "reward": -200.0, "% time spent exploring": 47, "mean 100 episode reward": -200.0}
{"steps": 23399, "episodes": 118, "reward": -200.0, "% time spent exploring": 47, "mean 100 episode reward": -200.0}
{"steps": 23599, "episodes": 119, "reward": -200.0, "% time spent exploring": 46, "mean 100 episode reward": -200.0}
{"steps": 23799, "episodes": 120, "reward": -200.0, "% time spent exploring": 46, "mean 100 episode reward": -200.0}
{"steps": 23999, "episodes": 121, "reward": -200.0, "% time spent exploring": 46, "mean 100 episode reward": -200.0}
{"steps": 24199, "episodes": 122, "reward": -200.0, "% time spent exploring": 45, "mean 100 episode reward": -200.0}
{"steps": 24399, "episodes": 123, "reward": -200.0, "% time spent exploring": 45, "mean 100 episode reward": -200.0}
{"steps": 24599, "episodes": 124, "reward": -200.0, "% time spent exploring": 44, "mean 100 episode reward": -200.0}
{"steps": 24799, "episodes": 125, "reward": -200.0, "% time spent exploring": 44, "mean 100 episode reward": -200.0}
{"steps": 24999, "episodes": 126, "reward": -200.0, "% time spent exploring": 43, "mean 100 episode reward": -200.0}
{"steps": 25199, "episodes": 127, "reward": -200.0, "% time spent exploring": 43, "mean 100 episode reward": -200.0}
{"steps": 25399, "episodes": 128, "reward": -200.0, "% time spent exploring": 42, "mean 100 episode reward": -200.0}
{"steps": 25599, "episodes": 129, "reward": -200.0, "% time spent exploring": 42, "mean 100 episode reward": -200.0}
{"steps": 25799, "episodes": 130, "reward": -200.0, "% time spent exploring": 41, "mean 100 episode reward": -200.0}
{"steps": 25999, "episodes": 131, "reward": -200.0, "% time spent exploring": 41, "mean 100 episode reward": -200.0}
{"steps": 26199, "episodes": 132, "reward": -200.0, "% time spent exploring": 41, "mean 100 episode reward": -200.0}
{"steps": 26399, "episodes": 133, "reward": -200.0, "% time spent exploring": 40, "mean 100 episode reward": -200.0}
{"steps": 26599, "episodes": 134, "reward": -200.0, "% time spent exploring": 40, "mean 100 episode reward": -200.0}
{"steps": 26799, "episodes": 135, "reward": -200.0, "% time spent exploring": 39, "mean 100 episode reward": -200.0}
{"steps": 26999, "episodes": 136, "reward": -200.0, "% time spent exploring": 39, "mean 100 episode reward": -200.0}
{"steps": 27199, "episodes": 137, "reward": -200.0, "% time spent exploring": 38, "mean 100 episode reward": -200.0}
{"steps": 27399, "episodes": 138, "reward": -200.0, "% time spent exploring": 38, "mean 100 episode reward": -200.0}
{"steps": 27599, "episodes": 139, "reward": -200.0, "% time spent exploring": 37, "mean 100 episode reward": -200.0}
{"steps": 27799, "episodes": 140, "reward": -200.0, "% time spent exploring": 37, "mean 100 episode reward": -200.0}
{"steps": 27999, "episodes": 141, "reward": -200.0, "% time spent exploring": 37, "mean 100 episode reward": -200.0}
{"steps": 28199, "episodes": 142, "reward": -200.0, "% time spent exploring": 36, "mean 100 episode reward": -200.0}
{"steps": 28399, "episodes": 143, "reward": -200.0, "% time spent exploring": 36, "mean 100 episode reward": -200.0}
{"steps": 28599, "episodes": 144, "reward": -200.0, "% time spent exploring": 35, "mean 100 episode reward": -200.0}
{"steps": 28799, "episodes": 145, "reward": -200.0, "% time spent exploring": 35, "mean 100 episode reward": -200.0}
{"steps": 28999, "episodes": 146, "reward": -200.0, "% time spent exploring": 34, "mean 100 episode reward": -200.0}
{"steps": 29199, "episodes": 147, "reward": -200.0, "% time spent exploring": 34, "mean 100 episode reward": -200.0}
{"steps": 29399, "episodes": 148, "reward": -200.0, "% time spent exploring": 33, "mean 100 episode reward": -200.0}
{"steps": 29599, "episodes": 149, "reward": -200.0, "% time spent exploring": 33, "mean 100 episode reward": -200.0}
{"steps": 29799, "episodes": 150, "reward": -200.0, "% time spent exploring": 32, "mean 100 episode reward": -200.0}
{"steps": 29999, "episodes": 151, "reward": -200.0, "% time spent exploring": 32, "mean 100 episode reward": -200.0}
{"steps": 30199, "episodes": 152, "reward": -200.0, "% time spent exploring": 32, "mean 100 episode reward": -200.0}
{"steps": 30399, "episodes": 153, "reward": -200.0, "% time spent exploring": 31, "mean 100 episode reward": -200.0}
{"steps": 30599, "episodes": 154, "reward": -200.0, "% time spent exploring": 31, "mean 100 episode reward": -200.0}
{"steps": 30799, "episodes": 155, "reward": -200.0, "% time spent exploring": 30, "mean 100 episode reward": -200.0}
{"steps": 30999, "episodes": 156, "reward": -200.0, "% time spent exploring": 30, "mean 100 episode reward": -200.0}
{"steps": 31199, "episodes": 157, "reward": -200.0, "% time spent exploring": 29, "mean 100 episode reward": -200.0}
{"steps": 31399, "episodes": 158, "reward": -200.0, "% time spent exploring": 29, "mean 100 episode reward": -200.0}
{"steps": 31599, "episodes": 159, "reward": -200.0, "% time spent exploring": 28, "mean 100 episode reward": -200.0}
{"steps": 31799, "episodes": 160, "reward": -200.0, "% time spent exploring": 28, "mean 100 episode reward": -200.0}
{"steps": 31999, "episodes": 161, "reward": -200.0, "% time spent exploring": 28, "mean 100 episode reward": -200.0}
{"steps": 32199, "episodes": 162, "reward": -200.0, "% time spent exploring": 27, "mean 100 episode reward": -200.0}
{"steps": 32399, "episodes": 163, "reward": -200.0, "% time spent exploring": 27, "mean 100 episode reward": -200.0}
{"steps": 32599, "episodes": 164, "reward": -200.0, "% time spent exploring": 26, "mean 100 episode reward": -200.0}
{"steps": 32799, "episodes": 165, "reward": -200.0, "% time spent exploring": 26, "mean 100 episode reward": -200.0}
{"steps": 32999, "episodes": 166, "reward": -200.0, "% time spent exploring": 25, "mean 100 episode reward": -200.0}
{"steps": 33199, "episodes": 167, "reward": -200.0, "% time spent exploring": 25, "mean 100 episode reward": -200.0}
{"steps": 33399, "episodes": 168, "reward": -200.0, "% time spent exploring": 24, "mean 100 episode reward": -200.0}
{"steps": 33599, "episodes": 169, "reward": -200.0, "% time spent exploring": 24, "mean 100 episode reward": -200.0}
{"steps": 33799, "episodes": 170, "reward": -200.0, "% time spent exploring": 23, "mean 100 episode reward": -200.0}
{"steps": 33999, "episodes": 171, "reward": -200.0, "% time spent exploring": 23, "mean 100 episode reward": -200.0}
{"steps": 34199, "episodes": 172, "reward": -200.0, "% time spent exploring": 23, "mean 100 episode reward": -200.0}
{"steps": 34399, "episodes": 173, "reward": -200.0, "% time spent exploring": 22, "mean 100 episode reward": -200.0}
{"steps": 34599, "episodes": 174, "reward": -200.0, "% time spent exploring": 22, "mean 100 episode reward": -200.0}
{"steps": 34799, "episodes": 175, "reward": -200.0, "% time spent exploring": 21, "mean 100 episode reward": -200.0}
{"steps": 34999, "episodes": 176, "reward": -200.0, "% time spent exploring": 21, "mean 100 episode reward": -200.0}
{"steps": 35199, "episodes": 177, "reward": -200.0, "% time spent exploring": 20, "mean 100 episode reward": -200.0}
{"steps": 35399, "episodes": 178, "reward": -200.0, "% time spent exploring": 20, "mean 100 episode reward": -200.0}
{"steps": 35599, "episodes": 179, "reward": -200.0, "% time spent exploring": 19, "mean 100 episode reward": -200.0}
{"steps": 35799, "episodes": 180, "reward": -200.0, "% time spent exploring": 19, "mean 100 episode reward": -200.0}
{"steps": 35999, "episodes": 181, "reward": -200.0, "% time spent exploring": 19, "mean 100 episode reward": -200.0}
{"steps": 36199, "episodes": 182, "reward": -200.0, "% time spent exploring": 18, "mean 100 episode reward": -200.0}
{"steps": 36399, "episodes": 183, "reward": -200.0, "% time spent exploring": 18, "mean 100 episode reward": -200.0}
{"steps": 36599, "episodes": 184, "reward": -200.0, "% time spent exploring": 17, "mean 100 episode reward": -200.0}
{"steps": 36799, "episodes": 185, "reward": -200.0, "% time spent exploring": 17, "mean 100 episode reward": -200.0}
{"steps": 36999, "episodes": 186, "reward": -200.0, "% time spent exploring": 16, "mean 100 episode reward": -200.0}
{"steps": 37199, "episodes": 187, "reward": -200.0, "% time spent exploring": 16, "mean 100 episode reward": -200.0}
{"steps": 37399, "episodes": 188, "reward": -200.0, "% time spent exploring": 15, "mean 100 episode reward": -200.0}
{"steps": 37599, "episodes": 189, "reward": -200.0, "% time spent exploring": 15, "mean 100 episode reward": -200.0}
{"steps": 37799, "episodes": 190, "reward": -200.0, "% time spent exploring": 14, "mean 100 episode reward": -200.0}
{"steps": 37999, "episodes": 191, "reward": -200.0, "% time spent exploring": 14, "mean 100 episode reward": -200.0}
{"steps": 38199, "episodes": 192, "reward": -200.0, "% time spent exploring": 14, "mean 100 episode reward": -200.0}
{"steps": 38399, "episodes": 193, "reward": -200.0, "% time spent exploring": 13, "mean 100 episode reward": -200.0}
{"steps": 38599, "episodes": 194, "reward": -200.0, "% time spent exploring": 13, "mean 100 episode reward": -200.0}
{"steps": 38799, "episodes": 195, "reward": -200.0, "% time spent exploring": 12, "mean 100 episode reward": -200.0}
{"steps": 38999, "episodes": 196, "reward": -200.0, "% time spent exploring": 12, "mean 100 episode reward": -200.0}
{"steps": 39199, "episodes": 197, "reward": -200.0, "% time spent exploring": 11, "mean 100 episode reward": -200.0}
{"steps": 39399, "episodes": 198, "reward": -200.0, "% time spent exploring": 11, "mean 100 episode reward": -200.0}
{"steps": 39599, "episodes": 199, "reward": -200.0, "% time spent exploring": 10, "mean 100 episode reward": -200.0}
{"steps": 39799, "episodes": 200, "reward": -200.0, "% time spent exploring": 10, "mean 100 episode reward": -200.0}
{"steps": 39999, "episodes": 201, "reward": -200.0, "% time spent exploring": 10, "mean 100 episode reward": -200.0}
{"steps": 40199, "episodes": 202, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 40399, "episodes": 203, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 40599, "episodes": 204, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 40799, "episodes": 205, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 40999, "episodes": 206, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 41199, "episodes": 207, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 41399, "episodes": 208, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 41599, "episodes": 209, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 41799, "episodes": 210, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 41999, "episodes": 211, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 42199, "episodes": 212, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 42399, "episodes": 213, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 42599, "episodes": 214, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 42799, "episodes": 215, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 42999, "episodes": 216, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 43199, "episodes": 217, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 43399, "episodes": 218, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 43599, "episodes": 219, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 43799, "episodes": 220, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 43999, "episodes": 221, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 44199, "episodes": 222, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 44399, "episodes": 223, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 44599, "episodes": 224, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 44799, "episodes": 225, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 44999, "episodes": 226, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 45199, "episodes": 227, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 45399, "episodes": 228, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 45599, "episodes": 229, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 45799, "episodes": 230, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 45999, "episodes": 231, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 46199, "episodes": 232, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 46399, "episodes": 233, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 46599, "episodes": 234, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 46799, "episodes": 235, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 46999, "episodes": 236, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 47199, "episodes": 237, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 47399, "episodes": 238, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 47599, "episodes": 239, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 47799, "episodes": 240, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 47999, "episodes": 241, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 48199, "episodes": 242, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 48399, "episodes": 243, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 48599, "episodes": 244, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 48799, "episodes": 245, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 48999, "episodes": 246, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 49199, "episodes": 247, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 49399, "episodes": 248, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 49599, "episodes": 249, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 49799, "episodes": 250, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 49999, "episodes": 251, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 50199, "episodes": 252, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 50399, "episodes": 253, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 50599, "episodes": 254, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 50799, "episodes": 255, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 50999, "episodes": 256, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 51199, "episodes": 257, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 51399, "episodes": 258, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 51599, "episodes": 259, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 51799, "episodes": 260, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 51999, "episodes": 261, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 52199, "episodes": 262, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 52399, "episodes": 263, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 52599, "episodes": 264, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 52799, "episodes": 265, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 52999, "episodes": 266, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 53199, "episodes": 267, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 53399, "episodes": 268, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 53599, "episodes": 269, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 53799, "episodes": 270, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 53999, "episodes": 271, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 54199, "episodes": 272, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 54399, "episodes": 273, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 54599, "episodes": 274, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 54799, "episodes": 275, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 54999, "episodes": 276, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 55199, "episodes": 277, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 55399, "episodes": 278, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 55599, "episodes": 279, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 55799, "episodes": 280, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 55999, "episodes": 281, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 56199, "episodes": 282, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 56399, "episodes": 283, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 56599, "episodes": 284, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 56799, "episodes": 285, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 56999, "episodes": 286, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 57199, "episodes": 287, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 57399, "episodes": 288, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 57599, "episodes": 289, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 57799, "episodes": 290, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 57999, "episodes": 291, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 58199, "episodes": 292, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 58399, "episodes": 293, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 58599, "episodes": 294, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 58799, "episodes": 295, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 58999, "episodes": 296, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 59199, "episodes": 297, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 59399, "episodes": 298, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 59599, "episodes": 299, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 59799, "episodes": 300, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 59999, "episodes": 301, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 60199, "episodes": 302, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 60399, "episodes": 303, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 60599, "episodes": 304, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 60799, "episodes": 305, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 60999, "episodes": 306, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 61199, "episodes": 307, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 61399, "episodes": 308, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 61599, "episodes": 309, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 61799, "episodes": 310, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 61999, "episodes": 311, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 62199, "episodes": 312, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 62399, "episodes": 313, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 62599, "episodes": 314, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 62799, "episodes": 315, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 62999, "episodes": 316, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 63199, "episodes": 317, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 63399, "episodes": 318, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 63599, "episodes": 319, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 63799, "episodes": 320, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 63999, "episodes": 321, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 64199, "episodes": 322, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 64399, "episodes": 323, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 64599, "episodes": 324, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 64799, "episodes": 325, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 64999, "episodes": 326, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 65199, "episodes": 327, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 65399, "episodes": 328, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 65599, "episodes": 329, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 65799, "episodes": 330, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 65999, "episodes": 331, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 66199, "episodes": 332, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 66399, "episodes": 333, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 66599, "episodes": 334, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 66799, "episodes": 335, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 66999, "episodes": 336, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 67199, "episodes": 337, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 67399, "episodes": 338, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 67599, "episodes": 339, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 67799, "episodes": 340, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 67999, "episodes": 341, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 68199, "episodes": 342, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 68399, "episodes": 343, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 68599, "episodes": 344, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 68799, "episodes": 345, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 68999, "episodes": 346, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 69199, "episodes": 347, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 69399, "episodes": 348, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 69599, "episodes": 349, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 69799, "episodes": 350, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 69999, "episodes": 351, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 70199, "episodes": 352, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 70399, "episodes": 353, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 70599, "episodes": 354, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 70799, "episodes": 355, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 70999, "episodes": 356, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 71199, "episodes": 357, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 71399, "episodes": 358, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 71599, "episodes": 359, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 71799, "episodes": 360, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 71999, "episodes": 361, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 72199, "episodes": 362, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 72399, "episodes": 363, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 72599, "episodes": 364, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 72799, "episodes": 365, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 72999, "episodes": 366, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 73199, "episodes": 367, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 73399, "episodes": 368, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 73599, "episodes": 369, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 73799, "episodes": 370, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 73999, "episodes": 371, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 74199, "episodes": 372, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 74399, "episodes": 373, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 74599, "episodes": 374, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 74799, "episodes": 375, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 74999, "episodes": 376, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 75199, "episodes": 377, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 75399, "episodes": 378, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 75599, "episodes": 379, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 75799, "episodes": 380, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 75999, "episodes": 381, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 76199, "episodes": 382, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 76399, "episodes": 383, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 76599, "episodes": 384, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 76799, "episodes": 385, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 76999, "episodes": 386, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 77199, "episodes": 387, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 77399, "episodes": 388, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 77599, "episodes": 389, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 77799, "episodes": 390, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 77999, "episodes": 391, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 78199, "episodes": 392, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 78399, "episodes": 393, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 78599, "episodes": 394, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 78799, "episodes": 395, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 78999, "episodes": 396, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 79199, "episodes": 397, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 79399, "episodes": 398, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 79599, "episodes": 399, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 79799, "episodes": 400, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 79999, "episodes": 401, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 80199, "episodes": 402, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 80399, "episodes": 403, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 80599, "episodes": 404, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 80799, "episodes": 405, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 80999, "episodes": 406, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 81199, "episodes": 407, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 81399, "episodes": 408, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 81599, "episodes": 409, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 81799, "episodes": 410, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 81999, "episodes": 411, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 82199, "episodes": 412, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 82399, "episodes": 413, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 82599, "episodes": 414, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 82799, "episodes": 415, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 82999, "episodes": 416, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 83199, "episodes": 417, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 83399, "episodes": 418, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 83599, "episodes": 419, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 83799, "episodes": 420, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 83999, "episodes": 421, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 84199, "episodes": 422, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 84399, "episodes": 423, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 84599, "episodes": 424, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 84799, "episodes": 425, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 84999, "episodes": 426, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 85199, "episodes": 427, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 85399, "episodes": 428, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 85599, "episodes": 429, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 85799, "episodes": 430, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 85999, "episodes": 431, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 86199, "episodes": 432, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 86399, "episodes": 433, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 86599, "episodes": 434, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 86799, "episodes": 435, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 86999, "episodes": 436, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 87199, "episodes": 437, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 87399, "episodes": 438, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 87599, "episodes": 439, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -200.0}
{"steps": 87788, "episodes": 440, "reward": -189.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 87988, "episodes": 441, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 88188, "episodes": 442, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 88388, "episodes": 443, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 88586, "episodes": 444, "reward": -198.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 88786, "episodes": 445, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 88986, "episodes": 446, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.9}
{"steps": 89171, "episodes": 447, "reward": -185.0, "% time spent exploring": 9, "mean 100 episode reward": -199.7}
{"steps": 89371, "episodes": 448, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.7}
{"steps": 89551, "episodes": 449, "reward": -180.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 89751, "episodes": 450, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 89951, "episodes": 451, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 90151, "episodes": 452, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 90351, "episodes": 453, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 90551, "episodes": 454, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 90751, "episodes": 455, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 90951, "episodes": 456, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 91151, "episodes": 457, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 91351, "episodes": 458, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.5}
{"steps": 91520, "episodes": 459, "reward": -169.0, "% time spent exploring": 9, "mean 100 episode reward": -199.2}
{"steps": 91720, "episodes": 460, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.2}
{"steps": 91895, "episodes": 461, "reward": -175.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 92095, "episodes": 462, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 92295, "episodes": 463, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 92495, "episodes": 464, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 92695, "episodes": 465, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 92895, "episodes": 466, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 93095, "episodes": 467, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 93295, "episodes": 468, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 93495, "episodes": 469, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 93695, "episodes": 470, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 93895, "episodes": 471, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 94095, "episodes": 472, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 94295, "episodes": 473, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 94495, "episodes": 474, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 94695, "episodes": 475, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 94895, "episodes": 476, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 95095, "episodes": 477, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 95295, "episodes": 478, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 95495, "episodes": 479, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 95695, "episodes": 480, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 95895, "episodes": 481, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 96095, "episodes": 482, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 96295, "episodes": 483, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 96495, "episodes": 484, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 96695, "episodes": 485, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 96895, "episodes": 486, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 97095, "episodes": 487, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 97295, "episodes": 488, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 97495, "episodes": 489, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 97695, "episodes": 490, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 97895, "episodes": 491, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 98095, "episodes": 492, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 98295, "episodes": 493, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 98495, "episodes": 494, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 98695, "episodes": 495, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 98895, "episodes": 496, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 99095, "episodes": 497, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 99295, "episodes": 498, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 99495, "episodes": 499, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 99695, "episodes": 500, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
{"steps": 99895, "episodes": 501, "reward": -200.0, "% time spent exploring": 9, "mean 100 episode reward": -199.0}
