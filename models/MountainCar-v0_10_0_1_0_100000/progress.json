{"reward": -200.0, "steps": 199, "episodes": 2, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 399, "episodes": 3, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 599, "episodes": 4, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 799, "episodes": 5, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 999, "episodes": 6, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 1199, "episodes": 7, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 1399, "episodes": 8, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 1599, "episodes": 9, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 1799, "episodes": 10, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 1999, "episodes": 11, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 2199, "episodes": 12, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 2399, "episodes": 13, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 2599, "episodes": 14, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 2799, "episodes": 15, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 2999, "episodes": 16, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 3199, "episodes": 17, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 3399, "episodes": 18, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 3599, "episodes": 19, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 3799, "episodes": 20, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 3999, "episodes": 21, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 4199, "episodes": 22, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 4399, "episodes": 23, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 4599, "episodes": 24, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 4799, "episodes": 25, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 4999, "episodes": 26, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 5199, "episodes": 27, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 5399, "episodes": 28, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 5599, "episodes": 29, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 5799, "episodes": 30, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 5999, "episodes": 31, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 6199, "episodes": 32, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 6399, "episodes": 33, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 6599, "episodes": 34, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 6799, "episodes": 35, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 6999, "episodes": 36, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 7199, "episodes": 37, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 7399, "episodes": 38, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 7599, "episodes": 39, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 7799, "episodes": 40, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 7999, "episodes": 41, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 8199, "episodes": 42, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 8399, "episodes": 43, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 8599, "episodes": 44, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 8799, "episodes": 45, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 8999, "episodes": 46, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 9199, "episodes": 47, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 9399, "episodes": 48, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 9599, "episodes": 49, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 9799, "episodes": 50, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 9999, "episodes": 51, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 10199, "episodes": 52, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 10399, "episodes": 53, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 10599, "episodes": 54, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 10799, "episodes": 55, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 10999, "episodes": 56, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 11199, "episodes": 57, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 11399, "episodes": 58, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 11599, "episodes": 59, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 11799, "episodes": 60, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 11999, "episodes": 61, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 12199, "episodes": 62, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 12399, "episodes": 63, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 12599, "episodes": 64, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 12799, "episodes": 65, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 12999, "episodes": 66, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 13199, "episodes": 67, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 13399, "episodes": 68, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 13599, "episodes": 69, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 13799, "episodes": 70, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 13999, "episodes": 71, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 14199, "episodes": 72, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 14399, "episodes": 73, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 14599, "episodes": 74, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 14799, "episodes": 75, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 14999, "episodes": 76, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 15199, "episodes": 77, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 15399, "episodes": 78, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 15599, "episodes": 79, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 15799, "episodes": 80, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 15999, "episodes": 81, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 16199, "episodes": 82, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 16399, "episodes": 83, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 16599, "episodes": 84, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 16799, "episodes": 85, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 16999, "episodes": 86, "mean 100 episode reward": -200.0}
{"reward": -200.0, "steps": 17199, "episodes": 87, "mean 100 episode reward": -200.0}
