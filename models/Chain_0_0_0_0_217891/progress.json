{"episodes": 2, "reward": 0.025000000000000015, "mean 100 episode reward": 0.0, "% time spent exploring": 99, "steps": 108}
{"episodes": 3, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 99, "steps": 217}
{"episodes": 4, "reward": 0.04100000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 98, "steps": 326}
{"episodes": 5, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 98, "steps": 435}
{"episodes": 6, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 97, "steps": 544}
{"episodes": 7, "reward": 0.01800000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 97, "steps": 653}
{"episodes": 8, "reward": 0.022000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 96, "steps": 762}
{"episodes": 9, "reward": 0.015000000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 96, "steps": 871}
{"episodes": 10, "reward": 0.025000000000000015, "mean 100 episode reward": 0.0, "% time spent exploring": 95, "steps": 980}
{"episodes": 11, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 95, "steps": 1089}
{"episodes": 12, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 95, "steps": 1198}
{"episodes": 13, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 94, "steps": 1307}
{"episodes": 14, "reward": 0.02000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 94, "steps": 1416}
{"episodes": 15, "reward": 0.024000000000000014, "mean 100 episode reward": 0.0, "% time spent exploring": 93, "steps": 1525}
{"episodes": 16, "reward": 0.023000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 93, "steps": 1634}
{"episodes": 17, "reward": 0.01800000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 92, "steps": 1743}
{"episodes": 18, "reward": 0.017000000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 92, "steps": 1852}
{"episodes": 19, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 91, "steps": 1961}
{"episodes": 20, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 91, "steps": 2070}
{"episodes": 21, "reward": 0.022000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 90, "steps": 2179}
{"episodes": 22, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 90, "steps": 2288}
{"episodes": 23, "reward": 0.03200000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 90, "steps": 2397}
{"episodes": 24, "reward": 0.035000000000000024, "mean 100 episode reward": 0.0, "% time spent exploring": 89, "steps": 2506}
{"episodes": 25, "reward": 0.02000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 89, "steps": 2615}
{"episodes": 26, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 88, "steps": 2724}
{"episodes": 27, "reward": 0.036000000000000025, "mean 100 episode reward": 0.0, "% time spent exploring": 88, "steps": 2833}
{"episodes": 28, "reward": 0.02100000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 87, "steps": 2942}
{"episodes": 29, "reward": 0.016000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 87, "steps": 3051}
{"episodes": 30, "reward": 0.03300000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 86, "steps": 3160}
{"episodes": 31, "reward": 0.01900000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 86, "steps": 3269}
{"episodes": 32, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 86, "steps": 3378}
{"episodes": 33, "reward": 0.01900000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 85, "steps": 3487}
{"episodes": 34, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 85, "steps": 3596}
{"episodes": 35, "reward": 0.016000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 84, "steps": 3705}
{"episodes": 36, "reward": 0.01900000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 84, "steps": 3814}
{"episodes": 37, "reward": 0.017000000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 83, "steps": 3923}
{"episodes": 38, "reward": 0.047000000000000035, "mean 100 episode reward": 0.0, "% time spent exploring": 83, "steps": 4032}
{"episodes": 39, "reward": 0.024000000000000014, "mean 100 episode reward": 0.0, "% time spent exploring": 82, "steps": 4141}
{"episodes": 40, "reward": 0.015000000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 82, "steps": 4250}
{"episodes": 41, "reward": 0.03100000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 81, "steps": 4359}
{"episodes": 42, "reward": 0.027000000000000017, "mean 100 episode reward": 0.0, "% time spent exploring": 81, "steps": 4468}
{"episodes": 43, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 81, "steps": 4577}
{"episodes": 44, "reward": 0.035000000000000024, "mean 100 episode reward": 0.0, "% time spent exploring": 80, "steps": 4686}
{"episodes": 45, "reward": 0.03900000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 80, "steps": 4795}
{"episodes": 46, "reward": 0.03200000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 79, "steps": 4904}
{"episodes": 47, "reward": 0.04300000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 79, "steps": 5013}
{"episodes": 48, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 78, "steps": 5122}
{"episodes": 49, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 78, "steps": 5231}
{"episodes": 50, "reward": 0.014000000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 77, "steps": 5340}
{"episodes": 51, "reward": 0.025000000000000015, "mean 100 episode reward": 0.0, "% time spent exploring": 77, "steps": 5449}
{"episodes": 52, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 77, "steps": 5558}
{"episodes": 53, "reward": 0.016000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 76, "steps": 5667}
{"episodes": 54, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 76, "steps": 5776}
{"episodes": 55, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 75, "steps": 5885}
{"episodes": 56, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 75, "steps": 5994}
{"episodes": 57, "reward": 0.009000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 74, "steps": 6103}
{"episodes": 58, "reward": 0.022000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 74, "steps": 6212}
{"episodes": 59, "reward": 0.05300000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 73, "steps": 6321}
{"episodes": 60, "reward": 0.04000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 73, "steps": 6430}
{"episodes": 61, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 72, "steps": 6539}
{"episodes": 62, "reward": 0.028000000000000018, "mean 100 episode reward": 0.0, "% time spent exploring": 72, "steps": 6648}
{"episodes": 63, "reward": 0.023000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 72, "steps": 6757}
{"episodes": 64, "reward": 0.016000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 71, "steps": 6866}
{"episodes": 65, "reward": 0.04400000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 71, "steps": 6975}
{"episodes": 66, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 70, "steps": 7084}
{"episodes": 67, "reward": 0.026000000000000016, "mean 100 episode reward": 0.0, "% time spent exploring": 70, "steps": 7193}
{"episodes": 68, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 69, "steps": 7302}
{"episodes": 69, "reward": 0.04300000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 69, "steps": 7411}
{"episodes": 70, "reward": 0.02900000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 68, "steps": 7520}
{"episodes": 71, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 68, "steps": 7629}
{"episodes": 72, "reward": 0.037000000000000026, "mean 100 episode reward": 0.0, "% time spent exploring": 68, "steps": 7738}
{"episodes": 73, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 67, "steps": 7847}
{"episodes": 74, "reward": 0.04000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 67, "steps": 7956}
{"episodes": 75, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 66, "steps": 8065}
{"episodes": 76, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 66, "steps": 8174}
{"episodes": 77, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 65, "steps": 8283}
{"episodes": 78, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 65, "steps": 8392}
{"episodes": 79, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 64, "steps": 8501}
{"episodes": 80, "reward": 0.024000000000000014, "mean 100 episode reward": 0.0, "% time spent exploring": 64, "steps": 8610}
{"episodes": 81, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 63, "steps": 8719}
{"episodes": 82, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 63, "steps": 8828}
{"episodes": 83, "reward": 0.01800000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 63, "steps": 8937}
{"episodes": 84, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 62, "steps": 9046}
{"episodes": 85, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 62, "steps": 9155}
{"episodes": 86, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 61, "steps": 9264}
{"episodes": 87, "reward": 0.028000000000000018, "mean 100 episode reward": 0.0, "% time spent exploring": 61, "steps": 9373}
{"episodes": 88, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 60, "steps": 9482}
{"episodes": 89, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 60, "steps": 9591}
{"episodes": 90, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 59, "steps": 9700}
{"episodes": 91, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 59, "steps": 9809}
{"episodes": 92, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 59, "steps": 9918}
{"episodes": 93, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 58, "steps": 10027}
{"episodes": 94, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 58, "steps": 10136}
{"episodes": 95, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 57, "steps": 10245}
{"episodes": 96, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 57, "steps": 10354}
{"episodes": 97, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 56, "steps": 10463}
{"episodes": 98, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 56, "steps": 10572}
{"episodes": 99, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 55, "steps": 10681}
{"episodes": 100, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 55, "steps": 10790}
{"episodes": 101, "reward": 0.017000000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 54, "steps": 10899}
{"episodes": 102, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 54, "steps": 11008}
{"episodes": 103, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 54, "steps": 11117}
{"episodes": 104, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 53, "steps": 11226}
{"episodes": 105, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 53, "steps": 11335}
{"episodes": 106, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 52, "steps": 11444}
{"episodes": 107, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 52, "steps": 11553}
{"episodes": 108, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 51, "steps": 11662}
{"episodes": 109, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 51, "steps": 11771}
{"episodes": 110, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 50, "steps": 11880}
{"episodes": 111, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 50, "steps": 11989}
{"episodes": 112, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 50, "steps": 12098}
{"episodes": 113, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 49, "steps": 12207}
{"episodes": 114, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 49, "steps": 12316}
{"episodes": 115, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 48, "steps": 12425}
{"episodes": 116, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 48, "steps": 12534}
{"episodes": 117, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 47, "steps": 12643}
{"episodes": 118, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 47, "steps": 12752}
{"episodes": 119, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 46, "steps": 12861}
{"episodes": 120, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 46, "steps": 12970}
{"episodes": 121, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 45, "steps": 13079}
{"episodes": 122, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 45, "steps": 13188}
{"episodes": 123, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 45, "steps": 13297}
{"episodes": 124, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 44, "steps": 13406}
{"episodes": 125, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 44, "steps": 13515}
{"episodes": 126, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 43, "steps": 13624}
{"episodes": 127, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 43, "steps": 13733}
{"episodes": 128, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 42, "steps": 13842}
{"episodes": 129, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 42, "steps": 13951}
{"episodes": 130, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 41, "steps": 14060}
{"episodes": 131, "reward": 0.009000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 41, "steps": 14169}
{"episodes": 132, "reward": 0.015000000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 41, "steps": 14278}
{"episodes": 133, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 40, "steps": 14387}
{"episodes": 134, "reward": 0.026000000000000016, "mean 100 episode reward": 0.0, "% time spent exploring": 40, "steps": 14496}
{"episodes": 135, "reward": 0.05200000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 39, "steps": 14605}
{"episodes": 136, "reward": 0.037000000000000026, "mean 100 episode reward": 0.0, "% time spent exploring": 39, "steps": 14714}
{"episodes": 137, "reward": 0.06300000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 38, "steps": 14823}
{"episodes": 138, "reward": 0.035000000000000024, "mean 100 episode reward": 0.0, "% time spent exploring": 38, "steps": 14932}
{"episodes": 139, "reward": 0.01800000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 37, "steps": 15041}
{"episodes": 140, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 37, "steps": 15150}
{"episodes": 141, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 36, "steps": 15259}
{"episodes": 142, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 36, "steps": 15368}
{"episodes": 143, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 36, "steps": 15477}
{"episodes": 144, "reward": 0.02100000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 35, "steps": 15586}
{"episodes": 145, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 35, "steps": 15695}
{"episodes": 146, "reward": 0.027000000000000017, "mean 100 episode reward": 0.0, "% time spent exploring": 34, "steps": 15804}
{"episodes": 147, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 34, "steps": 15913}
{"episodes": 148, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 33, "steps": 16022}
{"episodes": 149, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 33, "steps": 16131}
{"episodes": 150, "reward": 0.037000000000000026, "mean 100 episode reward": 0.0, "% time spent exploring": 32, "steps": 16240}
{"episodes": 151, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 32, "steps": 16349}
{"episodes": 152, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 32, "steps": 16458}
{"episodes": 153, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 31, "steps": 16567}
{"episodes": 154, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 31, "steps": 16676}
{"episodes": 155, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 30, "steps": 16785}
{"episodes": 156, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 30, "steps": 16894}
{"episodes": 157, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 29, "steps": 17003}
{"episodes": 158, "reward": 0.026000000000000016, "mean 100 episode reward": 0.0, "% time spent exploring": 29, "steps": 17112}
{"episodes": 159, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 28, "steps": 17221}
{"episodes": 160, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 28, "steps": 17330}
{"episodes": 161, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 27, "steps": 17439}
{"episodes": 162, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 27, "steps": 17548}
{"episodes": 163, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 27, "steps": 17657}
{"episodes": 164, "reward": 0.010000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 26, "steps": 17766}
{"episodes": 165, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 26, "steps": 17875}
{"episodes": 166, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 25, "steps": 17984}
{"episodes": 167, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 25, "steps": 18093}
{"episodes": 168, "reward": 0.017000000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 24, "steps": 18202}
{"episodes": 169, "reward": 0.07700000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 24, "steps": 18311}
{"episodes": 170, "reward": 0.04500000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 23, "steps": 18420}
{"episodes": 171, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 23, "steps": 18529}
{"episodes": 172, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 23, "steps": 18638}
{"episodes": 173, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 22, "steps": 18747}
{"episodes": 174, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 22, "steps": 18856}
{"episodes": 175, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 21, "steps": 18965}
{"episodes": 176, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 21, "steps": 19074}
{"episodes": 177, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 20, "steps": 19183}
{"episodes": 178, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 20, "steps": 19292}
{"episodes": 179, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 19, "steps": 19401}
{"episodes": 180, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 19, "steps": 19510}
{"episodes": 181, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 18, "steps": 19619}
{"episodes": 182, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 18, "steps": 19728}
{"episodes": 183, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 18, "steps": 19837}
{"episodes": 184, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 17, "steps": 19946}
{"episodes": 185, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 17, "steps": 20055}
{"episodes": 186, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 16, "steps": 20164}
{"episodes": 187, "reward": 0.009000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 16, "steps": 20273}
{"episodes": 188, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 15, "steps": 20382}
{"episodes": 189, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 15, "steps": 20491}
{"episodes": 190, "reward": 0.03100000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 14, "steps": 20600}
{"episodes": 191, "reward": 0.04500000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 14, "steps": 20709}
{"episodes": 192, "reward": 0.01900000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 14, "steps": 20818}
{"episodes": 193, "reward": 0.013000000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 13, "steps": 20927}
{"episodes": 194, "reward": 0.02100000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 13, "steps": 21036}
{"episodes": 195, "reward": 0.07600000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 12, "steps": 21145}
{"episodes": 196, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 12, "steps": 21254}
{"episodes": 197, "reward": 0.08100000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 11, "steps": 21363}
{"episodes": 198, "reward": 0.04900000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 11, "steps": 21472}
{"episodes": 199, "reward": 0.06600000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 10, "steps": 21581}
{"episodes": 200, "reward": 0.03400000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 10, "steps": 21690}
{"episodes": 201, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 21799}
{"episodes": 202, "reward": 0.004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 21908}
{"episodes": 203, "reward": 0.006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22017}
{"episodes": 204, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22126}
{"episodes": 205, "reward": 0.03000000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22235}
{"episodes": 206, "reward": 0.03100000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22344}
{"episodes": 207, "reward": 0.04500000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22453}
{"episodes": 208, "reward": 0.02900000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22562}
{"episodes": 209, "reward": 0.03300000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22671}
{"episodes": 210, "reward": 0.06500000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22780}
{"episodes": 211, "reward": 0.08800000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22889}
{"episodes": 212, "reward": 0.05300000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 22998}
{"episodes": 213, "reward": 0.09100000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23107}
{"episodes": 214, "reward": 0.002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23216}
{"episodes": 215, "reward": 0.05500000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23325}
{"episodes": 216, "reward": 0.10100000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23434}
{"episodes": 217, "reward": 0.09300000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23543}
{"episodes": 218, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23652}
{"episodes": 219, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23761}
{"episodes": 220, "reward": 0.03800000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23870}
{"episodes": 221, "reward": 0.008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 23979}
{"episodes": 222, "reward": 0.03800000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24088}
{"episodes": 223, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24197}
{"episodes": 224, "reward": 0.02000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24306}
{"episodes": 225, "reward": 0.035000000000000024, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24415}
{"episodes": 226, "reward": 0.048000000000000036, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24524}
{"episodes": 227, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24633}
{"episodes": 228, "reward": 0.001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24742}
{"episodes": 229, "reward": 0.03200000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24851}
{"episodes": 230, "reward": 0.04100000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 24960}
{"episodes": 231, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25069}
{"episodes": 232, "reward": 0.04100000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25178}
{"episodes": 233, "reward": 0.05500000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25287}
{"episodes": 234, "reward": 0.02100000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25396}
{"episodes": 235, "reward": 0.02100000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25505}
{"episodes": 236, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25614}
{"episodes": 237, "reward": 0.005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25723}
{"episodes": 238, "reward": 0.09500000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25832}
{"episodes": 239, "reward": 0.07800000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 25941}
{"episodes": 240, "reward": 0.08500000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26050}
{"episodes": 241, "reward": 0.10500000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26159}
{"episodes": 242, "reward": 0.06200000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26268}
{"episodes": 243, "reward": 0.10200000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26377}
{"episodes": 244, "reward": 0.09600000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26486}
{"episodes": 245, "reward": 0.10300000000000008, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26595}
{"episodes": 246, "reward": 0.059000000000000045, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26704}
{"episodes": 247, "reward": 0.02000000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26813}
{"episodes": 248, "reward": 0.06300000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 26922}
{"episodes": 249, "reward": 0.05200000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27031}
{"episodes": 250, "reward": 0.07500000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27140}
{"episodes": 251, "reward": 0.059000000000000045, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27249}
{"episodes": 252, "reward": 0.07800000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27358}
{"episodes": 253, "reward": 0.06600000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27467}
{"episodes": 254, "reward": 0.06800000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27576}
{"episodes": 255, "reward": 0.08300000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27685}
{"episodes": 256, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27794}
{"episodes": 257, "reward": 0.022000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 27903}
{"episodes": 258, "reward": 0.058000000000000045, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28012}
{"episodes": 259, "reward": 0.07800000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28121}
{"episodes": 260, "reward": 0.09500000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28230}
{"episodes": 261, "reward": 0.07700000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28339}
{"episodes": 262, "reward": 0.07900000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28448}
{"episodes": 263, "reward": 0.09000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28557}
{"episodes": 264, "reward": 0.07700000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28666}
{"episodes": 265, "reward": 0.07200000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28775}
{"episodes": 266, "reward": 0.07000000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28884}
{"episodes": 267, "reward": 0.07700000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 28993}
{"episodes": 268, "reward": 0.06100000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29102}
{"episodes": 269, "reward": 0.012000000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29211}
{"episodes": 270, "reward": 0.048000000000000036, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29320}
{"episodes": 271, "reward": 0.02900000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29429}
{"episodes": 272, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29538}
{"episodes": 273, "reward": 0.03900000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29647}
{"episodes": 274, "reward": 0.04000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29756}
{"episodes": 275, "reward": 0.060000000000000046, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29865}
{"episodes": 276, "reward": 0.0, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 29974}
{"episodes": 277, "reward": 0.04000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30083}
{"episodes": 278, "reward": 0.05100000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30192}
{"episodes": 279, "reward": 0.003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30301}
{"episodes": 280, "reward": 0.007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30410}
{"episodes": 281, "reward": 0.027000000000000017, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30519}
{"episodes": 282, "reward": 0.05500000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30628}
{"episodes": 283, "reward": 0.01800000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30737}
{"episodes": 284, "reward": 0.03100000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30846}
{"episodes": 285, "reward": 0.011000000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 30955}
{"episodes": 286, "reward": 0.037000000000000026, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31064}
{"episodes": 287, "reward": 0.060000000000000046, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31173}
{"episodes": 288, "reward": 0.023000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31282}
{"episodes": 289, "reward": 0.024000000000000014, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31391}
{"episodes": 290, "reward": 0.037000000000000026, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31500}
{"episodes": 291, "reward": 0.03300000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31609}
{"episodes": 292, "reward": 0.022000000000000013, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31718}
{"episodes": 293, "reward": 0.07000000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31827}
{"episodes": 294, "reward": 0.04200000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 31936}
{"episodes": 295, "reward": 0.03400000000000002, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32045}
{"episodes": 296, "reward": 0.028000000000000018, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32154}
{"episodes": 297, "reward": 0.026000000000000016, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32263}
{"episodes": 298, "reward": 0.01900000000000001, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32372}
{"episodes": 299, "reward": 0.04900000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32481}
{"episodes": 300, "reward": 0.036000000000000025, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32590}
{"episodes": 301, "reward": 0.07200000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32699}
{"episodes": 302, "reward": 0.05300000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32808}
{"episodes": 303, "reward": 0.09300000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 32917}
{"episodes": 304, "reward": 0.028000000000000018, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33026}
{"episodes": 305, "reward": 0.06900000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33135}
{"episodes": 306, "reward": 0.04100000000000003, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33244}
{"episodes": 307, "reward": 0.09500000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33353}
{"episodes": 308, "reward": 0.059000000000000045, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33462}
{"episodes": 309, "reward": 0.07200000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33571}
{"episodes": 310, "reward": 0.06700000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33680}
{"episodes": 311, "reward": 0.08800000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33789}
{"episodes": 312, "reward": 0.08600000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 33898}
{"episodes": 313, "reward": 0.05100000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34007}
{"episodes": 314, "reward": 0.06400000000000004, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34116}
{"episodes": 315, "reward": 0.09200000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34225}
{"episodes": 316, "reward": 0.10000000000000007, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34334}
{"episodes": 317, "reward": 0.07900000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34443}
{"episodes": 318, "reward": 0.08100000000000006, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34552}
{"episodes": 319, "reward": 0.07000000000000005, "mean 100 episode reward": 0.0, "% time spent exploring": 9, "steps": 34661}
{"episodes": 320, "reward": 0.10100000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 34770}
{"episodes": 321, "reward": 0.10100000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 34879}
{"episodes": 322, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 34988}
{"episodes": 323, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35097}
{"episodes": 324, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35206}
{"episodes": 325, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35315}
{"episodes": 326, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35424}
{"episodes": 327, "reward": 0.08600000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35533}
{"episodes": 328, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35642}
{"episodes": 329, "reward": 0.07600000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35751}
{"episodes": 330, "reward": 0.10000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35860}
{"episodes": 331, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 35969}
{"episodes": 332, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36078}
{"episodes": 333, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36187}
{"episodes": 334, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36296}
{"episodes": 335, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36405}
{"episodes": 336, "reward": 0.03900000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36514}
{"episodes": 337, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36623}
{"episodes": 338, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36732}
{"episodes": 339, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36841}
{"episodes": 340, "reward": 0.027000000000000017, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 36950}
{"episodes": 341, "reward": 0.04000000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37059}
{"episodes": 342, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37168}
{"episodes": 343, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37277}
{"episodes": 344, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37386}
{"episodes": 345, "reward": 0.05600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37495}
{"episodes": 346, "reward": 0.08700000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37604}
{"episodes": 347, "reward": 0.07000000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37713}
{"episodes": 348, "reward": 0.07800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37822}
{"episodes": 349, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 37931}
{"episodes": 350, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38040}
{"episodes": 351, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38149}
{"episodes": 352, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38258}
{"episodes": 353, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38367}
{"episodes": 354, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38476}
{"episodes": 355, "reward": 0.09800000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38585}
{"episodes": 356, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38694}
{"episodes": 357, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38803}
{"episodes": 358, "reward": 0.04300000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 38912}
{"episodes": 359, "reward": 0.026000000000000016, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39021}
{"episodes": 360, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39130}
{"episodes": 361, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39239}
{"episodes": 362, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39348}
{"episodes": 363, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39457}
{"episodes": 364, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39566}
{"episodes": 365, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39675}
{"episodes": 366, "reward": 0.02900000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39784}
{"episodes": 367, "reward": 0.024000000000000014, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 39893}
{"episodes": 368, "reward": 0.10600000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40002}
{"episodes": 369, "reward": 0.10400000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40111}
{"episodes": 370, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40220}
{"episodes": 371, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40329}
{"episodes": 372, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40438}
{"episodes": 373, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40547}
{"episodes": 374, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40656}
{"episodes": 375, "reward": 0.016000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40765}
{"episodes": 376, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40874}
{"episodes": 377, "reward": 0.08500000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 40983}
{"episodes": 378, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41092}
{"episodes": 379, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41201}
{"episodes": 380, "reward": 0.08500000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41310}
{"episodes": 381, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41419}
{"episodes": 382, "reward": 0.05600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41528}
{"episodes": 383, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41637}
{"episodes": 384, "reward": 0.08700000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41746}
{"episodes": 385, "reward": 0.07600000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41855}
{"episodes": 386, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 41964}
{"episodes": 387, "reward": 0.03400000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42073}
{"episodes": 388, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42182}
{"episodes": 389, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42291}
{"episodes": 390, "reward": 0.10400000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42400}
{"episodes": 391, "reward": 0.08800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42509}
{"episodes": 392, "reward": 0.09400000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42618}
{"episodes": 393, "reward": 0.10100000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42727}
{"episodes": 394, "reward": 0.10400000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42836}
{"episodes": 395, "reward": 0.08000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 42945}
{"episodes": 396, "reward": 0.10000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43054}
{"episodes": 397, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43163}
{"episodes": 398, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43272}
{"episodes": 399, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43381}
{"episodes": 400, "reward": 0.10000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43490}
{"episodes": 401, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43599}
{"episodes": 402, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43708}
{"episodes": 403, "reward": 0.08200000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43817}
{"episodes": 404, "reward": 0.09400000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 43926}
{"episodes": 405, "reward": 0.08000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44035}
{"episodes": 406, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44144}
{"episodes": 407, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44253}
{"episodes": 408, "reward": 0.028000000000000018, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44362}
{"episodes": 409, "reward": 0.07300000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44471}
{"episodes": 410, "reward": 0.04100000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44580}
{"episodes": 411, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44689}
{"episodes": 412, "reward": 0.09600000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44798}
{"episodes": 413, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 44907}
{"episodes": 414, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45016}
{"episodes": 415, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45125}
{"episodes": 416, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45234}
{"episodes": 417, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45343}
{"episodes": 418, "reward": 0.07600000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45452}
{"episodes": 419, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45561}
{"episodes": 420, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45670}
{"episodes": 421, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45779}
{"episodes": 422, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45888}
{"episodes": 423, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 45997}
{"episodes": 424, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46106}
{"episodes": 425, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46215}
{"episodes": 426, "reward": 0.10500000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46324}
{"episodes": 427, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46433}
{"episodes": 428, "reward": 0.10600000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46542}
{"episodes": 429, "reward": 0.08600000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46651}
{"episodes": 430, "reward": 0.10300000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46760}
{"episodes": 431, "reward": 0.09700000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46869}
{"episodes": 432, "reward": 0.09500000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 46978}
{"episodes": 433, "reward": 0.08800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47087}
{"episodes": 434, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47196}
{"episodes": 435, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47305}
{"episodes": 436, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47414}
{"episodes": 437, "reward": 0.09000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47523}
{"episodes": 438, "reward": 0.10700000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47632}
{"episodes": 439, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47741}
{"episodes": 440, "reward": 0.07800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47850}
{"episodes": 441, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 47959}
{"episodes": 442, "reward": 0.07300000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48068}
{"episodes": 443, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48177}
{"episodes": 444, "reward": 0.10000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48286}
{"episodes": 445, "reward": 0.09900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48395}
{"episodes": 446, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48504}
{"episodes": 447, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48613}
{"episodes": 448, "reward": 0.06500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48722}
{"episodes": 449, "reward": 0.09400000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48831}
{"episodes": 450, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 48940}
{"episodes": 451, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49049}
{"episodes": 452, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49158}
{"episodes": 453, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49267}
{"episodes": 454, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49376}
{"episodes": 455, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49485}
{"episodes": 456, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49594}
{"episodes": 457, "reward": 0.028000000000000018, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49703}
{"episodes": 458, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49812}
{"episodes": 459, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 49921}
{"episodes": 460, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50030}
{"episodes": 461, "reward": 0.08400000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50139}
{"episodes": 462, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50248}
{"episodes": 463, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50357}
{"episodes": 464, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50466}
{"episodes": 465, "reward": 0.09200000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50575}
{"episodes": 466, "reward": 0.09100000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50684}
{"episodes": 467, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50793}
{"episodes": 468, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 50902}
{"episodes": 469, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51011}
{"episodes": 470, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51120}
{"episodes": 471, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51229}
{"episodes": 472, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51338}
{"episodes": 473, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51447}
{"episodes": 474, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51556}
{"episodes": 475, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51665}
{"episodes": 476, "reward": 0.04300000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51774}
{"episodes": 477, "reward": 0.010000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51883}
{"episodes": 478, "reward": 0.04300000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 51992}
{"episodes": 479, "reward": 0.027000000000000017, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52101}
{"episodes": 480, "reward": 0.015000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52210}
{"episodes": 481, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52319}
{"episodes": 482, "reward": 0.04300000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52428}
{"episodes": 483, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52537}
{"episodes": 484, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52646}
{"episodes": 485, "reward": 0.05100000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52755}
{"episodes": 486, "reward": 0.03900000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52864}
{"episodes": 487, "reward": 0.02900000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 52973}
{"episodes": 488, "reward": 0.03400000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53082}
{"episodes": 489, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53191}
{"episodes": 490, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53300}
{"episodes": 491, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53409}
{"episodes": 492, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53518}
{"episodes": 493, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53627}
{"episodes": 494, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53736}
{"episodes": 495, "reward": 0.027000000000000017, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53845}
{"episodes": 496, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 53954}
{"episodes": 497, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54063}
{"episodes": 498, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54172}
{"episodes": 499, "reward": 0.03900000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54281}
{"episodes": 500, "reward": 0.022000000000000013, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54390}
{"episodes": 501, "reward": 0.03100000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54499}
{"episodes": 502, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54608}
{"episodes": 503, "reward": 0.013000000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54717}
{"episodes": 504, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54826}
{"episodes": 505, "reward": 0.10400000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 54935}
{"episodes": 506, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55044}
{"episodes": 507, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55153}
{"episodes": 508, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55262}
{"episodes": 509, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55371}
{"episodes": 510, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55480}
{"episodes": 511, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55589}
{"episodes": 512, "reward": 0.022000000000000013, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55698}
{"episodes": 513, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55807}
{"episodes": 514, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 55916}
{"episodes": 515, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56025}
{"episodes": 516, "reward": 0.015000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56134}
{"episodes": 517, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56243}
{"episodes": 518, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56352}
{"episodes": 519, "reward": 0.01900000000000001, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56461}
{"episodes": 520, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56570}
{"episodes": 521, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56679}
{"episodes": 522, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56788}
{"episodes": 523, "reward": 0.10500000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 56897}
{"episodes": 524, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57006}
{"episodes": 525, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57115}
{"episodes": 526, "reward": 0.08800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57224}
{"episodes": 527, "reward": 0.09500000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57333}
{"episodes": 528, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57442}
{"episodes": 529, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57551}
{"episodes": 530, "reward": 0.10300000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57660}
{"episodes": 531, "reward": 0.09100000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57769}
{"episodes": 532, "reward": 0.08800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57878}
{"episodes": 533, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 57987}
{"episodes": 534, "reward": 0.09000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58096}
{"episodes": 535, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58205}
{"episodes": 536, "reward": 0.05100000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58314}
{"episodes": 537, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58423}
{"episodes": 538, "reward": 0.03100000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58532}
{"episodes": 539, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58641}
{"episodes": 540, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58750}
{"episodes": 541, "reward": 0.10600000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58859}
{"episodes": 542, "reward": 0.08000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 58968}
{"episodes": 543, "reward": 0.08200000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59077}
{"episodes": 544, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59186}
{"episodes": 545, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59295}
{"episodes": 546, "reward": 0.07800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59404}
{"episodes": 547, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59513}
{"episodes": 548, "reward": 0.09100000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59622}
{"episodes": 549, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59731}
{"episodes": 550, "reward": 0.10400000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59840}
{"episodes": 551, "reward": 0.08500000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 59949}
{"episodes": 552, "reward": 0.09000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60058}
{"episodes": 553, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60167}
{"episodes": 554, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60276}
{"episodes": 555, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60385}
{"episodes": 556, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60494}
{"episodes": 557, "reward": 0.07300000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60603}
{"episodes": 558, "reward": 0.08000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60712}
{"episodes": 559, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60821}
{"episodes": 560, "reward": 0.09000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 60930}
{"episodes": 561, "reward": 0.10200000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61039}
{"episodes": 562, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61148}
{"episodes": 563, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61257}
{"episodes": 564, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61366}
{"episodes": 565, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61475}
{"episodes": 566, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61584}
{"episodes": 567, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61693}
{"episodes": 568, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61802}
{"episodes": 569, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 61911}
{"episodes": 570, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62020}
{"episodes": 571, "reward": 0.028000000000000018, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62129}
{"episodes": 572, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62238}
{"episodes": 573, "reward": 0.028000000000000018, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62347}
{"episodes": 574, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62456}
{"episodes": 575, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62565}
{"episodes": 576, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62674}
{"episodes": 577, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62783}
{"episodes": 578, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 62892}
{"episodes": 579, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63001}
{"episodes": 580, "reward": 0.08500000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63110}
{"episodes": 581, "reward": 0.07600000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63219}
{"episodes": 582, "reward": 0.10000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63328}
{"episodes": 583, "reward": 0.09700000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63437}
{"episodes": 584, "reward": 0.08400000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63546}
{"episodes": 585, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63655}
{"episodes": 586, "reward": 0.09400000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63764}
{"episodes": 587, "reward": 0.08600000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63873}
{"episodes": 588, "reward": 0.09000000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 63982}
{"episodes": 589, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64091}
{"episodes": 590, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64200}
{"episodes": 591, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64309}
{"episodes": 592, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64418}
{"episodes": 593, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64527}
{"episodes": 594, "reward": 0.07000000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64636}
{"episodes": 595, "reward": 0.08500000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64745}
{"episodes": 596, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64854}
{"episodes": 597, "reward": 0.07800000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 64963}
{"episodes": 598, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65072}
{"episodes": 599, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65181}
{"episodes": 600, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65290}
{"episodes": 601, "reward": 0.07700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65399}
{"episodes": 602, "reward": 0.02900000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65508}
{"episodes": 603, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65617}
{"episodes": 604, "reward": 0.03100000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65726}
{"episodes": 605, "reward": 0.04100000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65835}
{"episodes": 606, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 65944}
{"episodes": 607, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66053}
{"episodes": 608, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66162}
{"episodes": 609, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66271}
{"episodes": 610, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66380}
{"episodes": 611, "reward": 0.02900000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66489}
{"episodes": 612, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66598}
{"episodes": 613, "reward": 0.04100000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66707}
{"episodes": 614, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66816}
{"episodes": 615, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 66925}
{"episodes": 616, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67034}
{"episodes": 617, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67143}
{"episodes": 618, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67252}
{"episodes": 619, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67361}
{"episodes": 620, "reward": 0.04000000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67470}
{"episodes": 621, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67579}
{"episodes": 622, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67688}
{"episodes": 623, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67797}
{"episodes": 624, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 67906}
{"episodes": 625, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68015}
{"episodes": 626, "reward": 0.07600000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68124}
{"episodes": 627, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68233}
{"episodes": 628, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68342}
{"episodes": 629, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68451}
{"episodes": 630, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68560}
{"episodes": 631, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68669}
{"episodes": 632, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68778}
{"episodes": 633, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68887}
{"episodes": 634, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 68996}
{"episodes": 635, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69105}
{"episodes": 636, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69214}
{"episodes": 637, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69323}
{"episodes": 638, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69432}
{"episodes": 639, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69541}
{"episodes": 640, "reward": 0.026000000000000016, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69650}
{"episodes": 641, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69759}
{"episodes": 642, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69868}
{"episodes": 643, "reward": 0.04000000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 69977}
{"episodes": 644, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70086}
{"episodes": 645, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70195}
{"episodes": 646, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70304}
{"episodes": 647, "reward": 0.09900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70413}
{"episodes": 648, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70522}
{"episodes": 649, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70631}
{"episodes": 650, "reward": 0.05600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70740}
{"episodes": 651, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70849}
{"episodes": 652, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 70958}
{"episodes": 653, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71067}
{"episodes": 654, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71176}
{"episodes": 655, "reward": 0.08200000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71285}
{"episodes": 656, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71394}
{"episodes": 657, "reward": 0.04200000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71503}
{"episodes": 658, "reward": 0.026000000000000016, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71612}
{"episodes": 659, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71721}
{"episodes": 660, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71830}
{"episodes": 661, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 71939}
{"episodes": 662, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72048}
{"episodes": 663, "reward": 0.057000000000000044, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72157}
{"episodes": 664, "reward": 0.04300000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72266}
{"episodes": 665, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72375}
{"episodes": 666, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72484}
{"episodes": 667, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72593}
{"episodes": 668, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72702}
{"episodes": 669, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72811}
{"episodes": 670, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 72920}
{"episodes": 671, "reward": 0.09600000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73029}
{"episodes": 672, "reward": 0.07500000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73138}
{"episodes": 673, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73247}
{"episodes": 674, "reward": 0.04400000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73356}
{"episodes": 675, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73465}
{"episodes": 676, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73574}
{"episodes": 677, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73683}
{"episodes": 678, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73792}
{"episodes": 679, "reward": 0.09600000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 73901}
{"episodes": 680, "reward": 0.06500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74010}
{"episodes": 681, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74119}
{"episodes": 682, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74228}
{"episodes": 683, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74337}
{"episodes": 684, "reward": 0.028000000000000018, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74446}
{"episodes": 685, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74555}
{"episodes": 686, "reward": 0.06600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74664}
{"episodes": 687, "reward": 0.09600000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74773}
{"episodes": 688, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74882}
{"episodes": 689, "reward": 0.09200000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 74991}
{"episodes": 690, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75100}
{"episodes": 691, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75209}
{"episodes": 692, "reward": 0.04100000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75318}
{"episodes": 693, "reward": 0.03400000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75427}
{"episodes": 694, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75536}
{"episodes": 695, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75645}
{"episodes": 696, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75754}
{"episodes": 697, "reward": 0.03100000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75863}
{"episodes": 698, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 75972}
{"episodes": 699, "reward": 0.04900000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76081}
{"episodes": 700, "reward": 0.07200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76190}
{"episodes": 701, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76299}
{"episodes": 702, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76408}
{"episodes": 703, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76517}
{"episodes": 704, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76626}
{"episodes": 705, "reward": 0.10200000000000008, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76735}
{"episodes": 706, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76844}
{"episodes": 707, "reward": 0.08700000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 76953}
{"episodes": 708, "reward": 0.08400000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77062}
{"episodes": 709, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77171}
{"episodes": 710, "reward": 0.08900000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77280}
{"episodes": 711, "reward": 0.09100000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77389}
{"episodes": 712, "reward": 0.022000000000000013, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77498}
{"episodes": 713, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77607}
{"episodes": 714, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77716}
{"episodes": 715, "reward": 0.014000000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77825}
{"episodes": 716, "reward": 0.06400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 77934}
{"episodes": 717, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78043}
{"episodes": 718, "reward": 0.03200000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78152}
{"episodes": 719, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78261}
{"episodes": 720, "reward": 0.03000000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78370}
{"episodes": 721, "reward": 0.08700000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78479}
{"episodes": 722, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78588}
{"episodes": 723, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78697}
{"episodes": 724, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78806}
{"episodes": 725, "reward": 0.08000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 78915}
{"episodes": 726, "reward": 0.09300000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79024}
{"episodes": 727, "reward": 0.07300000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79133}
{"episodes": 728, "reward": 0.03900000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79242}
{"episodes": 729, "reward": 0.06300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79351}
{"episodes": 730, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79460}
{"episodes": 731, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79569}
{"episodes": 732, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79678}
{"episodes": 733, "reward": 0.08100000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79787}
{"episodes": 734, "reward": 0.06500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 79896}
{"episodes": 735, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80005}
{"episodes": 736, "reward": 0.058000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80114}
{"episodes": 737, "reward": 0.03800000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80223}
{"episodes": 738, "reward": 0.06800000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80332}
{"episodes": 739, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80441}
{"episodes": 740, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80550}
{"episodes": 741, "reward": 0.035000000000000024, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80659}
{"episodes": 742, "reward": 0.03100000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80768}
{"episodes": 743, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80877}
{"episodes": 744, "reward": 0.02900000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 80986}
{"episodes": 745, "reward": 0.037000000000000026, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81095}
{"episodes": 746, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81204}
{"episodes": 747, "reward": 0.03300000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81313}
{"episodes": 748, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81422}
{"episodes": 749, "reward": 0.012000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81531}
{"episodes": 750, "reward": 0.024000000000000014, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81640}
{"episodes": 751, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81749}
{"episodes": 752, "reward": 0.011000000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81858}
{"episodes": 753, "reward": 0.023000000000000013, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 81967}
{"episodes": 754, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82076}
{"episodes": 755, "reward": 0.08300000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82185}
{"episodes": 756, "reward": 0.05100000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82294}
{"episodes": 757, "reward": 0.04500000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82403}
{"episodes": 758, "reward": 0.07100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82512}
{"episodes": 759, "reward": 0.08400000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82621}
{"episodes": 760, "reward": 0.09600000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82730}
{"episodes": 761, "reward": 0.08700000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82839}
{"episodes": 762, "reward": 0.08200000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 82948}
{"episodes": 763, "reward": 0.07900000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83057}
{"episodes": 764, "reward": 0.05600000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83166}
{"episodes": 765, "reward": 0.04000000000000003, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83275}
{"episodes": 766, "reward": 0.05300000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83384}
{"episodes": 767, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83493}
{"episodes": 768, "reward": 0.06200000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83602}
{"episodes": 769, "reward": 0.06700000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83711}
{"episodes": 770, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83820}
{"episodes": 771, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 83929}
{"episodes": 772, "reward": 0.05500000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84038}
{"episodes": 773, "reward": 0.06900000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84147}
{"episodes": 774, "reward": 0.047000000000000035, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84256}
{"episodes": 775, "reward": 0.09500000000000007, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84365}
{"episodes": 776, "reward": 0.05000000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84474}
{"episodes": 777, "reward": 0.03200000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84583}
{"episodes": 778, "reward": 0.036000000000000025, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84692}
{"episodes": 779, "reward": 0.03400000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84801}
{"episodes": 780, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 84910}
{"episodes": 781, "reward": 0.046000000000000034, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85019}
{"episodes": 782, "reward": 0.03400000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85128}
{"episodes": 783, "reward": 0.024000000000000014, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85237}
{"episodes": 784, "reward": 0.048000000000000036, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85346}
{"episodes": 785, "reward": 0.06100000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85455}
{"episodes": 786, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85564}
{"episodes": 787, "reward": 0.05200000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85673}
{"episodes": 788, "reward": 0.07400000000000005, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85782}
{"episodes": 789, "reward": 0.05400000000000004, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 85891}
{"episodes": 790, "reward": 0.060000000000000046, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86000}
{"episodes": 791, "reward": 0.03200000000000002, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86109}
{"episodes": 792, "reward": 0.025000000000000015, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86218}
{"episodes": 793, "reward": 0.026000000000000016, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86327}
{"episodes": 794, "reward": 0.015000000000000006, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86436}
{"episodes": 795, "reward": 0.059000000000000045, "mean 100 episode reward": 0.1, "% time spent exploring": 9, "steps": 86545}
